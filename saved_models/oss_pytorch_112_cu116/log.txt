 0: 2022-11-03 06:12:04 | INFO | fairseq.distributed_utils | distributed init (rank 0): env://
 1: 2022-11-03 06:12:04 | INFO | fairseq.distributed_utils | distributed init (rank 1): env://
 1: 2022-11-03 06:12:04 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
 3: 2022-11-03 06:12:04 | INFO | fairseq.distributed_utils | distributed init (rank 3): env://
 2: 2022-11-03 06:12:04 | INFO | fairseq.distributed_utils | distributed init (rank 2): env://
 3: 2022-11-03 06:12:04 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
 2: 2022-11-03 06:12:04 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
 7: 2022-11-03 06:12:04 | INFO | fairseq.distributed_utils | distributed init (rank 7): env://
 7: 2022-11-03 06:12:04 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
 6: 2022-11-03 06:12:04 | INFO | fairseq.distributed_utils | distributed init (rank 6): env://
 6: 2022-11-03 06:12:04 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
 4: 2022-11-03 06:12:04 | INFO | fairseq.distributed_utils | distributed init (rank 4): env://
 4: 2022-11-03 06:12:04 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
 5: 2022-11-03 06:12:04 | INFO | fairseq.distributed_utils | distributed init (rank 5): env://
 5: 2022-11-03 06:12:04 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
14: 2022-11-03 06:12:04 | INFO | fairseq.distributed_utils | distributed init (rank 14): env://
 8: 2022-11-03 06:12:04 | INFO | fairseq.distributed_utils | distributed init (rank 8): env://
13: 2022-11-03 06:12:04 | INFO | fairseq.distributed_utils | distributed init (rank 13): env://
10: 2022-11-03 06:12:04 | INFO | fairseq.distributed_utils | distributed init (rank 10): env://
 8: 2022-11-03 06:12:04 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 8
14: 2022-11-03 06:12:04 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 14
15: 2022-11-03 06:12:04 | INFO | fairseq.distributed_utils | distributed init (rank 15): env://
13: 2022-11-03 06:12:04 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 13
10: 2022-11-03 06:12:04 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 10
 9: 2022-11-03 06:12:04 | INFO | fairseq.distributed_utils | distributed init (rank 9): env://
12: 2022-11-03 06:12:04 | INFO | fairseq.distributed_utils | distributed init (rank 12): env://
11: 2022-11-03 06:12:04 | INFO | fairseq.distributed_utils | distributed init (rank 11): env://
 9: 2022-11-03 06:12:04 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 9
15: 2022-11-03 06:12:04 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 15
12: 2022-11-03 06:12:04 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 12
11: 2022-11-03 06:12:04 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 11
 0: 2022-11-03 06:12:04 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
 0: 2022-11-03 06:12:04 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
 0: 2022-11-03 06:12:04 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-5 as rank 0
11: 2022-11-03 06:12:04 | INFO | torch.distributed.distributed_c10d | Rank 11: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
11: 2022-11-03 06:12:04 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-6 as rank 11
 3: 2022-11-03 06:12:04 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
 1: 2022-11-03 06:12:04 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
 2: 2022-11-03 06:12:04 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
 7: 2022-11-03 06:12:04 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
 3: 2022-11-03 06:12:04 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-5 as rank 3
 4: 2022-11-03 06:12:04 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
 5: 2022-11-03 06:12:04 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
 6: 2022-11-03 06:12:04 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
 1: 2022-11-03 06:12:04 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-5 as rank 1
 2: 2022-11-03 06:12:04 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-5 as rank 2
 7: 2022-11-03 06:12:04 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-5 as rank 7
 4: 2022-11-03 06:12:04 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-5 as rank 4
 6: 2022-11-03 06:12:04 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-5 as rank 6
 5: 2022-11-03 06:12:04 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-5 as rank 5
12: 2022-11-03 06:12:04 | INFO | torch.distributed.distributed_c10d | Rank 12: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
12: 2022-11-03 06:12:04 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-6 as rank 12
 8: 2022-11-03 06:12:04 | INFO | torch.distributed.distributed_c10d | Rank 8: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
14: 2022-11-03 06:12:04 | INFO | torch.distributed.distributed_c10d | Rank 14: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
10: 2022-11-03 06:12:04 | INFO | torch.distributed.distributed_c10d | Rank 10: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
13: 2022-11-03 06:12:04 | INFO | torch.distributed.distributed_c10d | Rank 13: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
 8: 2022-11-03 06:12:04 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-6 as rank 8
 9: 2022-11-03 06:12:04 | INFO | torch.distributed.distributed_c10d | Rank 9: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
13: 2022-11-03 06:12:04 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-6 as rank 13
14: 2022-11-03 06:12:04 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-6 as rank 14
15: 2022-11-03 06:12:04 | INFO | torch.distributed.distributed_c10d | Rank 15: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
10: 2022-11-03 06:12:04 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-6 as rank 10
 9: 2022-11-03 06:12:04 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-6 as rank 9
15: 2022-11-03 06:12:04 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-6 as rank 15
 0: queue1-dy-p4d24xlarge-5:26327:26327 [0] NCCL INFO Bootstrap : Using ens32:10.0.29.73<0>
 0: queue1-dy-p4d24xlarge-5:26327:26327 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v5 symbol.
 0: queue1-dy-p4d24xlarge-5:26327:26327 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.
 0: queue1-dy-p4d24xlarge-5:26327:26327 [0] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws
 0: queue1-dy-p4d24xlarge-5:26327:26327 [0] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /fsx/conda/envs/fairseq_aws/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml
 0: queue1-dy-p4d24xlarge-5:26327:26327 [0] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1
 0: queue1-dy-p4d24xlarge-5:26327:26327 [0] NCCL INFO NET/OFI Forcing AWS OFI ndev 4
 0: queue1-dy-p4d24xlarge-5:26327:26327 [0] NCCL INFO NET/OFI Selected Provider is efa
 0: queue1-dy-p4d24xlarge-5:26327:26327 [0] NCCL INFO Using network AWS Libfabric
 0: NCCL version 2.12.12+cuda11.6
 4: queue1-dy-p4d24xlarge-5:26331:26331 [4] NCCL INFO Bootstrap : Using ens32:10.0.29.73<0>
 6: queue1-dy-p4d24xlarge-5:26333:26333 [6] NCCL INFO Bootstrap : Using ens32:10.0.29.73<0>
15: queue1-dy-p4d24xlarge-6:22832:22832 [7] NCCL INFO Bootstrap : Using ens32:10.0.19.87<0>
 4: queue1-dy-p4d24xlarge-5:26331:26331 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v5 symbol.
 6: queue1-dy-p4d24xlarge-5:26333:26333 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v5 symbol.
 4: queue1-dy-p4d24xlarge-5:26331:26331 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.
 6: queue1-dy-p4d24xlarge-5:26333:26333 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.
 6: queue1-dy-p4d24xlarge-5:26333:26333 [6] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws
 4: queue1-dy-p4d24xlarge-5:26331:26331 [4] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws
 4: queue1-dy-p4d24xlarge-5:26331:26331 [4] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /fsx/conda/envs/fairseq_aws/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml
 6: queue1-dy-p4d24xlarge-5:26333:26333 [6] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /fsx/conda/envs/fairseq_aws/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml
 6: queue1-dy-p4d24xlarge-5:26333:26333 [6] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1
 4: queue1-dy-p4d24xlarge-5:26331:26331 [4] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1
 7: queue1-dy-p4d24xlarge-5:26334:26334 [7] NCCL INFO Bootstrap : Using ens32:10.0.29.73<0>
 7: queue1-dy-p4d24xlarge-5:26334:26334 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v5 symbol.
 7: queue1-dy-p4d24xlarge-5:26334:26334 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.
 7: queue1-dy-p4d24xlarge-5:26334:26334 [7] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws
 7: queue1-dy-p4d24xlarge-5:26334:26334 [7] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /fsx/conda/envs/fairseq_aws/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml
 7: queue1-dy-p4d24xlarge-5:26334:26334 [7] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1
 4: queue1-dy-p4d24xlarge-5:26331:26331 [4] NCCL INFO NET/OFI Forcing AWS OFI ndev 4
 4: queue1-dy-p4d24xlarge-5:26331:26331 [4] NCCL INFO NET/OFI Selected Provider is efa
 6: queue1-dy-p4d24xlarge-5:26333:26333 [6] NCCL INFO NET/OFI Forcing AWS OFI ndev 4
 6: queue1-dy-p4d24xlarge-5:26333:26333 [6] NCCL INFO NET/OFI Selected Provider is efa
 6: queue1-dy-p4d24xlarge-5:26333:26333 [6] NCCL INFO Using network AWS Libfabric
 4: queue1-dy-p4d24xlarge-5:26331:26331 [4] NCCL INFO Using network AWS Libfabric
15: queue1-dy-p4d24xlarge-6:22832:22832 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v5 symbol.
15: queue1-dy-p4d24xlarge-6:22832:22832 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.
 3: queue1-dy-p4d24xlarge-5:26330:26330 [3] NCCL INFO Bootstrap : Using ens32:10.0.29.73<0>
15: queue1-dy-p4d24xlarge-6:22832:22832 [7] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws
15: queue1-dy-p4d24xlarge-6:22832:22832 [7] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /fsx/conda/envs/fairseq_aws/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml
15: queue1-dy-p4d24xlarge-6:22832:22832 [7] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1
 7: queue1-dy-p4d24xlarge-5:26334:26334 [7] NCCL INFO NET/OFI Forcing AWS OFI ndev 4
 7: queue1-dy-p4d24xlarge-5:26334:26334 [7] NCCL INFO NET/OFI Selected Provider is efa
 7: queue1-dy-p4d24xlarge-5:26334:26334 [7] NCCL INFO Using network AWS Libfabric
 3: queue1-dy-p4d24xlarge-5:26330:26330 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v5 symbol.
 3: queue1-dy-p4d24xlarge-5:26330:26330 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.
 3: queue1-dy-p4d24xlarge-5:26330:26330 [3] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws
 3: queue1-dy-p4d24xlarge-5:26330:26330 [3] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /fsx/conda/envs/fairseq_aws/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml
 3: queue1-dy-p4d24xlarge-5:26330:26330 [3] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1
 3: queue1-dy-p4d24xlarge-5:26330:26330 [3] NCCL INFO NET/OFI Forcing AWS OFI ndev 4
 3: queue1-dy-p4d24xlarge-5:26330:26330 [3] NCCL INFO NET/OFI Selected Provider is efa
 3: queue1-dy-p4d24xlarge-5:26330:26330 [3] NCCL INFO Using network AWS Libfabric
15: queue1-dy-p4d24xlarge-6:22832:22832 [7] NCCL INFO NET/OFI Forcing AWS OFI ndev 4
15: queue1-dy-p4d24xlarge-6:22832:22832 [7] NCCL INFO NET/OFI Selected Provider is efa
15: queue1-dy-p4d24xlarge-6:22832:22832 [7] NCCL INFO Using network AWS Libfabric
14: queue1-dy-p4d24xlarge-6:22831:22831 [6] NCCL INFO Bootstrap : Using ens32:10.0.19.87<0>
14: queue1-dy-p4d24xlarge-6:22831:22831 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v5 symbol.
14: queue1-dy-p4d24xlarge-6:22831:22831 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.
14: queue1-dy-p4d24xlarge-6:22831:22831 [6] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws
14: queue1-dy-p4d24xlarge-6:22831:22831 [6] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /fsx/conda/envs/fairseq_aws/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml
14: queue1-dy-p4d24xlarge-6:22831:22831 [6] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1
 2: queue1-dy-p4d24xlarge-5:26329:26329 [2] NCCL INFO Bootstrap : Using ens32:10.0.29.73<0>
14: queue1-dy-p4d24xlarge-6:22831:22831 [6] NCCL INFO NET/OFI Forcing AWS OFI ndev 4
14: queue1-dy-p4d24xlarge-6:22831:22831 [6] NCCL INFO NET/OFI Selected Provider is efa
14: queue1-dy-p4d24xlarge-6:22831:22831 [6] NCCL INFO Using network AWS Libfabric
 2: queue1-dy-p4d24xlarge-5:26329:26329 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v5 symbol.
 2: queue1-dy-p4d24xlarge-5:26329:26329 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.
 2: queue1-dy-p4d24xlarge-5:26329:26329 [2] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws
 2: queue1-dy-p4d24xlarge-5:26329:26329 [2] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /fsx/conda/envs/fairseq_aws/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml
 2: queue1-dy-p4d24xlarge-5:26329:26329 [2] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1
 2: queue1-dy-p4d24xlarge-5:26329:26329 [2] NCCL INFO NET/OFI Forcing AWS OFI ndev 4
 2: queue1-dy-p4d24xlarge-5:26329:26329 [2] NCCL INFO NET/OFI Selected Provider is efa
 2: queue1-dy-p4d24xlarge-5:26329:26329 [2] NCCL INFO Using network AWS Libfabric
 1: queue1-dy-p4d24xlarge-5:26328:26328 [1] NCCL INFO Bootstrap : Using ens32:10.0.29.73<0>
 1: queue1-dy-p4d24xlarge-5:26328:26328 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v5 symbol.
 1: queue1-dy-p4d24xlarge-5:26328:26328 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.
 1: queue1-dy-p4d24xlarge-5:26328:26328 [1] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws
 1: queue1-dy-p4d24xlarge-5:26328:26328 [1] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /fsx/conda/envs/fairseq_aws/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml
 1: queue1-dy-p4d24xlarge-5:26328:26328 [1] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1
 1: queue1-dy-p4d24xlarge-5:26328:26328 [1] NCCL INFO NET/OFI Forcing AWS OFI ndev 4
 1: queue1-dy-p4d24xlarge-5:26328:26328 [1] NCCL INFO NET/OFI Selected Provider is efa
 1: queue1-dy-p4d24xlarge-5:26328:26328 [1] NCCL INFO Using network AWS Libfabric
 9: queue1-dy-p4d24xlarge-6:22826:22826 [1] NCCL INFO Bootstrap : Using ens32:10.0.19.87<0>
10: queue1-dy-p4d24xlarge-6:22827:22827 [2] NCCL INFO Bootstrap : Using ens32:10.0.19.87<0>
 9: queue1-dy-p4d24xlarge-6:22826:22826 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v5 symbol.
 9: queue1-dy-p4d24xlarge-6:22826:22826 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.
 9: queue1-dy-p4d24xlarge-6:22826:22826 [1] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws
 9: queue1-dy-p4d24xlarge-6:22826:22826 [1] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /fsx/conda/envs/fairseq_aws/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml
 9: queue1-dy-p4d24xlarge-6:22826:22826 [1] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1
10: queue1-dy-p4d24xlarge-6:22827:22827 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v5 symbol.
10: queue1-dy-p4d24xlarge-6:22827:22827 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.
10: queue1-dy-p4d24xlarge-6:22827:22827 [2] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws
10: queue1-dy-p4d24xlarge-6:22827:22827 [2] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /fsx/conda/envs/fairseq_aws/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml
10: queue1-dy-p4d24xlarge-6:22827:22827 [2] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1
 9: queue1-dy-p4d24xlarge-6:22826:22826 [1] NCCL INFO NET/OFI Forcing AWS OFI ndev 4
 9: queue1-dy-p4d24xlarge-6:22826:22826 [1] NCCL INFO NET/OFI Selected Provider is efa
 9: queue1-dy-p4d24xlarge-6:22826:22826 [1] NCCL INFO Using network AWS Libfabric
10: queue1-dy-p4d24xlarge-6:22827:22827 [2] NCCL INFO NET/OFI Forcing AWS OFI ndev 4
10: queue1-dy-p4d24xlarge-6:22827:22827 [2] NCCL INFO NET/OFI Selected Provider is efa
10: queue1-dy-p4d24xlarge-6:22827:22827 [2] NCCL INFO Using network AWS Libfabric
 5: queue1-dy-p4d24xlarge-5:26332:26332 [5] NCCL INFO Bootstrap : Using ens32:10.0.29.73<0>
 5: queue1-dy-p4d24xlarge-5:26332:26332 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v5 symbol.
 5: queue1-dy-p4d24xlarge-5:26332:26332 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.
 5: queue1-dy-p4d24xlarge-5:26332:26332 [5] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws
 5: queue1-dy-p4d24xlarge-5:26332:26332 [5] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /fsx/conda/envs/fairseq_aws/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml
 5: queue1-dy-p4d24xlarge-5:26332:26332 [5] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1
 5: queue1-dy-p4d24xlarge-5:26332:26332 [5] NCCL INFO NET/OFI Forcing AWS OFI ndev 4
 5: queue1-dy-p4d24xlarge-5:26332:26332 [5] NCCL INFO NET/OFI Selected Provider is efa
 5: queue1-dy-p4d24xlarge-5:26332:26332 [5] NCCL INFO Using network AWS Libfabric
 8: queue1-dy-p4d24xlarge-6:22825:22825 [0] NCCL INFO Bootstrap : Using ens32:10.0.19.87<0>
 8: queue1-dy-p4d24xlarge-6:22825:22825 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v5 symbol.
 8: queue1-dy-p4d24xlarge-6:22825:22825 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.
 8: queue1-dy-p4d24xlarge-6:22825:22825 [0] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws
 8: queue1-dy-p4d24xlarge-6:22825:22825 [0] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /fsx/conda/envs/fairseq_aws/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml
 8: queue1-dy-p4d24xlarge-6:22825:22825 [0] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1
 8: queue1-dy-p4d24xlarge-6:22825:22825 [0] NCCL INFO NET/OFI Forcing AWS OFI ndev 4
 8: queue1-dy-p4d24xlarge-6:22825:22825 [0] NCCL INFO NET/OFI Selected Provider is efa
 8: queue1-dy-p4d24xlarge-6:22825:22825 [0] NCCL INFO Using network AWS Libfabric
12: queue1-dy-p4d24xlarge-6:22829:22829 [4] NCCL INFO Bootstrap : Using ens32:10.0.19.87<0>
12: queue1-dy-p4d24xlarge-6:22829:22829 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v5 symbol.
12: queue1-dy-p4d24xlarge-6:22829:22829 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.
12: queue1-dy-p4d24xlarge-6:22829:22829 [4] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws
12: queue1-dy-p4d24xlarge-6:22829:22829 [4] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /fsx/conda/envs/fairseq_aws/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml
12: queue1-dy-p4d24xlarge-6:22829:22829 [4] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1
12: queue1-dy-p4d24xlarge-6:22829:22829 [4] NCCL INFO NET/OFI Forcing AWS OFI ndev 4
12: queue1-dy-p4d24xlarge-6:22829:22829 [4] NCCL INFO NET/OFI Selected Provider is efa
12: queue1-dy-p4d24xlarge-6:22829:22829 [4] NCCL INFO Using network AWS Libfabric
11: queue1-dy-p4d24xlarge-6:22828:22828 [3] NCCL INFO Bootstrap : Using ens32:10.0.19.87<0>
11: queue1-dy-p4d24xlarge-6:22828:22828 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v5 symbol.
11: queue1-dy-p4d24xlarge-6:22828:22828 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.
11: queue1-dy-p4d24xlarge-6:22828:22828 [3] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws
11: queue1-dy-p4d24xlarge-6:22828:22828 [3] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /fsx/conda/envs/fairseq_aws/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml
11: queue1-dy-p4d24xlarge-6:22828:22828 [3] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1
11: queue1-dy-p4d24xlarge-6:22828:22828 [3] NCCL INFO NET/OFI Forcing AWS OFI ndev 4
11: queue1-dy-p4d24xlarge-6:22828:22828 [3] NCCL INFO NET/OFI Selected Provider is efa
11: queue1-dy-p4d24xlarge-6:22828:22828 [3] NCCL INFO Using network AWS Libfabric
13: queue1-dy-p4d24xlarge-6:22830:22830 [5] NCCL INFO Bootstrap : Using ens32:10.0.19.87<0>
13: queue1-dy-p4d24xlarge-6:22830:22830 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v5 symbol.
13: queue1-dy-p4d24xlarge-6:22830:22830 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.
13: queue1-dy-p4d24xlarge-6:22830:22830 [5] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws
13: queue1-dy-p4d24xlarge-6:22830:22830 [5] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /fsx/conda/envs/fairseq_aws/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml
13: queue1-dy-p4d24xlarge-6:22830:22830 [5] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1
13: queue1-dy-p4d24xlarge-6:22830:22830 [5] NCCL INFO NET/OFI Forcing AWS OFI ndev 4
13: queue1-dy-p4d24xlarge-6:22830:22830 [5] NCCL INFO NET/OFI Selected Provider is efa
13: queue1-dy-p4d24xlarge-6:22830:22830 [5] NCCL INFO Using network AWS Libfabric
 5: queue1-dy-p4d24xlarge-5:26332:26575 [5] NCCL INFO NET/OFI [5] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10
 5: queue1-dy-p4d24xlarge-5:26332:26575 [5] NCCL INFO NET/OFI [5] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10
 5: queue1-dy-p4d24xlarge-5:26332:26575 [5] NCCL INFO NET/OFI [5] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/
 5: queue1-dy-p4d24xlarge-5:26332:26575 [5] NCCL INFO NET/OFI [5] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20
 5: queue1-dy-p4d24xlarge-5:26332:26575 [5] NCCL INFO Setting affinity for GPU 5 to 0fc000,0000000f,c0000000
 6: queue1-dy-p4d24xlarge-5:26333:26570 [6] NCCL INFO NET/OFI [6] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10
 6: queue1-dy-p4d24xlarge-5:26333:26570 [6] NCCL INFO NET/OFI [6] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10
 6: queue1-dy-p4d24xlarge-5:26333:26570 [6] NCCL INFO NET/OFI [6] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20
 6: queue1-dy-p4d24xlarge-5:26333:26570 [6] NCCL INFO NET/OFI [6] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/
15: queue1-dy-p4d24xlarge-6:22832:23064 [7] NCCL INFO NET/OFI [7] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10
15: queue1-dy-p4d24xlarge-6:22832:23064 [7] NCCL INFO NET/OFI [7] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10
15: queue1-dy-p4d24xlarge-6:22832:23064 [7] NCCL INFO NET/OFI [7] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20
15: queue1-dy-p4d24xlarge-6:22832:23064 [7] NCCL INFO NET/OFI [7] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/
 6: queue1-dy-p4d24xlarge-5:26333:26570 [6] NCCL INFO Setting affinity for GPU 6 to 03f00000,000003f0,00000000
15: queue1-dy-p4d24xlarge-6:22832:23064 [7] NCCL INFO Setting affinity for GPU 7 to fc000000,0000fc00,00000000
13: queue1-dy-p4d24xlarge-6:22830:23071 [5] NCCL INFO NET/OFI [5] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10
13: queue1-dy-p4d24xlarge-6:22830:23071 [5] NCCL INFO NET/OFI [5] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10
13: queue1-dy-p4d24xlarge-6:22830:23071 [5] NCCL INFO NET/OFI [5] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/
13: queue1-dy-p4d24xlarge-6:22830:23071 [5] NCCL INFO NET/OFI [5] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20
13: queue1-dy-p4d24xlarge-6:22830:23071 [5] NCCL INFO Setting affinity for GPU 5 to 0fc000,0000000f,c0000000
 7: queue1-dy-p4d24xlarge-5:26334:26571 [7] NCCL INFO NET/OFI [7] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10
 7: queue1-dy-p4d24xlarge-5:26334:26571 [7] NCCL INFO NET/OFI [7] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10
 7: queue1-dy-p4d24xlarge-5:26334:26571 [7] NCCL INFO NET/OFI [7] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20
 7: queue1-dy-p4d24xlarge-5:26334:26571 [7] NCCL INFO NET/OFI [7] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/
 4: queue1-dy-p4d24xlarge-5:26331:26569 [4] NCCL INFO NET/OFI [4] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10
 4: queue1-dy-p4d24xlarge-5:26331:26569 [4] NCCL INFO NET/OFI [4] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10
 4: queue1-dy-p4d24xlarge-5:26331:26569 [4] NCCL INFO NET/OFI [4] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/
 4: queue1-dy-p4d24xlarge-5:26331:26569 [4] NCCL INFO NET/OFI [4] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20
 7: queue1-dy-p4d24xlarge-5:26334:26571 [7] NCCL INFO Setting affinity for GPU 7 to fc000000,0000fc00,00000000
 2: queue1-dy-p4d24xlarge-5:26329:26573 [2] NCCL INFO NET/OFI [2] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10
 2: queue1-dy-p4d24xlarge-5:26329:26573 [2] NCCL INFO NET/OFI [2] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/
 2: queue1-dy-p4d24xlarge-5:26329:26573 [2] NCCL INFO NET/OFI [2] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20
 2: queue1-dy-p4d24xlarge-5:26329:26573 [2] NCCL INFO NET/OFI [2] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20
 3: queue1-dy-p4d24xlarge-5:26330:26572 [3] NCCL INFO NET/OFI [3] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10
 3: queue1-dy-p4d24xlarge-5:26330:26572 [3] NCCL INFO NET/OFI [3] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/
 3: queue1-dy-p4d24xlarge-5:26330:26572 [3] NCCL INFO NET/OFI [3] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20
 3: queue1-dy-p4d24xlarge-5:26330:26572 [3] NCCL INFO NET/OFI [3] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20
 1: queue1-dy-p4d24xlarge-5:26328:26574 [1] NCCL INFO NET/OFI [1] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/
 1: queue1-dy-p4d24xlarge-5:26328:26574 [1] NCCL INFO NET/OFI [1] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10
 1: queue1-dy-p4d24xlarge-5:26328:26574 [1] NCCL INFO NET/OFI [1] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20
 1: queue1-dy-p4d24xlarge-5:26328:26574 [1] NCCL INFO NET/OFI [1] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20
 0: queue1-dy-p4d24xlarge-5:26327:26568 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/
 0: queue1-dy-p4d24xlarge-5:26327:26568 [0] NCCL INFO NET/OFI [0] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10
 0: queue1-dy-p4d24xlarge-5:26327:26568 [0] NCCL INFO NET/OFI [0] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20
 0: queue1-dy-p4d24xlarge-5:26327:26568 [0] NCCL INFO NET/OFI [0] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20
 4: queue1-dy-p4d24xlarge-5:26331:26569 [4] NCCL INFO Setting affinity for GPU 4 to 3f00,00000000,3f000000
 2: queue1-dy-p4d24xlarge-5:26329:26573 [2] NCCL INFO Setting affinity for GPU 2 to 03,f0000000,0003f000
 3: queue1-dy-p4d24xlarge-5:26330:26572 [3] NCCL INFO Setting affinity for GPU 3 to fc,00000000,00fc0000
 1: queue1-dy-p4d24xlarge-5:26328:26574 [1] NCCL INFO Setting affinity for GPU 1 to 0fc00000,00000fc0
 0: queue1-dy-p4d24xlarge-5:26327:26568 [0] NCCL INFO Setting affinity for GPU 0 to 3f0000,0000003f
12: queue1-dy-p4d24xlarge-6:22829:23069 [4] NCCL INFO NET/OFI [4] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10
12: queue1-dy-p4d24xlarge-6:22829:23069 [4] NCCL INFO NET/OFI [4] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10
12: queue1-dy-p4d24xlarge-6:22829:23069 [4] NCCL INFO NET/OFI [4] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/
12: queue1-dy-p4d24xlarge-6:22829:23069 [4] NCCL INFO NET/OFI [4] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20
 8: queue1-dy-p4d24xlarge-6:22825:23068 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/
 8: queue1-dy-p4d24xlarge-6:22825:23068 [0] NCCL INFO NET/OFI [0] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10
 8: queue1-dy-p4d24xlarge-6:22825:23068 [0] NCCL INFO NET/OFI [0] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20
 8: queue1-dy-p4d24xlarge-6:22825:23068 [0] NCCL INFO NET/OFI [0] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20
11: queue1-dy-p4d24xlarge-6:22828:23070 [3] NCCL INFO NET/OFI [3] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10
11: queue1-dy-p4d24xlarge-6:22828:23070 [3] NCCL INFO NET/OFI [3] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/
11: queue1-dy-p4d24xlarge-6:22828:23070 [3] NCCL INFO NET/OFI [3] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20
11: queue1-dy-p4d24xlarge-6:22828:23070 [3] NCCL INFO NET/OFI [3] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20
14: queue1-dy-p4d24xlarge-6:22831:23065 [6] NCCL INFO NET/OFI [6] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10
14: queue1-dy-p4d24xlarge-6:22831:23065 [6] NCCL INFO NET/OFI [6] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10
14: queue1-dy-p4d24xlarge-6:22831:23065 [6] NCCL INFO NET/OFI [6] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20
14: queue1-dy-p4d24xlarge-6:22831:23065 [6] NCCL INFO NET/OFI [6] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/
 9: queue1-dy-p4d24xlarge-6:22826:23066 [1] NCCL INFO NET/OFI [1] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/
 9: queue1-dy-p4d24xlarge-6:22826:23066 [1] NCCL INFO NET/OFI [1] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10
 9: queue1-dy-p4d24xlarge-6:22826:23066 [1] NCCL INFO NET/OFI [1] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20
 9: queue1-dy-p4d24xlarge-6:22826:23066 [1] NCCL INFO NET/OFI [1] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20
10: queue1-dy-p4d24xlarge-6:22827:23067 [2] NCCL INFO NET/OFI [2] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10
10: queue1-dy-p4d24xlarge-6:22827:23067 [2] NCCL INFO NET/OFI [2] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/
10: queue1-dy-p4d24xlarge-6:22827:23067 [2] NCCL INFO NET/OFI [2] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20
10: queue1-dy-p4d24xlarge-6:22827:23067 [2] NCCL INFO NET/OFI [2] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20
12: queue1-dy-p4d24xlarge-6:22829:23069 [4] NCCL INFO Setting affinity for GPU 4 to 3f00,00000000,3f000000
 8: queue1-dy-p4d24xlarge-6:22825:23068 [0] NCCL INFO Setting affinity for GPU 0 to 3f0000,0000003f
11: queue1-dy-p4d24xlarge-6:22828:23070 [3] NCCL INFO Setting affinity for GPU 3 to fc,00000000,00fc0000
14: queue1-dy-p4d24xlarge-6:22831:23065 [6] NCCL INFO Setting affinity for GPU 6 to 03f00000,000003f0,00000000
 9: queue1-dy-p4d24xlarge-6:22826:23066 [1] NCCL INFO Setting affinity for GPU 1 to 0fc00000,00000fc0
10: queue1-dy-p4d24xlarge-6:22827:23067 [2] NCCL INFO Setting affinity for GPU 2 to 03,f0000000,0003f000
10: queue1-dy-p4d24xlarge-6:22827:23067 [2] NCCL INFO Trees [0] 11/-1/-1->10->9 [1] 11/-1/-1->10->2 [2] 11/-1/-1->10->9 [3] 11/2/-1->10->-1
13: queue1-dy-p4d24xlarge-6:22830:23071 [5] NCCL INFO Trees [0] 14/-1/-1->13->12 [1] 14/-1/-1->13->12 [2] 14/-1/-1->13->12 [3] 14/-1/-1->13->12
 6: queue1-dy-p4d24xlarge-5:26333:26570 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 7/-1/-1->6->5 [3] 7/-1/-1->6->5
14: queue1-dy-p4d24xlarge-6:22831:23065 [6] NCCL INFO Trees [0] 15/-1/-1->14->13 [1] 15/-1/-1->14->13 [2] 15/-1/-1->14->13 [3] 15/-1/-1->14->13
 7: queue1-dy-p4d24xlarge-5:26334:26571 [7] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] 0/-1/-1->7->6 [2] -1/-1/-1->7->6 [3] 0/-1/-1->7->6
15: queue1-dy-p4d24xlarge-6:22832:23064 [7] NCCL INFO Trees [0] -1/-1/-1->15->14 [1] 8/-1/-1->15->14 [2] -1/-1/-1->15->14 [3] 8/-1/-1->15->14
 0: queue1-dy-p4d24xlarge-5:26327:26568 [0] NCCL INFO Channel 00/04 :    0   7   6   5   4   3   2   1   8  15  14  13  12  11  10   9
 0: queue1-dy-p4d24xlarge-5:26327:26568 [0] NCCL INFO Channel 01/04 :    0   3  10  15  14  13  12   9   8  11   2   7   6   5   4   1
 8: queue1-dy-p4d24xlarge-6:22825:23068 [0] NCCL INFO Trees [0] 9/-1/-1->8->0 [1] 9/-1/-1->8->15 [2] 9/0/-1->8->-1 [3] 9/-1/-1->8->15
 1: queue1-dy-p4d24xlarge-5:26328:26574 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] -1/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] -1/-1/-1->1->0
 9: queue1-dy-p4d24xlarge-6:22826:23066 [1] NCCL INFO Trees [0] 10/-1/-1->9->8 [1] -1/-1/-1->9->8 [2] 10/-1/-1->9->8 [3] -1/-1/-1->9->8
 2: queue1-dy-p4d24xlarge-5:26329:26573 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/10/-1->2->-1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->10
11: queue1-dy-p4d24xlarge-6:22828:23070 [3] NCCL INFO Trees [0] 12/-1/-1->11->10 [1] 12/-1/-1->11->10 [2] 12/-1/-1->11->10 [3] 12/-1/-1->11->10
 3: queue1-dy-p4d24xlarge-5:26330:26572 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2 [2] 4/-1/-1->3->2 [3] 4/-1/-1->3->2
12: queue1-dy-p4d24xlarge-6:22829:23069 [4] NCCL INFO Trees [0] 13/-1/-1->12->11 [1] 13/-1/-1->12->11 [2] 13/-1/-1->12->11 [3] 13/-1/-1->12->11
 4: queue1-dy-p4d24xlarge-5:26331:26569 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3 [2] 5/-1/-1->4->3 [3] 5/-1/-1->4->3
 5: queue1-dy-p4d24xlarge-5:26332:26575 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4 [2] 6/-1/-1->5->4 [3] 6/-1/-1->5->4
 0: queue1-dy-p4d24xlarge-5:26327:26568 [0] NCCL INFO Channel 02/04 :    0   7   6   5   4   3   2   1   8  15  14  13  12  11  10   9
 0: queue1-dy-p4d24xlarge-5:26327:26568 [0] NCCL INFO Channel 03/04 :    0   3  10  15  14  13  12   9   8  11   2   7   6   5   4   1
 0: queue1-dy-p4d24xlarge-5:26327:26568 [0] NCCL INFO Trees [0] 1/8/-1->0->-1 [1] 1/-1/-1->0->7 [2] 1/-1/-1->0->8 [3] 1/-1/-1->0->7
10: queue1-dy-p4d24xlarge-6:22827:23067 [2] NCCL INFO Channel 01 : 10[201c0] -> 15[a01d0] via P2P/IPC/read
10: queue1-dy-p4d24xlarge-6:22827:23067 [2] NCCL INFO Channel 03 : 10[201c0] -> 15[a01d0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-6:22829:23069 [4] NCCL INFO Channel 01 : 12[901c0] -> 9[101d0] via P2P/IPC/read
 8: queue1-dy-p4d24xlarge-6:22825:23068 [0] NCCL INFO Channel 01 : 8[101c0] -> 11[201d0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-6:22829:23069 [4] NCCL INFO Channel 03 : 12[901c0] -> 9[101d0] via P2P/IPC/read
 8: queue1-dy-p4d24xlarge-6:22825:23068 [0] NCCL INFO Channel 03 : 8[101c0] -> 11[201d0] via P2P/IPC/read
 9: queue1-dy-p4d24xlarge-6:22826:23066 [1] NCCL INFO Channel 00/0 : 9[101d0] -> 0[101c0] [send] via NET/AWS Libfabric/0
 9: queue1-dy-p4d24xlarge-6:22826:23079 [1] NCCL INFO NET/OFI [1] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/
 9: queue1-dy-p4d24xlarge-6:22826:23066 [1] NCCL INFO Channel 02/0 : 9[101d0] -> 0[101c0] [send] via NET/AWS Libfabric/0
 9: queue1-dy-p4d24xlarge-6:22826:23079 [1] NCCL INFO NET/OFI [1] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/
14: queue1-dy-p4d24xlarge-6:22831:23065 [6] NCCL INFO Channel 00 : 14[a01c0] -> 13[901d0] via P2P/IPC/read
13: queue1-dy-p4d24xlarge-6:22830:23071 [5] NCCL INFO Channel 00 : 13[901d0] -> 12[901c0] via P2P/IPC/read
14: queue1-dy-p4d24xlarge-6:22831:23065 [6] NCCL INFO Channel 01 : 14[a01c0] -> 13[901d0] via P2P/IPC/read
13: queue1-dy-p4d24xlarge-6:22830:23071 [5] NCCL INFO Channel 01 : 13[901d0] -> 12[901c0] via P2P/IPC/read
10: queue1-dy-p4d24xlarge-6:22827:23080 [2] NCCL INFO NET/OFI [2] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/
13: queue1-dy-p4d24xlarge-6:22830:23071 [5] NCCL INFO Channel 02 : 13[901d0] -> 12[901c0] via P2P/IPC/read
14: queue1-dy-p4d24xlarge-6:22831:23065 [6] NCCL INFO Channel 02 : 14[a01c0] -> 13[901d0] via P2P/IPC/read
13: queue1-dy-p4d24xlarge-6:22830:23071 [5] NCCL INFO Channel 03 : 13[901d0] -> 12[901c0] via P2P/IPC/read
14: queue1-dy-p4d24xlarge-6:22831:23065 [6] NCCL INFO Channel 03 : 14[a01c0] -> 13[901d0] via P2P/IPC/read
11: queue1-dy-p4d24xlarge-6:22828:23070 [3] NCCL INFO Channel 01/0 : 11[201d0] -> 2[201c0] [send] via NET/AWS Libfabric/1
11: queue1-dy-p4d24xlarge-6:22828:23077 [3] NCCL INFO NET/OFI [3] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/
11: queue1-dy-p4d24xlarge-6:22828:23070 [3] NCCL INFO Channel 03/0 : 11[201d0] -> 2[201c0] [send] via NET/AWS Libfabric/1
11: queue1-dy-p4d24xlarge-6:22828:23077 [3] NCCL INFO NET/OFI [3] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/
 8: queue1-dy-p4d24xlarge-6:22825:23076 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/
10: queue1-dy-p4d24xlarge-6:22827:23067 [2] NCCL INFO Channel 01/0 : 3[201d0] -> 10[201c0] [receive] via NET/AWS Libfabric/1
10: queue1-dy-p4d24xlarge-6:22827:23080 [2] NCCL INFO NET/OFI [2] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/
10: queue1-dy-p4d24xlarge-6:22827:23067 [2] NCCL INFO Channel 03/0 : 3[201d0] -> 10[201c0] [receive] via NET/AWS Libfabric/1
 4: queue1-dy-p4d24xlarge-5:26331:26569 [4] NCCL INFO Channel 01 : 4[901c0] -> 1[101d0] via P2P/IPC/read
 2: queue1-dy-p4d24xlarge-5:26329:26573 [2] NCCL INFO Channel 01 : 2[201c0] -> 7[a01d0] via P2P/IPC/read
 0: queue1-dy-p4d24xlarge-5:26327:26568 [0] NCCL INFO Channel 01 : 0[101c0] -> 3[201d0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-5:26331:26569 [4] NCCL INFO Channel 03 : 4[901c0] -> 1[101d0] via P2P/IPC/read
 2: queue1-dy-p4d24xlarge-5:26329:26573 [2] NCCL INFO Channel 03 : 2[201c0] -> 7[a01d0] via P2P/IPC/read
 0: queue1-dy-p4d24xlarge-5:26327:26568 [0] NCCL INFO Channel 03 : 0[101c0] -> 3[201d0] via P2P/IPC/read
 1: queue1-dy-p4d24xlarge-5:26328:26574 [1] NCCL INFO Channel 00/0 : 1[101d0] -> 8[101c0] [send] via NET/AWS Libfabric/0
 1: queue1-dy-p4d24xlarge-5:26328:26583 [1] NCCL INFO NET/OFI [1] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/
 1: queue1-dy-p4d24xlarge-5:26328:26574 [1] NCCL INFO Channel 02/0 : 1[101d0] -> 8[101c0] [send] via NET/AWS Libfabric/0
 1: queue1-dy-p4d24xlarge-5:26328:26583 [1] NCCL INFO NET/OFI [1] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/
 6: queue1-dy-p4d24xlarge-5:26333:26570 [6] NCCL INFO Channel 00 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
 6: queue1-dy-p4d24xlarge-5:26333:26570 [6] NCCL INFO Channel 01 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
 5: queue1-dy-p4d24xlarge-5:26332:26575 [5] NCCL INFO Channel 00 : 5[901d0] -> 4[901c0] via P2P/IPC/read
 6: queue1-dy-p4d24xlarge-5:26333:26570 [6] NCCL INFO Channel 02 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
 5: queue1-dy-p4d24xlarge-5:26332:26575 [5] NCCL INFO Channel 01 : 5[901d0] -> 4[901c0] via P2P/IPC/read
 6: queue1-dy-p4d24xlarge-5:26333:26570 [6] NCCL INFO Channel 03 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
 5: queue1-dy-p4d24xlarge-5:26332:26575 [5] NCCL INFO Channel 02 : 5[901d0] -> 4[901c0] via P2P/IPC/read
 5: queue1-dy-p4d24xlarge-5:26332:26575 [5] NCCL INFO Channel 03 : 5[901d0] -> 4[901c0] via P2P/IPC/read
 2: queue1-dy-p4d24xlarge-5:26329:26581 [2] NCCL INFO NET/OFI [2] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/
 3: queue1-dy-p4d24xlarge-5:26330:26572 [3] NCCL INFO Channel 01/0 : 3[201d0] -> 10[201c0] [send] via NET/AWS Libfabric/1
 3: queue1-dy-p4d24xlarge-5:26330:26582 [3] NCCL INFO NET/OFI [3] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/
 3: queue1-dy-p4d24xlarge-5:26330:26572 [3] NCCL INFO Channel 03/0 : 3[201d0] -> 10[201c0] [send] via NET/AWS Libfabric/1
 3: queue1-dy-p4d24xlarge-5:26330:26582 [3] NCCL INFO NET/OFI [3] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/
 0: queue1-dy-p4d24xlarge-5:26327:26584 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/
 8: queue1-dy-p4d24xlarge-6:22825:23068 [0] NCCL INFO Channel 00/0 : 1[101d0] -> 8[101c0] [receive] via NET/AWS Libfabric/0
 2: queue1-dy-p4d24xlarge-5:26329:26573 [2] NCCL INFO Channel 01/0 : 11[201d0] -> 2[201c0] [receive] via NET/AWS Libfabric/1
 0: queue1-dy-p4d24xlarge-5:26327:26568 [0] NCCL INFO Channel 00/0 : 9[101d0] -> 0[101c0] [receive] via NET/AWS Libfabric/0
 0: queue1-dy-p4d24xlarge-5:26327:26584 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/
 2: queue1-dy-p4d24xlarge-5:26329:26581 [2] NCCL INFO NET/OFI [2] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/
 0: queue1-dy-p4d24xlarge-5:26327:26568 [0] NCCL INFO Channel 02/0 : 9[101d0] -> 0[101c0] [receive] via NET/AWS Libfabric/0
 2: queue1-dy-p4d24xlarge-5:26329:26573 [2] NCCL INFO Channel 03/0 : 11[201d0] -> 2[201c0] [receive] via NET/AWS Libfabric/1
 8: queue1-dy-p4d24xlarge-6:22825:23076 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/
 0: queue1-dy-p4d24xlarge-5:26327:26568 [0] NCCL INFO Channel 00 : 0[101c0] -> 7[a01d0] via P2P/IPC/read
 8: queue1-dy-p4d24xlarge-6:22825:23068 [0] NCCL INFO Channel 02/0 : 1[101d0] -> 8[101c0] [receive] via NET/AWS Libfabric/0
 8: queue1-dy-p4d24xlarge-6:22825:23068 [0] NCCL INFO Channel 00 : 8[101c0] -> 15[a01d0] via P2P/IPC/read
 0: queue1-dy-p4d24xlarge-5:26327:26568 [0] NCCL INFO Channel 02 : 0[101c0] -> 7[a01d0] via P2P/IPC/read
 8: queue1-dy-p4d24xlarge-6:22825:23068 [0] NCCL INFO Channel 02 : 8[101c0] -> 15[a01d0] via P2P/IPC/read
 7: queue1-dy-p4d24xlarge-5:26334:26571 [7] NCCL INFO Channel 00 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
 7: queue1-dy-p4d24xlarge-5:26334:26571 [7] NCCL INFO Channel 01 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
 7: queue1-dy-p4d24xlarge-5:26334:26571 [7] NCCL INFO Channel 02 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
15: queue1-dy-p4d24xlarge-6:22832:23064 [7] NCCL INFO Channel 00 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read
 7: queue1-dy-p4d24xlarge-5:26334:26571 [7] NCCL INFO Channel 03 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
15: queue1-dy-p4d24xlarge-6:22832:23064 [7] NCCL INFO Channel 01 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read
15: queue1-dy-p4d24xlarge-6:22832:23064 [7] NCCL INFO Channel 02 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read
15: queue1-dy-p4d24xlarge-6:22832:23064 [7] NCCL INFO Channel 03 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read
 7: queue1-dy-p4d24xlarge-5:26334:26571 [7] NCCL INFO Connected all rings
15: queue1-dy-p4d24xlarge-6:22832:23064 [7] NCCL INFO Connected all rings
 6: queue1-dy-p4d24xlarge-5:26333:26570 [6] NCCL INFO Connected all rings
14: queue1-dy-p4d24xlarge-6:22831:23065 [6] NCCL INFO Connected all rings
14: queue1-dy-p4d24xlarge-6:22831:23065 [6] NCCL INFO Channel 00 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read
14: queue1-dy-p4d24xlarge-6:22831:23065 [6] NCCL INFO Channel 01 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read
14: queue1-dy-p4d24xlarge-6:22831:23065 [6] NCCL INFO Channel 02 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read
 6: queue1-dy-p4d24xlarge-5:26333:26570 [6] NCCL INFO Channel 00 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
14: queue1-dy-p4d24xlarge-6:22831:23065 [6] NCCL INFO Channel 03 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read
 6: queue1-dy-p4d24xlarge-5:26333:26570 [6] NCCL INFO Channel 01 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
 6: queue1-dy-p4d24xlarge-5:26333:26570 [6] NCCL INFO Channel 02 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
 6: queue1-dy-p4d24xlarge-5:26333:26570 [6] NCCL INFO Channel 03 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
 3: queue1-dy-p4d24xlarge-5:26330:26572 [3] NCCL INFO Channel 00 : 3[201d0] -> 2[201c0] via P2P/IPC/read
 3: queue1-dy-p4d24xlarge-5:26330:26572 [3] NCCL INFO Channel 02 : 3[201d0] -> 2[201c0] via P2P/IPC/read
10: queue1-dy-p4d24xlarge-6:22827:23067 [2] NCCL INFO Channel 00 : 10[201c0] -> 9[101d0] via P2P/IPC/read
10: queue1-dy-p4d24xlarge-6:22827:23067 [2] NCCL INFO Channel 02 : 10[201c0] -> 9[101d0] via P2P/IPC/read
11: queue1-dy-p4d24xlarge-6:22828:23070 [3] NCCL INFO Channel 00 : 11[201d0] -> 10[201c0] via P2P/IPC/read
 1: queue1-dy-p4d24xlarge-5:26328:26574 [1] NCCL INFO Channel 01 : 1[101d0] -> 0[101c0] via P2P/IPC/read
 1: queue1-dy-p4d24xlarge-5:26328:26574 [1] NCCL INFO Channel 03 : 1[101d0] -> 0[101c0] via P2P/IPC/read
11: queue1-dy-p4d24xlarge-6:22828:23070 [3] NCCL INFO Channel 02 : 11[201d0] -> 10[201c0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-5:26331:26569 [4] NCCL INFO Channel 00 : 4[901c0] -> 3[201d0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-5:26331:26569 [4] NCCL INFO Channel 02 : 4[901c0] -> 3[201d0] via P2P/IPC/read
 9: queue1-dy-p4d24xlarge-6:22826:23066 [1] NCCL INFO Channel 01 : 9[101d0] -> 8[101c0] via P2P/IPC/read
 9: queue1-dy-p4d24xlarge-6:22826:23066 [1] NCCL INFO Channel 03 : 9[101d0] -> 8[101c0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-6:22829:23069 [4] NCCL INFO Channel 00 : 12[901c0] -> 11[201d0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-6:22829:23069 [4] NCCL INFO Channel 02 : 12[901c0] -> 11[201d0] via P2P/IPC/read
10: queue1-dy-p4d24xlarge-6:22827:23067 [2] NCCL INFO Connected all rings
10: queue1-dy-p4d24xlarge-6:22827:23067 [2] NCCL INFO Channel 00 : 10[201c0] -> 11[201d0] via P2P/IPC/read
10: queue1-dy-p4d24xlarge-6:22827:23067 [2] NCCL INFO Channel 01 : 10[201c0] -> 11[201d0] via P2P/IPC/read
10: queue1-dy-p4d24xlarge-6:22827:23067 [2] NCCL INFO Channel 02 : 10[201c0] -> 11[201d0] via P2P/IPC/read
10: queue1-dy-p4d24xlarge-6:22827:23067 [2] NCCL INFO Channel 03 : 10[201c0] -> 11[201d0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-5:26331:26569 [4] NCCL INFO Connected all rings
 5: queue1-dy-p4d24xlarge-5:26332:26575 [5] NCCL INFO Connected all rings
 5: queue1-dy-p4d24xlarge-5:26332:26575 [5] NCCL INFO Channel 00 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
 5: queue1-dy-p4d24xlarge-5:26332:26575 [5] NCCL INFO Channel 01 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
 5: queue1-dy-p4d24xlarge-5:26332:26575 [5] NCCL INFO Channel 02 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
 5: queue1-dy-p4d24xlarge-5:26332:26575 [5] NCCL INFO Channel 03 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
11: queue1-dy-p4d24xlarge-6:22828:23070 [3] NCCL INFO Connected all rings
12: queue1-dy-p4d24xlarge-6:22829:23069 [4] NCCL INFO Connected all rings
 4: queue1-dy-p4d24xlarge-5:26331:26569 [4] NCCL INFO Channel 00 : 4[901c0] -> 5[901d0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-5:26331:26569 [4] NCCL INFO Channel 01 : 4[901c0] -> 5[901d0] via P2P/IPC/read
 6: queue1-dy-p4d24xlarge-5:26333:26570 [6] NCCL INFO Connected all trees
 6: queue1-dy-p4d24xlarge-5:26333:26570 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
 6: queue1-dy-p4d24xlarge-5:26333:26570 [6] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
 4: queue1-dy-p4d24xlarge-5:26331:26569 [4] NCCL INFO Channel 02 : 4[901c0] -> 5[901d0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-5:26331:26569 [4] NCCL INFO Channel 03 : 4[901c0] -> 5[901d0] via P2P/IPC/read
11: queue1-dy-p4d24xlarge-6:22828:23070 [3] NCCL INFO Channel 00 : 11[201d0] -> 12[901c0] via P2P/IPC/read
 2: queue1-dy-p4d24xlarge-5:26329:26573 [2] NCCL INFO Channel 00 : 2[201c0] -> 1[101d0] via P2P/IPC/read
13: queue1-dy-p4d24xlarge-6:22830:23071 [5] NCCL INFO Connected all rings
 8: queue1-dy-p4d24xlarge-6:22825:23068 [0] NCCL INFO Connected all rings
 8: queue1-dy-p4d24xlarge-6:22825:23068 [0] NCCL INFO Channel 00 : 8[101c0] -> 9[101d0] via P2P/IPC/read
 2: queue1-dy-p4d24xlarge-5:26329:26573 [2] NCCL INFO Channel 02 : 2[201c0] -> 1[101d0] via P2P/IPC/read
11: queue1-dy-p4d24xlarge-6:22828:23070 [3] NCCL INFO Channel 01 : 11[201d0] -> 12[901c0] via P2P/IPC/read
 5: queue1-dy-p4d24xlarge-5:26332:26575 [5] NCCL INFO Connected all trees
12: queue1-dy-p4d24xlarge-6:22829:23069 [4] NCCL INFO Channel 00 : 12[901c0] -> 13[901d0] via P2P/IPC/read
 5: queue1-dy-p4d24xlarge-5:26332:26575 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
 5: queue1-dy-p4d24xlarge-5:26332:26575 [5] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
 8: queue1-dy-p4d24xlarge-6:22825:23068 [0] NCCL INFO Channel 01 : 8[101c0] -> 9[101d0] via P2P/IPC/read
 0: queue1-dy-p4d24xlarge-5:26327:26568 [0] NCCL INFO Connected all rings
 0: queue1-dy-p4d24xlarge-5:26327:26568 [0] NCCL INFO Channel 00 : 0[101c0] -> 1[101d0] via P2P/IPC/read
 9: queue1-dy-p4d24xlarge-6:22826:23066 [1] NCCL INFO Connected all rings
11: queue1-dy-p4d24xlarge-6:22828:23070 [3] NCCL INFO Channel 02 : 11[201d0] -> 12[901c0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-6:22829:23069 [4] NCCL INFO Channel 01 : 12[901c0] -> 13[901d0] via P2P/IPC/read
 8: queue1-dy-p4d24xlarge-6:22825:23068 [0] NCCL INFO Channel 02 : 8[101c0] -> 9[101d0] via P2P/IPC/read
11: queue1-dy-p4d24xlarge-6:22828:23070 [3] NCCL INFO Channel 03 : 11[201d0] -> 12[901c0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-6:22829:23069 [4] NCCL INFO Channel 02 : 12[901c0] -> 13[901d0] via P2P/IPC/read
 8: queue1-dy-p4d24xlarge-6:22825:23068 [0] NCCL INFO Channel 03 : 8[101c0] -> 9[101d0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-6:22829:23069 [4] NCCL INFO Channel 03 : 12[901c0] -> 13[901d0] via P2P/IPC/read
 0: queue1-dy-p4d24xlarge-5:26327:26568 [0] NCCL INFO Channel 01 : 0[101c0] -> 1[101d0] via P2P/IPC/read
13: queue1-dy-p4d24xlarge-6:22830:23071 [5] NCCL INFO Channel 00 : 13[901d0] -> 14[a01c0] via P2P/IPC/read
13: queue1-dy-p4d24xlarge-6:22830:23071 [5] NCCL INFO Channel 01 : 13[901d0] -> 14[a01c0] via P2P/IPC/read
 9: queue1-dy-p4d24xlarge-6:22826:23066 [1] NCCL INFO Channel 00 : 9[101d0] -> 10[201c0] via P2P/IPC/read
13: queue1-dy-p4d24xlarge-6:22830:23071 [5] NCCL INFO Channel 02 : 13[901d0] -> 14[a01c0] via P2P/IPC/read
 9: queue1-dy-p4d24xlarge-6:22826:23066 [1] NCCL INFO Channel 02 : 9[101d0] -> 10[201c0] via P2P/IPC/read
 0: queue1-dy-p4d24xlarge-5:26327:26568 [0] NCCL INFO Channel 02 : 0[101c0] -> 1[101d0] via P2P/IPC/read
13: queue1-dy-p4d24xlarge-6:22830:23071 [5] NCCL INFO Channel 03 : 13[901d0] -> 14[a01c0] via P2P/IPC/read
 3: queue1-dy-p4d24xlarge-5:26330:26572 [3] NCCL INFO Connected all rings
 1: queue1-dy-p4d24xlarge-5:26328:26574 [1] NCCL INFO Connected all rings
 0: queue1-dy-p4d24xlarge-5:26327:26568 [0] NCCL INFO Channel 03 : 0[101c0] -> 1[101d0] via P2P/IPC/read
 2: queue1-dy-p4d24xlarge-5:26329:26573 [2] NCCL INFO Connected all rings
 2: queue1-dy-p4d24xlarge-5:26329:26573 [2] NCCL INFO Channel 00 : 2[201c0] -> 3[201d0] via P2P/IPC/read
 2: queue1-dy-p4d24xlarge-5:26329:26573 [2] NCCL INFO Channel 01 : 2[201c0] -> 3[201d0] via P2P/IPC/read
 1: queue1-dy-p4d24xlarge-5:26328:26574 [1] NCCL INFO Channel 00 : 1[101d0] -> 2[201c0] via P2P/IPC/read
 3: queue1-dy-p4d24xlarge-5:26330:26572 [3] NCCL INFO Channel 00 : 3[201d0] -> 4[901c0] via P2P/IPC/read
 2: queue1-dy-p4d24xlarge-5:26329:26573 [2] NCCL INFO Channel 02 : 2[201c0] -> 3[201d0] via P2P/IPC/read
 1: queue1-dy-p4d24xlarge-5:26328:26574 [1] NCCL INFO Channel 02 : 1[101d0] -> 2[201c0] via P2P/IPC/read
 2: queue1-dy-p4d24xlarge-5:26329:26573 [2] NCCL INFO Channel 03 : 2[201c0] -> 3[201d0] via P2P/IPC/read
 3: queue1-dy-p4d24xlarge-5:26330:26572 [3] NCCL INFO Channel 01 : 3[201d0] -> 4[901c0] via P2P/IPC/read
 0: queue1-dy-p4d24xlarge-5:26327:26568 [0] NCCL INFO Channel 01 : 0[101c0] -> 7[a01d0] via P2P/IPC/read
 3: queue1-dy-p4d24xlarge-5:26330:26572 [3] NCCL INFO Channel 02 : 3[201d0] -> 4[901c0] via P2P/IPC/read
 0: queue1-dy-p4d24xlarge-5:26327:26568 [0] NCCL INFO Channel 03 : 0[101c0] -> 7[a01d0] via P2P/IPC/read
 1: queue1-dy-p4d24xlarge-5:26328:26574 [1] NCCL INFO Channel 00 : 1[101d0] -> 0[101c0] via P2P/IPC/read
 3: queue1-dy-p4d24xlarge-5:26330:26572 [3] NCCL INFO Channel 03 : 3[201d0] -> 4[901c0] via P2P/IPC/read
 1: queue1-dy-p4d24xlarge-5:26328:26574 [1] NCCL INFO Channel 02 : 1[101d0] -> 0[101c0] via P2P/IPC/read
 7: queue1-dy-p4d24xlarge-5:26334:26571 [7] NCCL INFO Channel 01 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
 7: queue1-dy-p4d24xlarge-5:26334:26571 [7] NCCL INFO Channel 03 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
 8: queue1-dy-p4d24xlarge-6:22825:23068 [0] NCCL INFO Channel 01 : 8[101c0] -> 15[a01d0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-5:26331:26569 [4] NCCL INFO Channel 01 : 4[901c0] -> 3[201d0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-5:26331:26569 [4] NCCL INFO Channel 03 : 4[901c0] -> 3[201d0] via P2P/IPC/read
 3: queue1-dy-p4d24xlarge-5:26330:26572 [3] NCCL INFO Channel 01 : 3[201d0] -> 2[201c0] via P2P/IPC/read
 3: queue1-dy-p4d24xlarge-5:26330:26572 [3] NCCL INFO Channel 03 : 3[201d0] -> 2[201c0] via P2P/IPC/read
 8: queue1-dy-p4d24xlarge-6:22825:23068 [0] NCCL INFO Channel 03 : 8[101c0] -> 15[a01d0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-5:26331:26569 [4] NCCL INFO Connected all trees
 4: queue1-dy-p4d24xlarge-5:26331:26569 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
 4: queue1-dy-p4d24xlarge-5:26331:26569 [4] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
 9: queue1-dy-p4d24xlarge-6:22826:23066 [1] NCCL INFO Channel 00 : 9[101d0] -> 8[101c0] via P2P/IPC/read
11: queue1-dy-p4d24xlarge-6:22828:23070 [3] NCCL INFO Channel 01 : 11[201d0] -> 10[201c0] via P2P/IPC/read
 9: queue1-dy-p4d24xlarge-6:22826:23066 [1] NCCL INFO Channel 02 : 9[101d0] -> 8[101c0] via P2P/IPC/read
11: queue1-dy-p4d24xlarge-6:22828:23070 [3] NCCL INFO Channel 03 : 11[201d0] -> 10[201c0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-6:22829:23069 [4] NCCL INFO Channel 01 : 12[901c0] -> 11[201d0] via P2P/IPC/read
14: queue1-dy-p4d24xlarge-6:22831:23065 [6] NCCL INFO Connected all trees
14: queue1-dy-p4d24xlarge-6:22831:23065 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
14: queue1-dy-p4d24xlarge-6:22831:23065 [6] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
13: queue1-dy-p4d24xlarge-6:22830:23071 [5] NCCL INFO Connected all trees
13: queue1-dy-p4d24xlarge-6:22830:23071 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
13: queue1-dy-p4d24xlarge-6:22830:23071 [5] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
15: queue1-dy-p4d24xlarge-6:22832:23064 [7] NCCL INFO Channel 01 : 15[a01d0] -> 8[101c0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-6:22829:23069 [4] NCCL INFO Channel 03 : 12[901c0] -> 11[201d0] via P2P/IPC/read
15: queue1-dy-p4d24xlarge-6:22832:23064 [7] NCCL INFO Channel 03 : 15[a01d0] -> 8[101c0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-6:22829:23069 [4] NCCL INFO Connected all trees
12: queue1-dy-p4d24xlarge-6:22829:23069 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
12: queue1-dy-p4d24xlarge-6:22829:23069 [4] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
 0: queue1-dy-p4d24xlarge-5:26327:26584 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/
 0: queue1-dy-p4d24xlarge-5:26327:26568 [0] NCCL INFO Channel 00/0 : 8[101c0] -> 0[101c0] [receive] via NET/AWS Libfabric/0
 0: queue1-dy-p4d24xlarge-5:26327:26584 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/
 0: queue1-dy-p4d24xlarge-5:26327:26568 [0] NCCL INFO Channel 02/0 : 8[101c0] -> 0[101c0] [receive] via NET/AWS Libfabric/0
 0: queue1-dy-p4d24xlarge-5:26327:26568 [0] NCCL INFO Channel 00/0 : 0[101c0] -> 8[101c0] [send] via NET/AWS Libfabric/0
 0: queue1-dy-p4d24xlarge-5:26327:26584 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/
 0: queue1-dy-p4d24xlarge-5:26327:26568 [0] NCCL INFO Channel 02/0 : 0[101c0] -> 8[101c0] [send] via NET/AWS Libfabric/0
 0: queue1-dy-p4d24xlarge-5:26327:26584 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/
 2: queue1-dy-p4d24xlarge-5:26329:26581 [2] NCCL INFO NET/OFI [2] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/
 2: queue1-dy-p4d24xlarge-5:26329:26573 [2] NCCL INFO Channel 01/0 : 10[201c0] -> 2[201c0] [receive] via NET/AWS Libfabric/1
 2: queue1-dy-p4d24xlarge-5:26329:26581 [2] NCCL INFO NET/OFI [2] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/
 2: queue1-dy-p4d24xlarge-5:26329:26573 [2] NCCL INFO Channel 03/0 : 10[201c0] -> 2[201c0] [receive] via NET/AWS Libfabric/1
 2: queue1-dy-p4d24xlarge-5:26329:26573 [2] NCCL INFO Channel 01/0 : 2[201c0] -> 10[201c0] [send] via NET/AWS Libfabric/1
 2: queue1-dy-p4d24xlarge-5:26329:26581 [2] NCCL INFO NET/OFI [2] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/
 2: queue1-dy-p4d24xlarge-5:26329:26573 [2] NCCL INFO Channel 03/0 : 2[201c0] -> 10[201c0] [send] via NET/AWS Libfabric/1
 2: queue1-dy-p4d24xlarge-5:26329:26581 [2] NCCL INFO NET/OFI [2] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/
10: queue1-dy-p4d24xlarge-6:22827:23080 [2] NCCL INFO NET/OFI [2] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/
10: queue1-dy-p4d24xlarge-6:22827:23067 [2] NCCL INFO Channel 01/0 : 2[201c0] -> 10[201c0] [receive] via NET/AWS Libfabric/1
10: queue1-dy-p4d24xlarge-6:22827:23080 [2] NCCL INFO NET/OFI [2] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/
10: queue1-dy-p4d24xlarge-6:22827:23067 [2] NCCL INFO Channel 03/0 : 2[201c0] -> 10[201c0] [receive] via NET/AWS Libfabric/1
10: queue1-dy-p4d24xlarge-6:22827:23067 [2] NCCL INFO Channel 01/0 : 10[201c0] -> 2[201c0] [send] via NET/AWS Libfabric/1
10: queue1-dy-p4d24xlarge-6:22827:23080 [2] NCCL INFO NET/OFI [2] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/
10: queue1-dy-p4d24xlarge-6:22827:23067 [2] NCCL INFO Channel 03/0 : 10[201c0] -> 2[201c0] [send] via NET/AWS Libfabric/1
10: queue1-dy-p4d24xlarge-6:22827:23080 [2] NCCL INFO NET/OFI [2] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/
 8: queue1-dy-p4d24xlarge-6:22825:23076 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/
 8: queue1-dy-p4d24xlarge-6:22825:23068 [0] NCCL INFO Channel 00/0 : 0[101c0] -> 8[101c0] [receive] via NET/AWS Libfabric/0
 8: queue1-dy-p4d24xlarge-6:22825:23076 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/
 8: queue1-dy-p4d24xlarge-6:22825:23068 [0] NCCL INFO Channel 02/0 : 0[101c0] -> 8[101c0] [receive] via NET/AWS Libfabric/0
 8: queue1-dy-p4d24xlarge-6:22825:23068 [0] NCCL INFO Channel 00/0 : 8[101c0] -> 0[101c0] [send] via NET/AWS Libfabric/0
 8: queue1-dy-p4d24xlarge-6:22825:23076 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/
 8: queue1-dy-p4d24xlarge-6:22825:23068 [0] NCCL INFO Channel 02/0 : 8[101c0] -> 0[101c0] [send] via NET/AWS Libfabric/0
 8: queue1-dy-p4d24xlarge-6:22825:23076 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/
 2: queue1-dy-p4d24xlarge-5:26329:26573 [2] NCCL INFO Connected all trees
 2: queue1-dy-p4d24xlarge-5:26329:26573 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
 2: queue1-dy-p4d24xlarge-5:26329:26573 [2] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
 3: queue1-dy-p4d24xlarge-5:26330:26572 [3] NCCL INFO Connected all trees
 3: queue1-dy-p4d24xlarge-5:26330:26572 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
 3: queue1-dy-p4d24xlarge-5:26330:26572 [3] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
10: queue1-dy-p4d24xlarge-6:22827:23067 [2] NCCL INFO Connected all trees
10: queue1-dy-p4d24xlarge-6:22827:23067 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
10: queue1-dy-p4d24xlarge-6:22827:23067 [2] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
11: queue1-dy-p4d24xlarge-6:22828:23070 [3] NCCL INFO Connected all trees
11: queue1-dy-p4d24xlarge-6:22828:23070 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
11: queue1-dy-p4d24xlarge-6:22828:23070 [3] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
15: queue1-dy-p4d24xlarge-6:22832:23064 [7] NCCL INFO Connected all trees
15: queue1-dy-p4d24xlarge-6:22832:23064 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
15: queue1-dy-p4d24xlarge-6:22832:23064 [7] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
 8: queue1-dy-p4d24xlarge-6:22825:23068 [0] NCCL INFO Connected all trees
 8: queue1-dy-p4d24xlarge-6:22825:23068 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
 8: queue1-dy-p4d24xlarge-6:22825:23068 [0] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
 9: queue1-dy-p4d24xlarge-6:22826:23066 [1] NCCL INFO Connected all trees
 9: queue1-dy-p4d24xlarge-6:22826:23066 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
 9: queue1-dy-p4d24xlarge-6:22826:23066 [1] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
 8: queue1-dy-p4d24xlarge-6:22825:23068 [0] NCCL INFO comm 0x7fc404002fb0 rank 8 nranks 16 cudaDev 0 busId 101c0 - Init COMPLETE
 9: queue1-dy-p4d24xlarge-6:22826:23066 [1] NCCL INFO comm 0x7f6898002fb0 rank 9 nranks 16 cudaDev 1 busId 101d0 - Init COMPLETE
10: queue1-dy-p4d24xlarge-6:22827:23067 [2] NCCL INFO comm 0x7f7db0002fb0 rank 10 nranks 16 cudaDev 2 busId 201c0 - Init COMPLETE
11: queue1-dy-p4d24xlarge-6:22828:23070 [3] NCCL INFO comm 0x7f8c8c002fb0 rank 11 nranks 16 cudaDev 3 busId 201d0 - Init COMPLETE
13: queue1-dy-p4d24xlarge-6:22830:23071 [5] NCCL INFO comm 0x7fad00002fb0 rank 13 nranks 16 cudaDev 5 busId 901d0 - Init COMPLETE
15: queue1-dy-p4d24xlarge-6:22832:23064 [7] NCCL INFO comm 0x7f4d48002fb0 rank 15 nranks 16 cudaDev 7 busId a01d0 - Init COMPLETE
12: queue1-dy-p4d24xlarge-6:22829:23069 [4] NCCL INFO comm 0x7ff6d4002fb0 rank 12 nranks 16 cudaDev 4 busId 901c0 - Init COMPLETE
14: queue1-dy-p4d24xlarge-6:22831:23065 [6] NCCL INFO comm 0x7fdfbc002fb0 rank 14 nranks 16 cudaDev 6 busId a01c0 - Init COMPLETE
 7: queue1-dy-p4d24xlarge-5:26334:26571 [7] NCCL INFO Connected all trees
 7: queue1-dy-p4d24xlarge-5:26334:26571 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
 7: queue1-dy-p4d24xlarge-5:26334:26571 [7] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
 0: queue1-dy-p4d24xlarge-5:26327:26568 [0] NCCL INFO Connected all trees
 0: queue1-dy-p4d24xlarge-5:26327:26568 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
 0: queue1-dy-p4d24xlarge-5:26327:26568 [0] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
 1: queue1-dy-p4d24xlarge-5:26328:26574 [1] NCCL INFO Connected all trees
 1: queue1-dy-p4d24xlarge-5:26328:26574 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
 1: queue1-dy-p4d24xlarge-5:26328:26574 [1] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
 0: queue1-dy-p4d24xlarge-5:26327:26568 [0] NCCL INFO comm 0x7f2a70002fb0 rank 0 nranks 16 cudaDev 0 busId 101c0 - Init COMPLETE
 1: queue1-dy-p4d24xlarge-5:26328:26574 [1] NCCL INFO comm 0x7f1d08002fb0 rank 1 nranks 16 cudaDev 1 busId 101d0 - Init COMPLETE
 3: queue1-dy-p4d24xlarge-5:26330:26572 [3] NCCL INFO comm 0x7fe6f0002fb0 rank 3 nranks 16 cudaDev 3 busId 201d0 - Init COMPLETE
 2: queue1-dy-p4d24xlarge-5:26329:26573 [2] NCCL INFO comm 0x7fea0c002fb0 rank 2 nranks 16 cudaDev 2 busId 201c0 - Init COMPLETE
 4: queue1-dy-p4d24xlarge-5:26331:26569 [4] NCCL INFO comm 0x7f6ed4002fb0 rank 4 nranks 16 cudaDev 4 busId 901c0 - Init COMPLETE
 5: queue1-dy-p4d24xlarge-5:26332:26575 [5] NCCL INFO comm 0x7f48d4002fb0 rank 5 nranks 16 cudaDev 5 busId 901d0 - Init COMPLETE
 6: queue1-dy-p4d24xlarge-5:26333:26570 [6] NCCL INFO comm 0x7feecc002fb0 rank 6 nranks 16 cudaDev 6 busId a01c0 - Init COMPLETE
 7: queue1-dy-p4d24xlarge-5:26334:26571 [7] NCCL INFO comm 0x7f5aa0002fb0 rank 7 nranks 16 cudaDev 7 busId a01d0 - Init COMPLETE
 0: queue1-dy-p4d24xlarge-5:26327:26327 [0] NCCL INFO Launch mode Parallel
 0: 2022-11-03 06:12:10 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.1, activation_fn='silu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=True, adaptive_input_cutoff='20000,60000', adaptive_input_factor=4, adaptive_softmax_cutoff='20000,60000', adaptive_softmax_dropout=0.2, adaptive_softmax_factor=4, add_bos_token=False, all_gather_list_size=16384, arch='mega_lm_adaptive_big', attention_activation_fn='softmax', attention_dropout=0.1, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', clip_mode='total', clip_norm=0.25, cpu=False, criterion='adaptive_loss', curriculum=0, data='/fsx/datasets/wikitext-103', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', decoder_chunk_size=1024, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_hidden_dim=2048, decoder_input_dim=1024, decoder_layers=16, decoder_n_dim=16, decoder_z_dim=256, device_id=0, disable_validation=False, distributed_b
 0: ackend='nccl', distributed_init_method='env://', distributed_no_spawn=True, distributed_port=-1, distributed_rank=0, distributed_world_size=16, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, end_learning_rate=0.0, fast_stat_sync=False, feature_dropout=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, future_target=False, hidden_dropout=0.1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, localsgd_frequency=3, log_format='simple', log_interval=100, lr=[0.005], lr_scheduler='linear_decay', max_epoch=0, max_sentences=None, max_sentences_valid=None, max_target_positions=8096, max_tokens=6144, max_tokens_valid=6144, max_update=400000, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_affine_final_norm=True
 0: , no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, normalization_type='layernorm', normalize_before=True, normalize_embedding=False, nprocs_per_node=8, num_workers=0, optimizer='adam', optimizer_overrides='{}', output_dictionary_size=-1, past_target=False, patience=-1, profile=False, quantization_config_path=None, rel_pos_bias='rotary', report_ema_alpha=False, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sample_break_mode='complete', save_dir='saved_models/cu116_oss_1121', save_interval=1, save_interval_updates=0, seed=42, self_target=False, sentence_avg=False, share_all_embeddings=False, share_decoder_input_output_embed=False, shorten_data_split_list='', shorten_method='none', skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop
 0: _min_lr=-1, stop_time_hours=0, task='language_modeling', tensorboard_logdir='', test_subset='test', threshold_loss_scale=None, tie_adaptive_proj=True, tie_adaptive_weights=True, tokenizer=None, tokens_per_sample=2048, total_num_update=400000, tpu=False, train_subset='train', truncation_length=8192, update_freq=[1], use_bmuf=False, use_old_adam=False, user_dir=None, valid_block='splits:10', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, variant_block_multiple_max=6, variant_block_multiple_min=2, wandb_entity=None, wandb_id=None, wandb_project=None, warmup_init_lr=1e-07, warmup_power=1, warmup_updates=24000, weight_decay=0.1, write_out_alpha=False)
 0: 2022-11-03 06:12:10 | INFO | fairseq.tasks.language_modeling | dictionary: 267744 types
 0: 2022-11-03 06:12:10 | INFO | fairseq.data.data_utils | loaded 3760 examples from: /fsx/datasets/wikitext-103/valid
 0: 2022-11-03 06:12:14 | INFO | fairseq_cli.train | MegaLanguageModel(
 0:   (decoder): MegaDecoderNoCrossAttn(
 0:     (embedding_dropout): FairseqDropout(p=0.3)
 0:     (embed_tokens): AdaptiveInput(
 0:       (embeddings): ModuleList(
 0:         (0): Sequential(
 0:           (0): Embedding(20000, 1024, padding_idx=1)
 0:           (1): Linear(in_features=1024, out_features=1024, bias=False)
 0:         )
 0:         (1): Sequential(
 0:           (0): Embedding(40000, 256)
 0:           (1): Linear(in_features=256, out_features=1024, bias=False)
 0:         )
 0:         (2): Sequential(
 0:           (0): Embedding(207744, 64)
 0:           (1): Linear(in_features=64, out_features=1024, bias=False)
 0:         )
 0:       )
 0:     )
 0:     (layers): ModuleList(
 0:       (0): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:       (1): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:       (2): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:       (3): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:       (4): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:       (5): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:       (6): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:       (7): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:       (8): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:       (9): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:       (10): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:       (11): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:       (12): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:       (13): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:       (14): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:       (15): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:     )
 0:     (final_norm): SequenceNorm(
 0:       (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=False)
 0:     )
 0:     (adaptive_softmax): AdaptiveSoftmax(
 0:       (dropout_module): FairseqDropout(p=0.2)
 0:       (lsm): LogSoftmax(dim=1)
 0:       (head): TiedHeadModule(
 0:         (word_proj): TiedLinear()
 0:         (class_proj): Linear(in_features=1024, out_features=2, bias=False)
 0:       )
 0:       (tail): ModuleList(
 0:         (0): Sequential(
 0:           (0): TiedLinear()
 0:           (1): Dropout(p=0.2, inplace=False)
 0:           (2): TiedLinear()
 0:         )
 0:         (1): Sequential(
 0:           (0): TiedLinear()
 0:           (1): Dropout(p=0.2, inplace=False)
 0:           (2): TiedLinear()
 0:         )
 0:       )
 0:     )
 0:   )
 0: )
 0: 2022-11-03 06:12:14 | INFO | fairseq_cli.train | task: language_modeling (LanguageModelingTask)
 0: 2022-11-03 06:12:14 | INFO | fairseq_cli.train | model: mega_lm_adaptive_big (MegaLanguageModel)
 0: 2022-11-03 06:12:14 | INFO | fairseq_cli.train | criterion: adaptive_loss (AdaptiveLoss)
 0: 2022-11-03 06:12:14 | INFO | fairseq_cli.train | num. model params: 252237824 (num. trained: 252237824)
 0: 2022-11-03 06:12:15 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.embeddings.0.0.weight <- decoder.adaptive_softmax.head.word_proj.weight
 0: 2022-11-03 06:12:15 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.embeddings.0.1.bias <- decoder.embed_tokens.embeddings.1.1.bias
 0: 2022-11-03 06:12:15 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.embeddings.0.1.bias <- decoder.embed_tokens.embeddings.2.1.bias
 0: 2022-11-03 06:12:15 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.embeddings.0.1.bias <- decoder.final_norm.norm.weight
 0: 2022-11-03 06:12:15 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.embeddings.0.1.bias <- decoder.final_norm.norm.bias
 0: 2022-11-03 06:12:15 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.embeddings.0.1.bias <- decoder.adaptive_softmax.head.class_proj.bias
 0: 2022-11-03 06:12:15 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.embeddings.1.0.weight <- decoder.adaptive_softmax.tail.0.2.weight
 0: 2022-11-03 06:12:15 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.embeddings.1.1.weight <- decoder.adaptive_softmax.tail.0.0.weight
 0: 2022-11-03 06:12:15 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.embeddings.2.0.weight <- decoder.adaptive_softmax.tail.1.2.weight
 0: 2022-11-03 06:12:15 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.embeddings.2.1.weight <- decoder.adaptive_softmax.tail.1.0.weight
 0: 2022-11-03 06:12:15 | INFO | fairseq.utils | ***********************CUDA enviroments for all 16 workers***********************
 0: 2022-11-03 06:12:15 | INFO | fairseq.utils | rank   0: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-03 06:12:15 | INFO | fairseq.utils | rank   1: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-03 06:12:15 | INFO | fairseq.utils | rank   2: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-03 06:12:15 | INFO | fairseq.utils | rank   3: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-03 06:12:15 | INFO | fairseq.utils | rank   4: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-03 06:12:15 | INFO | fairseq.utils | rank   5: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-03 06:12:15 | INFO | fairseq.utils | rank   6: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-03 06:12:15 | INFO | fairseq.utils | rank   7: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-03 06:12:15 | INFO | fairseq.utils | rank   8: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-03 06:12:15 | INFO | fairseq.utils | rank   9: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-03 06:12:15 | INFO | fairseq.utils | rank  10: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-03 06:12:15 | INFO | fairseq.utils | rank  11: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-03 06:12:15 | INFO | fairseq.utils | rank  12: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-03 06:12:15 | INFO | fairseq.utils | rank  13: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-03 06:12:15 | INFO | fairseq.utils | rank  14: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-03 06:12:15 | INFO | fairseq.utils | rank  15: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-03 06:12:15 | INFO | fairseq.utils | ***********************CUDA enviroments for all 16 workers***********************
 0: 2022-11-03 06:12:15 | INFO | fairseq_cli.train | training on 16 devices (GPUs/TPUs)
 0: 2022-11-03 06:12:15 | INFO | fairseq_cli.train | max tokens per GPU = 6144 and max sentences per GPU = None
 0: 2022-11-03 06:12:15 | INFO | fairseq.trainer | no existing checkpoint found saved_models/cu116_oss_1121/checkpoint_last.pt
 0: 2022-11-03 06:12:15 | INFO | fairseq.trainer | loading train data for epoch 1
 0: 2022-11-03 06:12:16 | INFO | fairseq.data.data_utils | loaded 1801350 examples from: /fsx/datasets/wikitext-103/train
 0: 2022-11-03 06:12:16 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16
 0: 2022-11-03 06:12:16 | INFO | fairseq.optim.adam | using FusedAdam
 0: 2022-11-03 06:12:16 | INFO | fairseq.trainer | begin training epoch 1
 1: 2022-11-03 06:13:05 | INFO | fairseq.distributed_utils | distributed init (rank 1): env://
 4: 2022-11-03 06:13:05 | INFO | fairseq.distributed_utils | distributed init (rank 4): env://
 6: 2022-11-03 06:13:05 | INFO | fairseq.distributed_utils | distributed init (rank 6): env://
 7: 2022-11-03 06:13:05 | INFO | fairseq.distributed_utils | distributed init (rank 7): env://
 2: 2022-11-03 06:13:05 | INFO | fairseq.distributed_utils | distributed init (rank 2): env://
 0: 2022-11-03 06:13:05 | INFO | fairseq.distributed_utils | distributed init (rank 0): env://
 3: 2022-11-03 06:13:05 | INFO | fairseq.distributed_utils | distributed init (rank 3): env://
 5: 2022-11-03 06:13:05 | INFO | fairseq.distributed_utils | distributed init (rank 5): env://
 4: 2022-11-03 06:13:05 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
 3: 2022-11-03 06:13:05 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
 6: 2022-11-03 06:13:05 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
 7: 2022-11-03 06:13:05 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
 2: 2022-11-03 06:13:05 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
 5: 2022-11-03 06:13:05 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
 1: 2022-11-03 06:13:06 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
12: 2022-11-03 06:13:10 | INFO | fairseq.distributed_utils | distributed init (rank 12): env://
10: 2022-11-03 06:13:10 | INFO | fairseq.distributed_utils | distributed init (rank 10): env://
12: 2022-11-03 06:13:10 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 12
13: 2022-11-03 06:13:10 | INFO | fairseq.distributed_utils | distributed init (rank 13): env://
10: 2022-11-03 06:13:10 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 10
15: 2022-11-03 06:13:10 | INFO | fairseq.distributed_utils | distributed init (rank 15): env://
13: 2022-11-03 06:13:10 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 13
14: 2022-11-03 06:13:10 | INFO | fairseq.distributed_utils | distributed init (rank 14): env://
 9: 2022-11-03 06:13:10 | INFO | fairseq.distributed_utils | distributed init (rank 9): env://
15: 2022-11-03 06:13:10 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 15
 9: 2022-11-03 06:13:10 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 9
14: 2022-11-03 06:13:10 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 14
11: 2022-11-03 06:13:10 | INFO | fairseq.distributed_utils | distributed init (rank 11): env://
 8: 2022-11-03 06:13:10 | INFO | fairseq.distributed_utils | distributed init (rank 8): env://
11: 2022-11-03 06:13:10 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 11
 0: 2022-11-03 06:13:10 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
 8: 2022-11-03 06:13:10 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 8
 8: 2022-11-03 06:13:10 | INFO | torch.distributed.distributed_c10d | Rank 8: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
 8: 2022-11-03 06:13:10 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-6 as rank 8
 4: 2022-11-03 06:13:10 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
 6: 2022-11-03 06:13:10 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
 2: 2022-11-03 06:13:10 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
 7: 2022-11-03 06:13:10 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
 3: 2022-11-03 06:13:10 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
 4: 2022-11-03 06:13:10 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-5 as rank 4
 5: 2022-11-03 06:13:10 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
 6: 2022-11-03 06:13:10 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-5 as rank 6
 7: 2022-11-03 06:13:10 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-5 as rank 7
12: 2022-11-03 06:13:10 | INFO | torch.distributed.distributed_c10d | Rank 12: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
 1: 2022-11-03 06:13:10 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
 2: 2022-11-03 06:13:10 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-5 as rank 2
 3: 2022-11-03 06:13:10 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-5 as rank 3
 1: 2022-11-03 06:13:10 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-5 as rank 1
 5: 2022-11-03 06:13:10 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-5 as rank 5
13: 2022-11-03 06:13:10 | INFO | torch.distributed.distributed_c10d | Rank 13: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
10: 2022-11-03 06:13:10 | INFO | torch.distributed.distributed_c10d | Rank 10: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
14: 2022-11-03 06:13:10 | INFO | torch.distributed.distributed_c10d | Rank 14: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
 9: 2022-11-03 06:13:10 | INFO | torch.distributed.distributed_c10d | Rank 9: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
15: 2022-11-03 06:13:10 | INFO | torch.distributed.distributed_c10d | Rank 15: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
12: 2022-11-03 06:13:10 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-6 as rank 12
13: 2022-11-03 06:13:10 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-6 as rank 13
14: 2022-11-03 06:13:10 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-6 as rank 14
10: 2022-11-03 06:13:10 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-6 as rank 10
15: 2022-11-03 06:13:10 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-6 as rank 15
 9: 2022-11-03 06:13:10 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-6 as rank 9
 0: 2022-11-03 06:13:10 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
 0: 2022-11-03 06:13:10 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-5 as rank 0
11: 2022-11-03 06:13:10 | INFO | torch.distributed.distributed_c10d | Rank 11: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
11: 2022-11-03 06:13:10 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-6 as rank 11
 0: queue1-dy-p4d24xlarge-5:26818:26818 [0] NCCL INFO Bootstrap : Using ens32:10.0.29.73<0>
 0: queue1-dy-p4d24xlarge-5:26818:26818 [0] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v4 symbol.
 0: queue1-dy-p4d24xlarge-5:26818:26818 [0] NCCL INFO NET/IB : No device found.
 0: queue1-dy-p4d24xlarge-5:26818:26818 [0] NCCL INFO NET/Socket : Using [0]ens32:10.0.29.73<0> [1]ens65:10.0.24.54<0> [2]ens129:10.0.30.91<0> [3]ens161:10.0.26.186<0>
 0: queue1-dy-p4d24xlarge-5:26818:26818 [0] NCCL INFO Using network Socket
 0: NCCL version 2.10.3+cuda11.6
 1: queue1-dy-p4d24xlarge-5:26819:26819 [1] NCCL INFO Bootstrap : Using ens32:10.0.29.73<0>
 2: queue1-dy-p4d24xlarge-5:26820:26820 [2] NCCL INFO Bootstrap : Using ens32:10.0.29.73<0>
 3: queue1-dy-p4d24xlarge-5:26821:26821 [3] NCCL INFO Bootstrap : Using ens32:10.0.29.73<0>
 5: queue1-dy-p4d24xlarge-5:26823:26823 [5] NCCL INFO Bootstrap : Using ens32:10.0.29.73<0>
 4: queue1-dy-p4d24xlarge-5:26822:26822 [4] NCCL INFO Bootstrap : Using ens32:10.0.29.73<0>
 8: queue1-dy-p4d24xlarge-6:23311:23311 [0] NCCL INFO Bootstrap : Using ens32:10.0.19.87<0>
10: queue1-dy-p4d24xlarge-6:23313:23313 [2] NCCL INFO Bootstrap : Using ens32:10.0.19.87<0>
11: queue1-dy-p4d24xlarge-6:23314:23314 [3] NCCL INFO Bootstrap : Using ens32:10.0.19.87<0>
12: queue1-dy-p4d24xlarge-6:23315:23315 [4] NCCL INFO Bootstrap : Using ens32:10.0.19.87<0>
13: queue1-dy-p4d24xlarge-6:23316:23316 [5] NCCL INFO Bootstrap : Using ens32:10.0.19.87<0>
14: queue1-dy-p4d24xlarge-6:23317:23317 [6] NCCL INFO Bootstrap : Using ens32:10.0.19.87<0>
 5: queue1-dy-p4d24xlarge-5:26823:26823 [5] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v4 symbol.
 4: queue1-dy-p4d24xlarge-5:26822:26822 [4] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v4 symbol.
 1: queue1-dy-p4d24xlarge-5:26819:26819 [1] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v4 symbol.
 2: queue1-dy-p4d24xlarge-5:26820:26820 [2] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v4 symbol.
 3: queue1-dy-p4d24xlarge-5:26821:26821 [3] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v4 symbol.
 4: queue1-dy-p4d24xlarge-5:26822:26822 [4] NCCL INFO NET/IB : No device found.
 5: queue1-dy-p4d24xlarge-5:26823:26823 [5] NCCL INFO NET/IB : No device found.
 3: queue1-dy-p4d24xlarge-5:26821:26821 [3] NCCL INFO NET/IB : No device found.
 2: queue1-dy-p4d24xlarge-5:26820:26820 [2] NCCL INFO NET/IB : No device found.
 1: queue1-dy-p4d24xlarge-5:26819:26819 [1] NCCL INFO NET/IB : No device found.
 4: queue1-dy-p4d24xlarge-5:26822:26822 [4] NCCL INFO NET/Socket : Using [0]ens32:10.0.29.73<0> [1]ens65:10.0.24.54<0> [2]ens129:10.0.30.91<0> [3]ens161:10.0.26.186<0>
 4: queue1-dy-p4d24xlarge-5:26822:26822 [4] NCCL INFO Using network Socket
 5: queue1-dy-p4d24xlarge-5:26823:26823 [5] NCCL INFO NET/Socket : Using [0]ens32:10.0.29.73<0> [1]ens65:10.0.24.54<0> [2]ens129:10.0.30.91<0> [3]ens161:10.0.26.186<0>
 5: queue1-dy-p4d24xlarge-5:26823:26823 [5] NCCL INFO Using network Socket
 1: queue1-dy-p4d24xlarge-5:26819:26819 [1] NCCL INFO NET/Socket : Using [0]ens32:10.0.29.73<0> [1]ens65:10.0.24.54<0> [2]ens129:10.0.30.91<0> [3]ens161:10.0.26.186<0>
 1: queue1-dy-p4d24xlarge-5:26819:26819 [1] NCCL INFO Using network Socket
 2: queue1-dy-p4d24xlarge-5:26820:26820 [2] NCCL INFO NET/Socket : Using [0]ens32:10.0.29.73<0> [1]ens65:10.0.24.54<0> [2]ens129:10.0.30.91<0> [3]ens161:10.0.26.186<0>
 2: queue1-dy-p4d24xlarge-5:26820:26820 [2] NCCL INFO Using network Socket
 3: queue1-dy-p4d24xlarge-5:26821:26821 [3] NCCL INFO NET/Socket : Using [0]ens32:10.0.29.73<0> [1]ens65:10.0.24.54<0> [2]ens129:10.0.30.91<0> [3]ens161:10.0.26.186<0>
 3: queue1-dy-p4d24xlarge-5:26821:26821 [3] NCCL INFO Using network Socket
10: queue1-dy-p4d24xlarge-6:23313:23313 [2] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v4 symbol.
14: queue1-dy-p4d24xlarge-6:23317:23317 [6] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v4 symbol.
 8: queue1-dy-p4d24xlarge-6:23311:23311 [0] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v4 symbol.
12: queue1-dy-p4d24xlarge-6:23315:23315 [4] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v4 symbol.
13: queue1-dy-p4d24xlarge-6:23316:23316 [5] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v4 symbol.
11: queue1-dy-p4d24xlarge-6:23314:23314 [3] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v4 symbol.
12: queue1-dy-p4d24xlarge-6:23315:23315 [4] NCCL INFO NET/IB : No device found.
14: queue1-dy-p4d24xlarge-6:23317:23317 [6] NCCL INFO NET/IB : No device found.
13: queue1-dy-p4d24xlarge-6:23316:23316 [5] NCCL INFO NET/IB : No device found.
 8: queue1-dy-p4d24xlarge-6:23311:23311 [0] NCCL INFO NET/IB : No device found.
10: queue1-dy-p4d24xlarge-6:23313:23313 [2] NCCL INFO NET/IB : No device found.
11: queue1-dy-p4d24xlarge-6:23314:23314 [3] NCCL INFO NET/IB : No device found.
14: queue1-dy-p4d24xlarge-6:23317:23317 [6] NCCL INFO NET/Socket : Using [0]ens32:10.0.19.87<0> [1]ens65:10.0.23.6<0> [2]ens129:10.0.26.195<0> [3]ens161:10.0.30.69<0>
14: queue1-dy-p4d24xlarge-6:23317:23317 [6] NCCL INFO Using network Socket
13: queue1-dy-p4d24xlarge-6:23316:23316 [5] NCCL INFO NET/Socket : Using [0]ens32:10.0.19.87<0> [1]ens65:10.0.23.6<0> [2]ens129:10.0.26.195<0> [3]ens161:10.0.30.69<0>
13: queue1-dy-p4d24xlarge-6:23316:23316 [5] NCCL INFO Using network Socket
12: queue1-dy-p4d24xlarge-6:23315:23315 [4] NCCL INFO NET/Socket : Using [0]ens32:10.0.19.87<0> [1]ens65:10.0.23.6<0> [2]ens129:10.0.26.195<0> [3]ens161:10.0.30.69<0>
12: queue1-dy-p4d24xlarge-6:23315:23315 [4] NCCL INFO Using network Socket
 8: queue1-dy-p4d24xlarge-6:23311:23311 [0] NCCL INFO NET/Socket : Using [0]ens32:10.0.19.87<0> [1]ens65:10.0.23.6<0> [2]ens129:10.0.26.195<0> [3]ens161:10.0.30.69<0>
 8: queue1-dy-p4d24xlarge-6:23311:23311 [0] NCCL INFO Using network Socket
10: queue1-dy-p4d24xlarge-6:23313:23313 [2] NCCL INFO NET/Socket : Using [0]ens32:10.0.19.87<0> [1]ens65:10.0.23.6<0> [2]ens129:10.0.26.195<0> [3]ens161:10.0.30.69<0>
10: queue1-dy-p4d24xlarge-6:23313:23313 [2] NCCL INFO Using network Socket
11: queue1-dy-p4d24xlarge-6:23314:23314 [3] NCCL INFO NET/Socket : Using [0]ens32:10.0.19.87<0> [1]ens65:10.0.23.6<0> [2]ens129:10.0.26.195<0> [3]ens161:10.0.30.69<0>
11: queue1-dy-p4d24xlarge-6:23314:23314 [3] NCCL INFO Using network Socket
15: queue1-dy-p4d24xlarge-6:23318:23318 [7] NCCL INFO Bootstrap : Using ens32:10.0.19.87<0>
15: queue1-dy-p4d24xlarge-6:23318:23318 [7] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v4 symbol.
15: queue1-dy-p4d24xlarge-6:23318:23318 [7] NCCL INFO NET/IB : No device found.
15: queue1-dy-p4d24xlarge-6:23318:23318 [7] NCCL INFO NET/Socket : Using [0]ens32:10.0.19.87<0> [1]ens65:10.0.23.6<0> [2]ens129:10.0.26.195<0> [3]ens161:10.0.30.69<0>
15: queue1-dy-p4d24xlarge-6:23318:23318 [7] NCCL INFO Using network Socket
 6: queue1-dy-p4d24xlarge-5:26824:26824 [6] NCCL INFO Bootstrap : Using ens32:10.0.29.73<0>
 6: queue1-dy-p4d24xlarge-5:26824:26824 [6] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v4 symbol.
 6: queue1-dy-p4d24xlarge-5:26824:26824 [6] NCCL INFO NET/IB : No device found.
 6: queue1-dy-p4d24xlarge-5:26824:26824 [6] NCCL INFO NET/Socket : Using [0]ens32:10.0.29.73<0> [1]ens65:10.0.24.54<0> [2]ens129:10.0.30.91<0> [3]ens161:10.0.26.186<0>
 6: queue1-dy-p4d24xlarge-5:26824:26824 [6] NCCL INFO Using network Socket
 7: queue1-dy-p4d24xlarge-5:26825:26825 [7] NCCL INFO Bootstrap : Using ens32:10.0.29.73<0>
 7: queue1-dy-p4d24xlarge-5:26825:26825 [7] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v4 symbol.
 7: queue1-dy-p4d24xlarge-5:26825:26825 [7] NCCL INFO NET/IB : No device found.
 7: queue1-dy-p4d24xlarge-5:26825:26825 [7] NCCL INFO NET/Socket : Using [0]ens32:10.0.29.73<0> [1]ens65:10.0.24.54<0> [2]ens129:10.0.30.91<0> [3]ens161:10.0.26.186<0>
 7: queue1-dy-p4d24xlarge-5:26825:26825 [7] NCCL INFO Using network Socket
 9: queue1-dy-p4d24xlarge-6:23312:23312 [1] NCCL INFO Bootstrap : Using ens32:10.0.19.87<0>
 9: queue1-dy-p4d24xlarge-6:23312:23312 [1] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v4 symbol.
 9: queue1-dy-p4d24xlarge-6:23312:23312 [1] NCCL INFO NET/IB : No device found.
 9: queue1-dy-p4d24xlarge-6:23312:23312 [1] NCCL INFO NET/Socket : Using [0]ens32:10.0.19.87<0> [1]ens65:10.0.23.6<0> [2]ens129:10.0.26.195<0> [3]ens161:10.0.30.69<0>
 9: queue1-dy-p4d24xlarge-6:23312:23312 [1] NCCL INFO Using network Socket
 8: queue1-dy-p4d24xlarge-6:23311:23482 [0] NCCL INFO Trees [0] 9/-1/-1->8->0 [1] 9/-1/-1->8->0 [2] 9/-1/-1->8->15 [3] 9/-1/-1->8->15 [4] 9/0/-1->8->-1 [5] 9/0/-1->8->-1 [6] 9/-1/-1->8->15 [7] 9/-1/-1->8->15
 8: queue1-dy-p4d24xlarge-6:23311:23482 [0] NCCL INFO Setting affinity for GPU 0 to 3f0000,0000003f
 9: queue1-dy-p4d24xlarge-6:23312:23485 [1] NCCL INFO Trees [0] 10/-1/-1->9->8 [1] 10/-1/-1->9->8 [2] 10/-1/-1->9->8 [3] 10/-1/-1->9->8 [4] 10/-1/-1->9->8 [5] 10/-1/-1->9->8 [6] 10/-1/-1->9->8 [7] 10/-1/-1->9->8
 9: queue1-dy-p4d24xlarge-6:23312:23485 [1] NCCL INFO Setting affinity for GPU 1 to 0fc00000,00000fc0
10: queue1-dy-p4d24xlarge-6:23313:23478 [2] NCCL INFO Trees [0] 11/-1/-1->10->9 [1] 11/-1/-1->10->9 [2] 11/-1/-1->10->9 [3] 11/-1/-1->10->9 [4] 11/-1/-1->10->9 [5] 11/-1/-1->10->9 [6] 11/-1/-1->10->9 [7] 11/-1/-1->10->9
10: queue1-dy-p4d24xlarge-6:23313:23478 [2] NCCL INFO Setting affinity for GPU 2 to 03,f0000000,0003f000
 6: queue1-dy-p4d24xlarge-5:26824:26994 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 7/-1/-1->6->5 [3] 7/-1/-1->6->5 [4] 7/-1/-1->6->5 [5] 7/-1/-1->6->5 [6] 7/-1/-1->6->5 [7] 7/-1/-1->6->5
 6: queue1-dy-p4d24xlarge-5:26824:26994 [6] NCCL INFO Setting affinity for GPU 6 to 03f00000,000003f0,00000000
12: queue1-dy-p4d24xlarge-6:23315:23479 [4] NCCL INFO Trees [0] 13/-1/-1->12->11 [1] 13/-1/-1->12->11 [2] 13/-1/-1->12->4 [3] 13/-1/-1->12->4 [4] 13/-1/-1->12->11 [5] 13/-1/-1->12->11 [6] 13/4/-1->12->-1 [7] 13/4/-1->12->-1
12: queue1-dy-p4d24xlarge-6:23315:23479 [4] NCCL INFO Setting affinity for GPU 4 to 3f00,00000000,3f000000
13: queue1-dy-p4d24xlarge-6:23316:23483 [5] NCCL INFO Trees [0] 14/-1/-1->13->12 [1] 14/-1/-1->13->12 [2] 14/-1/-1->13->12 [3] 14/-1/-1->13->12 [4] 14/-1/-1->13->12 [5] 14/-1/-1->13->12 [6] 14/-1/-1->13->12 [7] 14/-1/-1->13->12
13: queue1-dy-p4d24xlarge-6:23316:23483 [5] NCCL INFO Setting affinity for GPU 5 to 0fc000,0000000f,c0000000
14: queue1-dy-p4d24xlarge-6:23317:23480 [6] NCCL INFO Trees [0] 15/-1/-1->14->13 [1] 15/-1/-1->14->13 [2] 15/-1/-1->14->13 [3] 15/-1/-1->14->13 [4] 15/-1/-1->14->13 [5] 15/-1/-1->14->13 [6] 15/-1/-1->14->13 [7] 15/-1/-1->14->13
14: queue1-dy-p4d24xlarge-6:23317:23480 [6] NCCL INFO Setting affinity for GPU 6 to 03f00000,000003f0,00000000
15: queue1-dy-p4d24xlarge-6:23318:23484 [7] NCCL INFO Trees [0] -1/-1/-1->15->14 [1] -1/-1/-1->15->14 [2] 8/-1/-1->15->14 [3] 8/-1/-1->15->14 [4] -1/-1/-1->15->14 [5] -1/-1/-1->15->14 [6] 8/-1/-1->15->14 [7] 8/-1/-1->15->14
15: queue1-dy-p4d24xlarge-6:23318:23484 [7] NCCL INFO Setting affinity for GPU 7 to fc000000,0000fc00,00000000
11: queue1-dy-p4d24xlarge-6:23314:23481 [3] NCCL INFO Trees [0] 12/-1/-1->11->10 [1] 12/-1/-1->11->10 [2] -1/-1/-1->11->10 [3] -1/-1/-1->11->10 [4] 12/-1/-1->11->10 [5] 12/-1/-1->11->10 [6] -1/-1/-1->11->10 [7] -1/-1/-1->11->10
11: queue1-dy-p4d24xlarge-6:23314:23481 [3] NCCL INFO Setting affinity for GPU 3 to fc,00000000,00fc0000
 7: queue1-dy-p4d24xlarge-5:26825:26995 [7] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6 [2] 0/-1/-1->7->6 [3] 0/-1/-1->7->6 [4] -1/-1/-1->7->6 [5] -1/-1/-1->7->6 [6] 0/-1/-1->7->6 [7] 0/-1/-1->7->6
 7: queue1-dy-p4d24xlarge-5:26825:26995 [7] NCCL INFO Setting affinity for GPU 7 to fc000000,0000fc00,00000000
 0: queue1-dy-p4d24xlarge-5:26818:26988 [0] NCCL INFO Channel 00/08 :    0   7   6   5   4   3   2   1   8  15  14  13  12  11  10   9
 0: queue1-dy-p4d24xlarge-5:26818:26988 [0] NCCL INFO Channel 01/08 :    0   7   6   5   4   3   2   1   8  15  14  13  12  11  10   9
 0: queue1-dy-p4d24xlarge-5:26818:26988 [0] NCCL INFO Channel 02/08 :    0   3   2   7   6   5  12   9   8  11  10  15  14  13   4   1
 0: queue1-dy-p4d24xlarge-5:26818:26988 [0] NCCL INFO Channel 03/08 :    0   3   2   7   6   5  12   9   8  11  10  15  14  13   4   1
 0: queue1-dy-p4d24xlarge-5:26818:26988 [0] NCCL INFO Channel 04/08 :    0   7   6   5   4   3   2   1   8  15  14  13  12  11  10   9
 0: queue1-dy-p4d24xlarge-5:26818:26988 [0] NCCL INFO Channel 05/08 :    0   7   6   5   4   3   2   1   8  15  14  13  12  11  10   9
 0: queue1-dy-p4d24xlarge-5:26818:26988 [0] NCCL INFO Channel 06/08 :    0   3   2   7   6   5  12   9   8  11  10  15  14  13   4   1
 0: queue1-dy-p4d24xlarge-5:26818:26988 [0] NCCL INFO Channel 07/08 :    0   3   2   7   6   5  12   9   8  11  10  15  14  13   4   1
 1: queue1-dy-p4d24xlarge-5:26819:26991 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] 2/-1/-1->1->0 [6] 2/-1/-1->1->0 [7] 2/-1/-1->1->0
 1: queue1-dy-p4d24xlarge-5:26819:26991 [1] NCCL INFO Setting affinity for GPU 1 to 0fc00000,00000fc0
 2: queue1-dy-p4d24xlarge-5:26820:26992 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->1 [6] 3/-1/-1->2->1 [7] 3/-1/-1->2->1
 2: queue1-dy-p4d24xlarge-5:26820:26992 [2] NCCL INFO Setting affinity for GPU 2 to 03,f0000000,0003f000
 3: queue1-dy-p4d24xlarge-5:26821:26993 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2 [2] -1/-1/-1->3->2 [3] -1/-1/-1->3->2 [4] 4/-1/-1->3->2 [5] 4/-1/-1->3->2 [6] -1/-1/-1->3->2 [7] -1/-1/-1->3->2
 3: queue1-dy-p4d24xlarge-5:26821:26993 [3] NCCL INFO Setting affinity for GPU 3 to fc,00000000,00fc0000
 4: queue1-dy-p4d24xlarge-5:26822:26990 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3 [2] 5/12/-1->4->-1 [3] 5/12/-1->4->-1 [4] 5/-1/-1->4->3 [5] 5/-1/-1->4->3 [6] 5/-1/-1->4->12 [7] 5/-1/-1->4->12
 4: queue1-dy-p4d24xlarge-5:26822:26990 [4] NCCL INFO Setting affinity for GPU 4 to 3f00,00000000,3f000000
 5: queue1-dy-p4d24xlarge-5:26823:26989 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4 [2] 6/-1/-1->5->4 [3] 6/-1/-1->5->4 [4] 6/-1/-1->5->4 [5] 6/-1/-1->5->4 [6] 6/-1/-1->5->4 [7] 6/-1/-1->5->4
 5: queue1-dy-p4d24xlarge-5:26823:26989 [5] NCCL INFO Setting affinity for GPU 5 to 0fc000,0000000f,c0000000
 0: queue1-dy-p4d24xlarge-5:26818:26988 [0] NCCL INFO Trees [0] 1/8/-1->0->-1 [1] 1/8/-1->0->-1 [2] 1/-1/-1->0->7 [3] 1/-1/-1->0->7 [4] 1/-1/-1->0->8 [5] 1/-1/-1->0->8 [6] 1/-1/-1->0->7 [7] 1/-1/-1->0->7
 0: queue1-dy-p4d24xlarge-5:26818:26988 [0] NCCL INFO Setting affinity for GPU 0 to 3f0000,0000003f
 8: queue1-dy-p4d24xlarge-6:23311:23482 [0] NCCL INFO Channel 02 : 8[101c0] -> 11[201d0] via P2P/IPC/read
10: queue1-dy-p4d24xlarge-6:23313:23478 [2] NCCL INFO Channel 02 : 10[201c0] -> 15[a01d0] via P2P/IPC/read
 8: queue1-dy-p4d24xlarge-6:23311:23482 [0] NCCL INFO Channel 03 : 8[101c0] -> 11[201d0] via P2P/IPC/read
10: queue1-dy-p4d24xlarge-6:23313:23478 [2] NCCL INFO Channel 03 : 10[201c0] -> 15[a01d0] via P2P/IPC/read
 2: queue1-dy-p4d24xlarge-5:26820:26992 [2] NCCL INFO Channel 02 : 2[201c0] -> 7[a01d0] via P2P/IPC/read
 8: queue1-dy-p4d24xlarge-6:23311:23482 [0] NCCL INFO Channel 06 : 8[101c0] -> 11[201d0] via P2P/IPC/read
 0: queue1-dy-p4d24xlarge-5:26818:26988 [0] NCCL INFO Channel 02 : 0[101c0] -> 3[201d0] via P2P/IPC/read
10: queue1-dy-p4d24xlarge-6:23313:23478 [2] NCCL INFO Channel 06 : 10[201c0] -> 15[a01d0] via P2P/IPC/read
 2: queue1-dy-p4d24xlarge-5:26820:26992 [2] NCCL INFO Channel 03 : 2[201c0] -> 7[a01d0] via P2P/IPC/read
 0: queue1-dy-p4d24xlarge-5:26818:26988 [0] NCCL INFO Channel 03 : 0[101c0] -> 3[201d0] via P2P/IPC/read
 8: queue1-dy-p4d24xlarge-6:23311:23482 [0] NCCL INFO Channel 07 : 8[101c0] -> 11[201d0] via P2P/IPC/read
10: queue1-dy-p4d24xlarge-6:23313:23478 [2] NCCL INFO Channel 07 : 10[201c0] -> 15[a01d0] via P2P/IPC/read
 2: queue1-dy-p4d24xlarge-5:26820:26992 [2] NCCL INFO Channel 06 : 2[201c0] -> 7[a01d0] via P2P/IPC/read
 0: queue1-dy-p4d24xlarge-5:26818:26988 [0] NCCL INFO Channel 06 : 0[101c0] -> 3[201d0] via P2P/IPC/read
 2: queue1-dy-p4d24xlarge-5:26820:26992 [2] NCCL INFO Channel 07 : 2[201c0] -> 7[a01d0] via P2P/IPC/read
 9: queue1-dy-p4d24xlarge-6:23312:23485 [1] NCCL INFO Channel 00 : 9[101d0] -> 0[101c0] [send] via NET/Socket/0
13: queue1-dy-p4d24xlarge-6:23316:23483 [5] NCCL INFO Channel 02 : 13[901d0] -> 4[901c0] [send] via NET/Socket/2
 1: queue1-dy-p4d24xlarge-5:26819:26991 [1] NCCL INFO Channel 00 : 1[101d0] -> 8[101c0] [send] via NET/Socket/0
 0: queue1-dy-p4d24xlarge-5:26818:26988 [0] NCCL INFO Channel 07 : 0[101c0] -> 3[201d0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-5:26822:26990 [4] NCCL INFO Channel 02 : 13[901d0] -> 4[901c0] [receive] via NET/Socket/2
 4: queue1-dy-p4d24xlarge-5:26822:26990 [4] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
12: queue1-dy-p4d24xlarge-6:23315:23479 [4] NCCL INFO Channel 02 : 5[901d0] -> 12[901c0] [receive] via NET/Socket/2
12: queue1-dy-p4d24xlarge-6:23315:23479 [4] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
 5: queue1-dy-p4d24xlarge-5:26823:26989 [5] NCCL INFO Channel 02 : 5[901d0] -> 12[901c0] [send] via NET/Socket/2
 9: queue1-dy-p4d24xlarge-6:23312:23485 [1] NCCL INFO Channel 01 : 9[101d0] -> 0[101c0] [send] via NET/Socket/1
13: queue1-dy-p4d24xlarge-6:23316:23483 [5] NCCL INFO Channel 03 : 13[901d0] -> 4[901c0] [send] via NET/Socket/3
 1: queue1-dy-p4d24xlarge-5:26819:26991 [1] NCCL INFO Channel 01 : 1[101d0] -> 8[101c0] [send] via NET/Socket/1
 4: queue1-dy-p4d24xlarge-5:26822:26990 [4] NCCL INFO Channel 03 : 13[901d0] -> 4[901c0] [receive] via NET/Socket/3
 4: queue1-dy-p4d24xlarge-5:26822:26990 [4] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
12: queue1-dy-p4d24xlarge-6:23315:23479 [4] NCCL INFO Channel 03 : 5[901d0] -> 12[901c0] [receive] via NET/Socket/3
12: queue1-dy-p4d24xlarge-6:23315:23479 [4] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
 5: queue1-dy-p4d24xlarge-5:26823:26989 [5] NCCL INFO Channel 03 : 5[901d0] -> 12[901c0] [send] via NET/Socket/3
 6: queue1-dy-p4d24xlarge-5:26824:26994 [6] NCCL INFO Channel 00 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
 9: queue1-dy-p4d24xlarge-6:23312:23485 [1] NCCL INFO Channel 04 : 9[101d0] -> 0[101c0] [send] via NET/Socket/0
14: queue1-dy-p4d24xlarge-6:23317:23480 [6] NCCL INFO Channel 00 : 14[a01c0] -> 13[901d0] via P2P/IPC/read
 6: queue1-dy-p4d24xlarge-5:26824:26994 [6] NCCL INFO Channel 01 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
13: queue1-dy-p4d24xlarge-6:23316:23483 [5] NCCL INFO Channel 06 : 13[901d0] -> 4[901c0] [send] via NET/Socket/2
14: queue1-dy-p4d24xlarge-6:23317:23480 [6] NCCL INFO Channel 01 : 14[a01c0] -> 13[901d0] via P2P/IPC/read
 1: queue1-dy-p4d24xlarge-5:26819:26991 [1] NCCL INFO Channel 04 : 1[101d0] -> 8[101c0] [send] via NET/Socket/0
 6: queue1-dy-p4d24xlarge-5:26824:26994 [6] NCCL INFO Channel 02 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
14: queue1-dy-p4d24xlarge-6:23317:23480 [6] NCCL INFO Channel 02 : 14[a01c0] -> 13[901d0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-5:26822:26990 [4] NCCL INFO Channel 06 : 13[901d0] -> 4[901c0] [receive] via NET/Socket/2
 4: queue1-dy-p4d24xlarge-5:26822:26990 [4] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
 6: queue1-dy-p4d24xlarge-5:26824:26994 [6] NCCL INFO Channel 03 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-6:23315:23479 [4] NCCL INFO Channel 06 : 5[901d0] -> 12[901c0] [receive] via NET/Socket/2
12: queue1-dy-p4d24xlarge-6:23315:23479 [4] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
14: queue1-dy-p4d24xlarge-6:23317:23480 [6] NCCL INFO Channel 03 : 14[a01c0] -> 13[901d0] via P2P/IPC/read
 5: queue1-dy-p4d24xlarge-5:26823:26989 [5] NCCL INFO Channel 06 : 5[901d0] -> 12[901c0] [send] via NET/Socket/2
11: queue1-dy-p4d24xlarge-6:23314:23481 [3] NCCL INFO Channel 00 : 11[201d0] -> 10[201c0] via P2P/IPC/read
 3: queue1-dy-p4d24xlarge-5:26821:26993 [3] NCCL INFO Channel 00 : 3[201d0] -> 2[201c0] via P2P/IPC/read
 6: queue1-dy-p4d24xlarge-5:26824:26994 [6] NCCL INFO Channel 04 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
 8: queue1-dy-p4d24xlarge-6:23311:23482 [0] NCCL INFO Channel 00 : 1[101d0] -> 8[101c0] [receive] via NET/Socket/0
14: queue1-dy-p4d24xlarge-6:23317:23480 [6] NCCL INFO Channel 04 : 14[a01c0] -> 13[901d0] via P2P/IPC/read
 8: queue1-dy-p4d24xlarge-6:23311:23482 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
11: queue1-dy-p4d24xlarge-6:23314:23481 [3] NCCL INFO Channel 01 : 11[201d0] -> 10[201c0] via P2P/IPC/read
 3: queue1-dy-p4d24xlarge-5:26821:26993 [3] NCCL INFO Channel 01 : 3[201d0] -> 2[201c0] via P2P/IPC/read
14: queue1-dy-p4d24xlarge-6:23317:23480 [6] NCCL INFO Channel 05 : 14[a01c0] -> 13[901d0] via P2P/IPC/read
 6: queue1-dy-p4d24xlarge-5:26824:26994 [6] NCCL INFO Channel 05 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
 0: queue1-dy-p4d24xlarge-5:26818:26988 [0] NCCL INFO Channel 00 : 9[101d0] -> 0[101c0] [receive] via NET/Socket/0
 0: queue1-dy-p4d24xlarge-5:26818:26988 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
11: queue1-dy-p4d24xlarge-6:23314:23481 [3] NCCL INFO Channel 02 : 11[201d0] -> 10[201c0] via P2P/IPC/read
 3: queue1-dy-p4d24xlarge-5:26821:26993 [3] NCCL INFO Channel 02 : 3[201d0] -> 2[201c0] via P2P/IPC/read
 6: queue1-dy-p4d24xlarge-5:26824:26994 [6] NCCL INFO Channel 06 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
14: queue1-dy-p4d24xlarge-6:23317:23480 [6] NCCL INFO Channel 06 : 14[a01c0] -> 13[901d0] via P2P/IPC/read
 3: queue1-dy-p4d24xlarge-5:26821:26993 [3] NCCL INFO Channel 03 : 3[201d0] -> 2[201c0] via P2P/IPC/read
 6: queue1-dy-p4d24xlarge-5:26824:26994 [6] NCCL INFO Channel 07 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
 2: queue1-dy-p4d24xlarge-5:26820:26992 [2] NCCL INFO Channel 00 : 2[201c0] -> 1[101d0] via P2P/IPC/read
 3: queue1-dy-p4d24xlarge-5:26821:26993 [3] NCCL INFO Channel 04 : 3[201d0] -> 2[201c0] via P2P/IPC/read
 2: queue1-dy-p4d24xlarge-5:26820:26992 [2] NCCL INFO Channel 01 : 2[201c0] -> 1[101d0] via P2P/IPC/read
11: queue1-dy-p4d24xlarge-6:23314:23481 [3] NCCL INFO Channel 03 : 11[201d0] -> 10[201c0] via P2P/IPC/read
 3: queue1-dy-p4d24xlarge-5:26821:26993 [3] NCCL INFO Channel 05 : 3[201d0] -> 2[201c0] via P2P/IPC/read
14: queue1-dy-p4d24xlarge-6:23317:23480 [6] NCCL INFO Channel 07 : 14[a01c0] -> 13[901d0] via P2P/IPC/read
10: queue1-dy-p4d24xlarge-6:23313:23478 [2] NCCL INFO Channel 00 : 10[201c0] -> 9[101d0] via P2P/IPC/read
 2: queue1-dy-p4d24xlarge-5:26820:26992 [2] NCCL INFO Channel 04 : 2[201c0] -> 1[101d0] via P2P/IPC/read
11: queue1-dy-p4d24xlarge-6:23314:23481 [3] NCCL INFO Channel 04 : 11[201d0] -> 10[201c0] via P2P/IPC/read
 9: queue1-dy-p4d24xlarge-6:23312:23485 [1] NCCL INFO Channel 05 : 9[101d0] -> 0[101c0] [send] via NET/Socket/1
13: queue1-dy-p4d24xlarge-6:23316:23483 [5] NCCL INFO Channel 07 : 13[901d0] -> 4[901c0] [send] via NET/Socket/3
 3: queue1-dy-p4d24xlarge-5:26821:26993 [3] NCCL INFO Channel 06 : 3[201d0] -> 2[201c0] via P2P/IPC/read
10: queue1-dy-p4d24xlarge-6:23313:23478 [2] NCCL INFO Channel 01 : 10[201c0] -> 9[101d0] via P2P/IPC/read
 2: queue1-dy-p4d24xlarge-5:26820:26992 [2] NCCL INFO Channel 05 : 2[201c0] -> 1[101d0] via P2P/IPC/read
11: queue1-dy-p4d24xlarge-6:23314:23481 [3] NCCL INFO Channel 05 : 11[201d0] -> 10[201c0] via P2P/IPC/read
10: queue1-dy-p4d24xlarge-6:23313:23478 [2] NCCL INFO Channel 04 : 10[201c0] -> 9[101d0] via P2P/IPC/read
11: queue1-dy-p4d24xlarge-6:23314:23481 [3] NCCL INFO Channel 06 : 11[201d0] -> 10[201c0] via P2P/IPC/read
 1: queue1-dy-p4d24xlarge-5:26819:26991 [1] NCCL INFO Channel 05 : 1[101d0] -> 8[101c0] [send] via NET/Socket/1
10: queue1-dy-p4d24xlarge-6:23313:23478 [2] NCCL INFO Channel 05 : 10[201c0] -> 9[101d0] via P2P/IPC/read
 3: queue1-dy-p4d24xlarge-5:26821:26993 [3] NCCL INFO Channel 07 : 3[201d0] -> 2[201c0] via P2P/IPC/read
11: queue1-dy-p4d24xlarge-6:23314:23481 [3] NCCL INFO Channel 07 : 11[201d0] -> 10[201c0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-5:26822:26990 [4] NCCL INFO Channel 07 : 13[901d0] -> 4[901c0] [receive] via NET/Socket/3
 4: queue1-dy-p4d24xlarge-5:26822:26990 [4] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
12: queue1-dy-p4d24xlarge-6:23315:23479 [4] NCCL INFO Channel 07 : 5[901d0] -> 12[901c0] [receive] via NET/Socket/3
12: queue1-dy-p4d24xlarge-6:23315:23479 [4] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
 5: queue1-dy-p4d24xlarge-5:26823:26989 [5] NCCL INFO Channel 07 : 5[901d0] -> 12[901c0] [send] via NET/Socket/3
 8: queue1-dy-p4d24xlarge-6:23311:23482 [0] NCCL INFO Channel 01 : 1[101d0] -> 8[101c0] [receive] via NET/Socket/1
 8: queue1-dy-p4d24xlarge-6:23311:23482 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
 0: queue1-dy-p4d24xlarge-5:26818:26988 [0] NCCL INFO Channel 01 : 9[101d0] -> 0[101c0] [receive] via NET/Socket/1
 0: queue1-dy-p4d24xlarge-5:26818:26988 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
 8: queue1-dy-p4d24xlarge-6:23311:23482 [0] NCCL INFO Channel 04 : 1[101d0] -> 8[101c0] [receive] via NET/Socket/0
 8: queue1-dy-p4d24xlarge-6:23311:23482 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
 0: queue1-dy-p4d24xlarge-5:26818:26988 [0] NCCL INFO Channel 04 : 9[101d0] -> 0[101c0] [receive] via NET/Socket/0
 0: queue1-dy-p4d24xlarge-5:26818:26988 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
 8: queue1-dy-p4d24xlarge-6:23311:23482 [0] NCCL INFO Channel 05 : 1[101d0] -> 8[101c0] [receive] via NET/Socket/1
 8: queue1-dy-p4d24xlarge-6:23311:23482 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
 8: queue1-dy-p4d24xlarge-6:23311:23482 [0] NCCL INFO Channel 00 : 8[101c0] -> 15[a01d0] via P2P/IPC/read
 0: queue1-dy-p4d24xlarge-5:26818:26988 [0] NCCL INFO Channel 05 : 9[101d0] -> 0[101c0] [receive] via NET/Socket/1
 8: queue1-dy-p4d24xlarge-6:23311:23482 [0] NCCL INFO Channel 01 : 8[101c0] -> 15[a01d0] via P2P/IPC/read
 0: queue1-dy-p4d24xlarge-5:26818:26988 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
 8: queue1-dy-p4d24xlarge-6:23311:23482 [0] NCCL INFO Channel 04 : 8[101c0] -> 15[a01d0] via P2P/IPC/read
 0: queue1-dy-p4d24xlarge-5:26818:26988 [0] NCCL INFO Channel 00 : 0[101c0] -> 7[a01d0] via P2P/IPC/read
 8: queue1-dy-p4d24xlarge-6:23311:23482 [0] NCCL INFO Channel 05 : 8[101c0] -> 15[a01d0] via P2P/IPC/read
 0: queue1-dy-p4d24xlarge-5:26818:26988 [0] NCCL INFO Channel 01 : 0[101c0] -> 7[a01d0] via P2P/IPC/read
 0: queue1-dy-p4d24xlarge-5:26818:26988 [0] NCCL INFO Channel 04 : 0[101c0] -> 7[a01d0] via P2P/IPC/read
 0: queue1-dy-p4d24xlarge-5:26818:26988 [0] NCCL INFO Channel 05 : 0[101c0] -> 7[a01d0] via P2P/IPC/read
15: queue1-dy-p4d24xlarge-6:23318:23484 [7] NCCL INFO Channel 00 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read
15: queue1-dy-p4d24xlarge-6:23318:23484 [7] NCCL INFO Channel 01 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read
15: queue1-dy-p4d24xlarge-6:23318:23484 [7] NCCL INFO Channel 02 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read
 7: queue1-dy-p4d24xlarge-5:26825:26995 [7] NCCL INFO Channel 00 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
15: queue1-dy-p4d24xlarge-6:23318:23484 [7] NCCL INFO Channel 03 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read
 7: queue1-dy-p4d24xlarge-5:26825:26995 [7] NCCL INFO Channel 01 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
15: queue1-dy-p4d24xlarge-6:23318:23484 [7] NCCL INFO Channel 04 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read
 7: queue1-dy-p4d24xlarge-5:26825:26995 [7] NCCL INFO Channel 02 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
15: queue1-dy-p4d24xlarge-6:23318:23484 [7] NCCL INFO Channel 05 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read
 7: queue1-dy-p4d24xlarge-5:26825:26995 [7] NCCL INFO Channel 03 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
15: queue1-dy-p4d24xlarge-6:23318:23484 [7] NCCL INFO Channel 06 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read
 7: queue1-dy-p4d24xlarge-5:26825:26995 [7] NCCL INFO Channel 04 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
15: queue1-dy-p4d24xlarge-6:23318:23484 [7] NCCL INFO Channel 07 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read
 7: queue1-dy-p4d24xlarge-5:26825:26995 [7] NCCL INFO Channel 05 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
 7: queue1-dy-p4d24xlarge-5:26825:26995 [7] NCCL INFO Channel 06 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
 7: queue1-dy-p4d24xlarge-5:26825:26995 [7] NCCL INFO Channel 07 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
15: queue1-dy-p4d24xlarge-6:23318:23484 [7] NCCL INFO Connected all rings
 7: queue1-dy-p4d24xlarge-5:26825:26995 [7] NCCL INFO Connected all rings
12: queue1-dy-p4d24xlarge-6:23315:23479 [4] NCCL INFO Channel 02 : 12[901c0] -> 9[101d0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-6:23315:23479 [4] NCCL INFO Channel 03 : 12[901c0] -> 9[101d0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-6:23315:23479 [4] NCCL INFO Channel 06 : 12[901c0] -> 9[101d0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-6:23315:23479 [4] NCCL INFO Channel 07 : 12[901c0] -> 9[101d0] via P2P/IPC/read
 5: queue1-dy-p4d24xlarge-5:26823:26989 [5] NCCL INFO Channel 00 : 5[901d0] -> 4[901c0] via P2P/IPC/read
 5: queue1-dy-p4d24xlarge-5:26823:26989 [5] NCCL INFO Channel 01 : 5[901d0] -> 4[901c0] via P2P/IPC/read
 5: queue1-dy-p4d24xlarge-5:26823:26989 [5] NCCL INFO Channel 04 : 5[901d0] -> 4[901c0] via P2P/IPC/read
 5: queue1-dy-p4d24xlarge-5:26823:26989 [5] NCCL INFO Channel 05 : 5[901d0] -> 4[901c0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-6:23315:23479 [4] NCCL INFO Channel 00 : 12[901c0] -> 11[201d0] via P2P/IPC/read
 9: queue1-dy-p4d24xlarge-6:23312:23485 [1] NCCL INFO Channel 02 : 9[101d0] -> 8[101c0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-6:23315:23479 [4] NCCL INFO Channel 01 : 12[901c0] -> 11[201d0] via P2P/IPC/read
 9: queue1-dy-p4d24xlarge-6:23312:23485 [1] NCCL INFO Channel 03 : 9[101d0] -> 8[101c0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-6:23315:23479 [4] NCCL INFO Channel 04 : 12[901c0] -> 11[201d0] via P2P/IPC/read
 9: queue1-dy-p4d24xlarge-6:23312:23485 [1] NCCL INFO Channel 06 : 9[101d0] -> 8[101c0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-6:23315:23479 [4] NCCL INFO Channel 05 : 12[901c0] -> 11[201d0] via P2P/IPC/read
 9: queue1-dy-p4d24xlarge-6:23312:23485 [1] NCCL INFO Channel 07 : 9[101d0] -> 8[101c0] via P2P/IPC/read
 6: queue1-dy-p4d24xlarge-5:26824:26994 [6] NCCL INFO Connected all rings
 6: queue1-dy-p4d24xlarge-5:26824:26994 [6] NCCL INFO Channel 00 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
 6: queue1-dy-p4d24xlarge-5:26824:26994 [6] NCCL INFO Channel 01 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
 6: queue1-dy-p4d24xlarge-5:26824:26994 [6] NCCL INFO Channel 02 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
 6: queue1-dy-p4d24xlarge-5:26824:26994 [6] NCCL INFO Channel 03 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
 6: queue1-dy-p4d24xlarge-5:26824:26994 [6] NCCL INFO Channel 04 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
 6: queue1-dy-p4d24xlarge-5:26824:26994 [6] NCCL INFO Channel 05 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
 6: queue1-dy-p4d24xlarge-5:26824:26994 [6] NCCL INFO Channel 06 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
 6: queue1-dy-p4d24xlarge-5:26824:26994 [6] NCCL INFO Channel 07 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
 8: queue1-dy-p4d24xlarge-6:23311:23482 [0] NCCL INFO Connected all rings
 8: queue1-dy-p4d24xlarge-6:23311:23482 [0] NCCL INFO Channel 00 : 8[101c0] -> 9[101d0] via P2P/IPC/read
 8: queue1-dy-p4d24xlarge-6:23311:23482 [0] NCCL INFO Channel 01 : 8[101c0] -> 9[101d0] via P2P/IPC/read
 9: queue1-dy-p4d24xlarge-6:23312:23485 [1] NCCL INFO Connected all rings
 8: queue1-dy-p4d24xlarge-6:23311:23482 [0] NCCL INFO Channel 02 : 8[101c0] -> 9[101d0] via P2P/IPC/read
 8: queue1-dy-p4d24xlarge-6:23311:23482 [0] NCCL INFO Channel 03 : 8[101c0] -> 9[101d0] via P2P/IPC/read
10: queue1-dy-p4d24xlarge-6:23313:23478 [2] NCCL INFO Connected all rings
 8: queue1-dy-p4d24xlarge-6:23311:23482 [0] NCCL INFO Channel 04 : 8[101c0] -> 9[101d0] via P2P/IPC/read
11: queue1-dy-p4d24xlarge-6:23314:23481 [3] NCCL INFO Connected all rings
 8: queue1-dy-p4d24xlarge-6:23311:23482 [0] NCCL INFO Channel 05 : 8[101c0] -> 9[101d0] via P2P/IPC/read
 8: queue1-dy-p4d24xlarge-6:23311:23482 [0] NCCL INFO Channel 06 : 8[101c0] -> 9[101d0] via P2P/IPC/read
 8: queue1-dy-p4d24xlarge-6:23311:23482 [0] NCCL INFO Channel 07 : 8[101c0] -> 9[101d0] via P2P/IPC/read
 9: queue1-dy-p4d24xlarge-6:23312:23485 [1] NCCL INFO Channel 00 : 9[101d0] -> 10[201c0] via P2P/IPC/read
 9: queue1-dy-p4d24xlarge-6:23312:23485 [1] NCCL INFO Channel 01 : 9[101d0] -> 10[201c0] via P2P/IPC/read
 9: queue1-dy-p4d24xlarge-6:23312:23485 [1] NCCL INFO Channel 02 : 9[101d0] -> 10[201c0] via P2P/IPC/read
10: queue1-dy-p4d24xlarge-6:23313:23478 [2] NCCL INFO Channel 00 : 10[201c0] -> 11[201d0] via P2P/IPC/read
11: queue1-dy-p4d24xlarge-6:23314:23481 [3] NCCL INFO Channel 00 : 11[201d0] -> 12[901c0] via P2P/IPC/read
 9: queue1-dy-p4d24xlarge-6:23312:23485 [1] NCCL INFO Channel 03 : 9[101d0] -> 10[201c0] via P2P/IPC/read
10: queue1-dy-p4d24xlarge-6:23313:23478 [2] NCCL INFO Channel 01 : 10[201c0] -> 11[201d0] via P2P/IPC/read
11: queue1-dy-p4d24xlarge-6:23314:23481 [3] NCCL INFO Channel 01 : 11[201d0] -> 12[901c0] via P2P/IPC/read
 9: queue1-dy-p4d24xlarge-6:23312:23485 [1] NCCL INFO Channel 04 : 9[101d0] -> 10[201c0] via P2P/IPC/read
10: queue1-dy-p4d24xlarge-6:23313:23478 [2] NCCL INFO Channel 02 : 10[201c0] -> 11[201d0] via P2P/IPC/read
11: queue1-dy-p4d24xlarge-6:23314:23481 [3] NCCL INFO Channel 04 : 11[201d0] -> 12[901c0] via P2P/IPC/read
 9: queue1-dy-p4d24xlarge-6:23312:23485 [1] NCCL INFO Channel 05 : 9[101d0] -> 10[201c0] via P2P/IPC/read
10: queue1-dy-p4d24xlarge-6:23313:23478 [2] NCCL INFO Channel 03 : 10[201c0] -> 11[201d0] via P2P/IPC/read
11: queue1-dy-p4d24xlarge-6:23314:23481 [3] NCCL INFO Channel 05 : 11[201d0] -> 12[901c0] via P2P/IPC/read
 9: queue1-dy-p4d24xlarge-6:23312:23485 [1] NCCL INFO Channel 06 : 9[101d0] -> 10[201c0] via P2P/IPC/read
10: queue1-dy-p4d24xlarge-6:23313:23478 [2] NCCL INFO Channel 04 : 10[201c0] -> 11[201d0] via P2P/IPC/read
 9: queue1-dy-p4d24xlarge-6:23312:23485 [1] NCCL INFO Channel 07 : 9[101d0] -> 10[201c0] via P2P/IPC/read
10: queue1-dy-p4d24xlarge-6:23313:23478 [2] NCCL INFO Channel 05 : 10[201c0] -> 11[201d0] via P2P/IPC/read
10: queue1-dy-p4d24xlarge-6:23313:23478 [2] NCCL INFO Channel 06 : 10[201c0] -> 11[201d0] via P2P/IPC/read
10: queue1-dy-p4d24xlarge-6:23313:23478 [2] NCCL INFO Channel 07 : 10[201c0] -> 11[201d0] via P2P/IPC/read
 8: queue1-dy-p4d24xlarge-6:23311:23482 [0] NCCL INFO Channel 02 : 8[101c0] -> 15[a01d0] via P2P/IPC/read
 8: queue1-dy-p4d24xlarge-6:23311:23482 [0] NCCL INFO Channel 03 : 8[101c0] -> 15[a01d0] via P2P/IPC/read
 8: queue1-dy-p4d24xlarge-6:23311:23482 [0] NCCL INFO Channel 06 : 8[101c0] -> 15[a01d0] via P2P/IPC/read
 8: queue1-dy-p4d24xlarge-6:23311:23482 [0] NCCL INFO Channel 07 : 8[101c0] -> 15[a01d0] via P2P/IPC/read
10: queue1-dy-p4d24xlarge-6:23313:23478 [2] NCCL INFO Channel 02 : 10[201c0] -> 9[101d0] via P2P/IPC/read
10: queue1-dy-p4d24xlarge-6:23313:23478 [2] NCCL INFO Channel 03 : 10[201c0] -> 9[101d0] via P2P/IPC/read
10: queue1-dy-p4d24xlarge-6:23313:23478 [2] NCCL INFO Channel 06 : 10[201c0] -> 9[101d0] via P2P/IPC/read
10: queue1-dy-p4d24xlarge-6:23313:23478 [2] NCCL INFO Channel 07 : 10[201c0] -> 9[101d0] via P2P/IPC/read
 9: queue1-dy-p4d24xlarge-6:23312:23485 [1] NCCL INFO Channel 00 : 9[101d0] -> 8[101c0] via P2P/IPC/read
 9: queue1-dy-p4d24xlarge-6:23312:23485 [1] NCCL INFO Channel 01 : 9[101d0] -> 8[101c0] via P2P/IPC/read
 9: queue1-dy-p4d24xlarge-6:23312:23485 [1] NCCL INFO Channel 04 : 9[101d0] -> 8[101c0] via P2P/IPC/read
 9: queue1-dy-p4d24xlarge-6:23312:23485 [1] NCCL INFO Channel 05 : 9[101d0] -> 8[101c0] via P2P/IPC/read
10: queue1-dy-p4d24xlarge-6:23313:23478 [2] NCCL INFO Connected all trees
10: queue1-dy-p4d24xlarge-6:23313:23478 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
10: queue1-dy-p4d24xlarge-6:23313:23478 [2] NCCL INFO 8 coll channels, 8 p2p channels, 1 p2p channels per peer
 4: queue1-dy-p4d24xlarge-5:26822:26990 [4] NCCL INFO Channel 02 : 4[901c0] -> 1[101d0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-5:26822:26990 [4] NCCL INFO Channel 03 : 4[901c0] -> 1[101d0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-5:26822:26990 [4] NCCL INFO Channel 06 : 4[901c0] -> 1[101d0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-5:26822:26990 [4] NCCL INFO Channel 07 : 4[901c0] -> 1[101d0] via P2P/IPC/read
13: queue1-dy-p4d24xlarge-6:23316:23483 [5] NCCL INFO Channel 00 : 13[901d0] -> 12[901c0] via P2P/IPC/read
13: queue1-dy-p4d24xlarge-6:23316:23483 [5] NCCL INFO Channel 01 : 13[901d0] -> 12[901c0] via P2P/IPC/read
13: queue1-dy-p4d24xlarge-6:23316:23483 [5] NCCL INFO Channel 04 : 13[901d0] -> 12[901c0] via P2P/IPC/read
13: queue1-dy-p4d24xlarge-6:23316:23483 [5] NCCL INFO Channel 05 : 13[901d0] -> 12[901c0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-5:26822:26990 [4] NCCL INFO Channel 00 : 4[901c0] -> 3[201d0] via P2P/IPC/read
 1: queue1-dy-p4d24xlarge-5:26819:26991 [1] NCCL INFO Channel 02 : 1[101d0] -> 0[101c0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-5:26822:26990 [4] NCCL INFO Channel 01 : 4[901c0] -> 3[201d0] via P2P/IPC/read
 1: queue1-dy-p4d24xlarge-5:26819:26991 [1] NCCL INFO Channel 03 : 1[101d0] -> 0[101c0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-5:26822:26990 [4] NCCL INFO Channel 04 : 4[901c0] -> 3[201d0] via P2P/IPC/read
 1: queue1-dy-p4d24xlarge-5:26819:26991 [1] NCCL INFO Channel 06 : 1[101d0] -> 0[101c0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-5:26822:26990 [4] NCCL INFO Channel 05 : 4[901c0] -> 3[201d0] via P2P/IPC/read
 1: queue1-dy-p4d24xlarge-5:26819:26991 [1] NCCL INFO Channel 07 : 1[101d0] -> 0[101c0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-6:23315:23479 [4] NCCL INFO Connected all rings
13: queue1-dy-p4d24xlarge-6:23316:23483 [5] NCCL INFO Connected all rings
14: queue1-dy-p4d24xlarge-6:23317:23480 [6] NCCL INFO Connected all rings
12: queue1-dy-p4d24xlarge-6:23315:23479 [4] NCCL INFO Channel 00 : 12[901c0] -> 13[901d0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-6:23315:23479 [4] NCCL INFO Channel 01 : 12[901c0] -> 13[901d0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-6:23315:23479 [4] NCCL INFO Channel 02 : 12[901c0] -> 13[901d0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-6:23315:23479 [4] NCCL INFO Channel 03 : 12[901c0] -> 13[901d0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-6:23315:23479 [4] NCCL INFO Channel 04 : 12[901c0] -> 13[901d0] via P2P/IPC/read
 0: queue1-dy-p4d24xlarge-5:26818:26988 [0] NCCL INFO Connected all rings
12: queue1-dy-p4d24xlarge-6:23315:23479 [4] NCCL INFO Channel 05 : 12[901c0] -> 13[901d0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-6:23315:23479 [4] NCCL INFO Channel 06 : 12[901c0] -> 13[901d0] via P2P/IPC/read
13: queue1-dy-p4d24xlarge-6:23316:23483 [5] NCCL INFO Channel 00 : 13[901d0] -> 14[a01c0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-6:23315:23479 [4] NCCL INFO Channel 07 : 12[901c0] -> 13[901d0] via P2P/IPC/read
13: queue1-dy-p4d24xlarge-6:23316:23483 [5] NCCL INFO Channel 01 : 13[901d0] -> 14[a01c0] via P2P/IPC/read
13: queue1-dy-p4d24xlarge-6:23316:23483 [5] NCCL INFO Channel 02 : 13[901d0] -> 14[a01c0] via P2P/IPC/read
14: queue1-dy-p4d24xlarge-6:23317:23480 [6] NCCL INFO Channel 00 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read
13: queue1-dy-p4d24xlarge-6:23316:23483 [5] NCCL INFO Channel 03 : 13[901d0] -> 14[a01c0] via P2P/IPC/read
14: queue1-dy-p4d24xlarge-6:23317:23480 [6] NCCL INFO Channel 01 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read
 0: queue1-dy-p4d24xlarge-5:26818:26988 [0] NCCL INFO Channel 00 : 0[101c0] -> 1[101d0] via P2P/IPC/read
13: queue1-dy-p4d24xlarge-6:23316:23483 [5] NCCL INFO Channel 04 : 13[901d0] -> 14[a01c0] via P2P/IPC/read
14: queue1-dy-p4d24xlarge-6:23317:23480 [6] NCCL INFO Channel 02 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read
13: queue1-dy-p4d24xlarge-6:23316:23483 [5] NCCL INFO Channel 05 : 13[901d0] -> 14[a01c0] via P2P/IPC/read
14: queue1-dy-p4d24xlarge-6:23317:23480 [6] NCCL INFO Channel 03 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read
13: queue1-dy-p4d24xlarge-6:23316:23483 [5] NCCL INFO Channel 06 : 13[901d0] -> 14[a01c0] via P2P/IPC/read
14: queue1-dy-p4d24xlarge-6:23317:23480 [6] NCCL INFO Channel 04 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read
13: queue1-dy-p4d24xlarge-6:23316:23483 [5] NCCL INFO Channel 07 : 13[901d0] -> 14[a01c0] via P2P/IPC/read
14: queue1-dy-p4d24xlarge-6:23317:23480 [6] NCCL INFO Channel 05 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read
 0: queue1-dy-p4d24xlarge-5:26818:26988 [0] NCCL INFO Channel 01 : 0[101c0] -> 1[101d0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-5:26822:26990 [4] NCCL INFO Connected all rings
14: queue1-dy-p4d24xlarge-6:23317:23480 [6] NCCL INFO Channel 06 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read
11: queue1-dy-p4d24xlarge-6:23314:23481 [3] NCCL INFO Connected all trees
11: queue1-dy-p4d24xlarge-6:23314:23481 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
11: queue1-dy-p4d24xlarge-6:23314:23481 [3] NCCL INFO 8 coll channels, 8 p2p channels, 1 p2p channels per peer
 1: queue1-dy-p4d24xlarge-5:26819:26991 [1] NCCL INFO Connected all rings
14: queue1-dy-p4d24xlarge-6:23317:23480 [6] NCCL INFO Channel 07 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read
 0: queue1-dy-p4d24xlarge-5:26818:26988 [0] NCCL INFO Channel 02 : 0[101c0] -> 1[101d0] via P2P/IPC/read
 0: queue1-dy-p4d24xlarge-5:26818:26988 [0] NCCL INFO Channel 03 : 0[101c0] -> 1[101d0] via P2P/IPC/read
 5: queue1-dy-p4d24xlarge-5:26823:26989 [5] NCCL INFO Connected all rings
 3: queue1-dy-p4d24xlarge-5:26821:26993 [3] NCCL INFO Connected all rings
 2: queue1-dy-p4d24xlarge-5:26820:26992 [2] NCCL INFO Connected all rings
 0: queue1-dy-p4d24xlarge-5:26818:26988 [0] NCCL INFO Channel 04 : 0[101c0] -> 1[101d0] via P2P/IPC/read
 0: queue1-dy-p4d24xlarge-5:26818:26988 [0] NCCL INFO Channel 05 : 0[101c0] -> 1[101d0] via P2P/IPC/read
 0: queue1-dy-p4d24xlarge-5:26818:26988 [0] NCCL INFO Channel 06 : 0[101c0] -> 1[101d0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-5:26822:26990 [4] NCCL INFO Channel 00 : 4[901c0] -> 5[901d0] via P2P/IPC/read
 0: queue1-dy-p4d24xlarge-5:26818:26988 [0] NCCL INFO Channel 07 : 0[101c0] -> 1[101d0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-5:26822:26990 [4] NCCL INFO Channel 01 : 4[901c0] -> 5[901d0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-5:26822:26990 [4] NCCL INFO Channel 02 : 4[901c0] -> 5[901d0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-5:26822:26990 [4] NCCL INFO Channel 03 : 4[901c0] -> 5[901d0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-5:26822:26990 [4] NCCL INFO Channel 04 : 4[901c0] -> 5[901d0] via P2P/IPC/read
 1: queue1-dy-p4d24xlarge-5:26819:26991 [1] NCCL INFO Channel 00 : 1[101d0] -> 2[201c0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-5:26822:26990 [4] NCCL INFO Channel 05 : 4[901c0] -> 5[901d0] via P2P/IPC/read
 1: queue1-dy-p4d24xlarge-5:26819:26991 [1] NCCL INFO Channel 01 : 1[101d0] -> 2[201c0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-5:26822:26990 [4] NCCL INFO Channel 06 : 4[901c0] -> 5[901d0] via P2P/IPC/read
 1: queue1-dy-p4d24xlarge-5:26819:26991 [1] NCCL INFO Channel 02 : 1[101d0] -> 2[201c0] via P2P/IPC/read
 5: queue1-dy-p4d24xlarge-5:26823:26989 [5] NCCL INFO Channel 00 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
 3: queue1-dy-p4d24xlarge-5:26821:26993 [3] NCCL INFO Channel 00 : 3[201d0] -> 4[901c0] via P2P/IPC/read
 2: queue1-dy-p4d24xlarge-5:26820:26992 [2] NCCL INFO Channel 00 : 2[201c0] -> 3[201d0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-5:26822:26990 [4] NCCL INFO Channel 07 : 4[901c0] -> 5[901d0] via P2P/IPC/read
 1: queue1-dy-p4d24xlarge-5:26819:26991 [1] NCCL INFO Channel 03 : 1[101d0] -> 2[201c0] via P2P/IPC/read
 5: queue1-dy-p4d24xlarge-5:26823:26989 [5] NCCL INFO Channel 01 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
 3: queue1-dy-p4d24xlarge-5:26821:26993 [3] NCCL INFO Channel 01 : 3[201d0] -> 4[901c0] via P2P/IPC/read
 2: queue1-dy-p4d24xlarge-5:26820:26992 [2] NCCL INFO Channel 01 : 2[201c0] -> 3[201d0] via P2P/IPC/read
 1: queue1-dy-p4d24xlarge-5:26819:26991 [1] NCCL INFO Channel 04 : 1[101d0] -> 2[201c0] via P2P/IPC/read
 5: queue1-dy-p4d24xlarge-5:26823:26989 [5] NCCL INFO Channel 02 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
 3: queue1-dy-p4d24xlarge-5:26821:26993 [3] NCCL INFO Channel 04 : 3[201d0] -> 4[901c0] via P2P/IPC/read
 2: queue1-dy-p4d24xlarge-5:26820:26992 [2] NCCL INFO Channel 02 : 2[201c0] -> 3[201d0] via P2P/IPC/read
 5: queue1-dy-p4d24xlarge-5:26823:26989 [5] NCCL INFO Channel 03 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
 1: queue1-dy-p4d24xlarge-5:26819:26991 [1] NCCL INFO Channel 05 : 1[101d0] -> 2[201c0] via P2P/IPC/read
 3: queue1-dy-p4d24xlarge-5:26821:26993 [3] NCCL INFO Channel 05 : 3[201d0] -> 4[901c0] via P2P/IPC/read
 2: queue1-dy-p4d24xlarge-5:26820:26992 [2] NCCL INFO Channel 03 : 2[201c0] -> 3[201d0] via P2P/IPC/read
 5: queue1-dy-p4d24xlarge-5:26823:26989 [5] NCCL INFO Channel 04 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
 1: queue1-dy-p4d24xlarge-5:26819:26991 [1] NCCL INFO Channel 06 : 1[101d0] -> 2[201c0] via P2P/IPC/read
 2: queue1-dy-p4d24xlarge-5:26820:26992 [2] NCCL INFO Channel 04 : 2[201c0] -> 3[201d0] via P2P/IPC/read
 5: queue1-dy-p4d24xlarge-5:26823:26989 [5] NCCL INFO Channel 05 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
 1: queue1-dy-p4d24xlarge-5:26819:26991 [1] NCCL INFO Channel 07 : 1[101d0] -> 2[201c0] via P2P/IPC/read
 2: queue1-dy-p4d24xlarge-5:26820:26992 [2] NCCL INFO Channel 05 : 2[201c0] -> 3[201d0] via P2P/IPC/read
 5: queue1-dy-p4d24xlarge-5:26823:26989 [5] NCCL INFO Channel 06 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
 2: queue1-dy-p4d24xlarge-5:26820:26992 [2] NCCL INFO Channel 06 : 2[201c0] -> 3[201d0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-6:23315:23479 [4] NCCL INFO Channel 02 : 4[901c0] -> 12[901c0] [receive] via NET/Socket/2
 5: queue1-dy-p4d24xlarge-5:26823:26989 [5] NCCL INFO Channel 07 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-6:23315:23479 [4] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
 2: queue1-dy-p4d24xlarge-5:26820:26992 [2] NCCL INFO Channel 07 : 2[201c0] -> 3[201d0] via P2P/IPC/read
14: queue1-dy-p4d24xlarge-6:23317:23480 [6] NCCL INFO Connected all trees
14: queue1-dy-p4d24xlarge-6:23317:23480 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
14: queue1-dy-p4d24xlarge-6:23317:23480 [6] NCCL INFO 8 coll channels, 8 p2p channels, 1 p2p channels per peer
13: queue1-dy-p4d24xlarge-6:23316:23483 [5] NCCL INFO Channel 02 : 13[901d0] -> 12[901c0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-6:23315:23479 [4] NCCL INFO Channel 03 : 4[901c0] -> 12[901c0] [receive] via NET/Socket/3
12: queue1-dy-p4d24xlarge-6:23315:23479 [4] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
13: queue1-dy-p4d24xlarge-6:23316:23483 [5] NCCL INFO Channel 03 : 13[901d0] -> 12[901c0] via P2P/IPC/read
13: queue1-dy-p4d24xlarge-6:23316:23483 [5] NCCL INFO Channel 06 : 13[901d0] -> 12[901c0] via P2P/IPC/read
15: queue1-dy-p4d24xlarge-6:23318:23484 [7] NCCL INFO Channel 02 : 15[a01d0] -> 8[101c0] via P2P/IPC/read
13: queue1-dy-p4d24xlarge-6:23316:23483 [5] NCCL INFO Channel 07 : 13[901d0] -> 12[901c0] via P2P/IPC/read
15: queue1-dy-p4d24xlarge-6:23318:23484 [7] NCCL INFO Channel 03 : 15[a01d0] -> 8[101c0] via P2P/IPC/read
15: queue1-dy-p4d24xlarge-6:23318:23484 [7] NCCL INFO Channel 06 : 15[a01d0] -> 8[101c0] via P2P/IPC/read
15: queue1-dy-p4d24xlarge-6:23318:23484 [7] NCCL INFO Channel 07 : 15[a01d0] -> 8[101c0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-6:23315:23479 [4] NCCL INFO Channel 06 : 4[901c0] -> 12[901c0] [receive] via NET/Socket/2
12: queue1-dy-p4d24xlarge-6:23315:23479 [4] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
 8: queue1-dy-p4d24xlarge-6:23311:23482 [0] NCCL INFO Channel 00 : 0[101c0] -> 8[101c0] [receive] via NET/Socket/0
 8: queue1-dy-p4d24xlarge-6:23311:23482 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
12: queue1-dy-p4d24xlarge-6:23315:23479 [4] NCCL INFO Channel 07 : 4[901c0] -> 12[901c0] [receive] via NET/Socket/3
12: queue1-dy-p4d24xlarge-6:23315:23479 [4] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
 8: queue1-dy-p4d24xlarge-6:23311:23482 [0] NCCL INFO Channel 01 : 0[101c0] -> 8[101c0] [receive] via NET/Socket/1
 8: queue1-dy-p4d24xlarge-6:23311:23482 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
 0: queue1-dy-p4d24xlarge-5:26818:26988 [0] NCCL INFO Channel 02 : 0[101c0] -> 7[a01d0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-6:23315:23479 [4] NCCL INFO Channel 02 : 12[901c0] -> 4[901c0] [send] via NET/Socket/2
 8: queue1-dy-p4d24xlarge-6:23311:23482 [0] NCCL INFO Channel 04 : 0[101c0] -> 8[101c0] [receive] via NET/Socket/0
 8: queue1-dy-p4d24xlarge-6:23311:23482 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
 0: queue1-dy-p4d24xlarge-5:26818:26988 [0] NCCL INFO Channel 03 : 0[101c0] -> 7[a01d0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-6:23315:23479 [4] NCCL INFO Channel 03 : 12[901c0] -> 4[901c0] [send] via NET/Socket/3
 8: queue1-dy-p4d24xlarge-6:23311:23482 [0] NCCL INFO Channel 05 : 0[101c0] -> 8[101c0] [receive] via NET/Socket/1
 8: queue1-dy-p4d24xlarge-6:23311:23482 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
 0: queue1-dy-p4d24xlarge-5:26818:26988 [0] NCCL INFO Channel 06 : 0[101c0] -> 7[a01d0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-6:23315:23479 [4] NCCL INFO Channel 06 : 12[901c0] -> 4[901c0] [send] via NET/Socket/2
 8: queue1-dy-p4d24xlarge-6:23311:23482 [0] NCCL INFO Channel 00 : 8[101c0] -> 0[101c0] [send] via NET/Socket/0
 0: queue1-dy-p4d24xlarge-5:26818:26988 [0] NCCL INFO Channel 07 : 0[101c0] -> 7[a01d0] via P2P/IPC/read
 3: queue1-dy-p4d24xlarge-5:26821:26993 [3] NCCL INFO Connected all trees
 3: queue1-dy-p4d24xlarge-5:26821:26993 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
 3: queue1-dy-p4d24xlarge-5:26821:26993 [3] NCCL INFO 8 coll channels, 8 p2p channels, 1 p2p channels per peer
12: queue1-dy-p4d24xlarge-6:23315:23479 [4] NCCL INFO Channel 07 : 12[901c0] -> 4[901c0] [send] via NET/Socket/3
 8: queue1-dy-p4d24xlarge-6:23311:23482 [0] NCCL INFO Channel 01 : 8[101c0] -> 0[101c0] [send] via NET/Socket/1
 4: queue1-dy-p4d24xlarge-5:26822:26990 [4] NCCL INFO Channel 02 : 12[901c0] -> 4[901c0] [receive] via NET/Socket/2
 4: queue1-dy-p4d24xlarge-5:26822:26990 [4] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
 8: queue1-dy-p4d24xlarge-6:23311:23482 [0] NCCL INFO Channel 04 : 8[101c0] -> 0[101c0] [send] via NET/Socket/0
 6: queue1-dy-p4d24xlarge-5:26824:26994 [6] NCCL INFO Connected all trees
 6: queue1-dy-p4d24xlarge-5:26824:26994 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
 6: queue1-dy-p4d24xlarge-5:26824:26994 [6] NCCL INFO 8 coll channels, 8 p2p channels, 1 p2p channels per peer
 8: queue1-dy-p4d24xlarge-6:23311:23482 [0] NCCL INFO Channel 05 : 8[101c0] -> 0[101c0] [send] via NET/Socket/1
 7: queue1-dy-p4d24xlarge-5:26825:26995 [7] NCCL INFO Channel 02 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
 5: queue1-dy-p4d24xlarge-5:26823:26989 [5] NCCL INFO Channel 02 : 5[901d0] -> 4[901c0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-5:26822:26990 [4] NCCL INFO Channel 03 : 12[901c0] -> 4[901c0] [receive] via NET/Socket/3
 4: queue1-dy-p4d24xlarge-5:26822:26990 [4] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
 2: queue1-dy-p4d24xlarge-5:26820:26992 [2] NCCL INFO Channel 02 : 2[201c0] -> 1[101d0] via P2P/IPC/read
 5: queue1-dy-p4d24xlarge-5:26823:26989 [5] NCCL INFO Channel 03 : 5[901d0] -> 4[901c0] via P2P/IPC/read
 7: queue1-dy-p4d24xlarge-5:26825:26995 [7] NCCL INFO Channel 03 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
 2: queue1-dy-p4d24xlarge-5:26820:26992 [2] NCCL INFO Channel 03 : 2[201c0] -> 1[101d0] via P2P/IPC/read
 5: queue1-dy-p4d24xlarge-5:26823:26989 [5] NCCL INFO Channel 06 : 5[901d0] -> 4[901c0] via P2P/IPC/read
 7: queue1-dy-p4d24xlarge-5:26825:26995 [7] NCCL INFO Channel 06 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
 2: queue1-dy-p4d24xlarge-5:26820:26992 [2] NCCL INFO Channel 06 : 2[201c0] -> 1[101d0] via P2P/IPC/read
 5: queue1-dy-p4d24xlarge-5:26823:26989 [5] NCCL INFO Channel 07 : 5[901d0] -> 4[901c0] via P2P/IPC/read
 7: queue1-dy-p4d24xlarge-5:26825:26995 [7] NCCL INFO Channel 07 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
 0: queue1-dy-p4d24xlarge-5:26818:26988 [0] NCCL INFO Channel 00 : 8[101c0] -> 0[101c0] [receive] via NET/Socket/0
 2: queue1-dy-p4d24xlarge-5:26820:26992 [2] NCCL INFO Channel 07 : 2[201c0] -> 1[101d0] via P2P/IPC/read
 0: queue1-dy-p4d24xlarge-5:26818:26988 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
 1: queue1-dy-p4d24xlarge-5:26819:26991 [1] NCCL INFO Channel 00 : 1[101d0] -> 0[101c0] via P2P/IPC/read
 1: queue1-dy-p4d24xlarge-5:26819:26991 [1] NCCL INFO Channel 01 : 1[101d0] -> 0[101c0] via P2P/IPC/read
 1: queue1-dy-p4d24xlarge-5:26819:26991 [1] NCCL INFO Channel 04 : 1[101d0] -> 0[101c0] via P2P/IPC/read
 1: queue1-dy-p4d24xlarge-5:26819:26991 [1] NCCL INFO Channel 05 : 1[101d0] -> 0[101c0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-5:26822:26990 [4] NCCL INFO Channel 06 : 12[901c0] -> 4[901c0] [receive] via NET/Socket/2
 4: queue1-dy-p4d24xlarge-5:26822:26990 [4] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
 2: queue1-dy-p4d24xlarge-5:26820:26992 [2] NCCL INFO Connected all trees
 2: queue1-dy-p4d24xlarge-5:26820:26992 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
 2: queue1-dy-p4d24xlarge-5:26820:26992 [2] NCCL INFO 8 coll channels, 8 p2p channels, 1 p2p channels per peer
 0: queue1-dy-p4d24xlarge-5:26818:26988 [0] NCCL INFO Channel 01 : 8[101c0] -> 0[101c0] [receive] via NET/Socket/1
 0: queue1-dy-p4d24xlarge-5:26818:26988 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
 4: queue1-dy-p4d24xlarge-5:26822:26990 [4] NCCL INFO Channel 07 : 12[901c0] -> 4[901c0] [receive] via NET/Socket/3
 4: queue1-dy-p4d24xlarge-5:26822:26990 [4] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
 0: queue1-dy-p4d24xlarge-5:26818:26988 [0] NCCL INFO Channel 04 : 8[101c0] -> 0[101c0] [receive] via NET/Socket/0
 0: queue1-dy-p4d24xlarge-5:26818:26988 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
 4: queue1-dy-p4d24xlarge-5:26822:26990 [4] NCCL INFO Channel 02 : 4[901c0] -> 12[901c0] [send] via NET/Socket/2
 0: queue1-dy-p4d24xlarge-5:26818:26988 [0] NCCL INFO Channel 05 : 8[101c0] -> 0[101c0] [receive] via NET/Socket/1
 0: queue1-dy-p4d24xlarge-5:26818:26988 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
 4: queue1-dy-p4d24xlarge-5:26822:26990 [4] NCCL INFO Channel 03 : 4[901c0] -> 12[901c0] [send] via NET/Socket/3
 0: queue1-dy-p4d24xlarge-5:26818:26988 [0] NCCL INFO Channel 00 : 0[101c0] -> 8[101c0] [send] via NET/Socket/0
 4: queue1-dy-p4d24xlarge-5:26822:26990 [4] NCCL INFO Channel 06 : 4[901c0] -> 12[901c0] [send] via NET/Socket/2
 0: queue1-dy-p4d24xlarge-5:26818:26988 [0] NCCL INFO Channel 01 : 0[101c0] -> 8[101c0] [send] via NET/Socket/1
 4: queue1-dy-p4d24xlarge-5:26822:26990 [4] NCCL INFO Channel 07 : 4[901c0] -> 12[901c0] [send] via NET/Socket/3
 0: queue1-dy-p4d24xlarge-5:26818:26988 [0] NCCL INFO Channel 04 : 0[101c0] -> 8[101c0] [send] via NET/Socket/0
 0: queue1-dy-p4d24xlarge-5:26818:26988 [0] NCCL INFO Channel 05 : 0[101c0] -> 8[101c0] [send] via NET/Socket/1
 4: queue1-dy-p4d24xlarge-5:26822:26990 [4] NCCL INFO Connected all trees
 4: queue1-dy-p4d24xlarge-5:26822:26990 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
 4: queue1-dy-p4d24xlarge-5:26822:26990 [4] NCCL INFO 8 coll channels, 8 p2p channels, 1 p2p channels per peer
 5: queue1-dy-p4d24xlarge-5:26823:26989 [5] NCCL INFO Connected all trees
 5: queue1-dy-p4d24xlarge-5:26823:26989 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
 5: queue1-dy-p4d24xlarge-5:26823:26989 [5] NCCL INFO 8 coll channels, 8 p2p channels, 1 p2p channels per peer
 7: queue1-dy-p4d24xlarge-5:26825:26995 [7] NCCL INFO Connected all trees
 7: queue1-dy-p4d24xlarge-5:26825:26995 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
 7: queue1-dy-p4d24xlarge-5:26825:26995 [7] NCCL INFO 8 coll channels, 8 p2p channels, 1 p2p channels per peer
15: queue1-dy-p4d24xlarge-6:23318:23484 [7] NCCL INFO Connected all trees
15: queue1-dy-p4d24xlarge-6:23318:23484 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
15: queue1-dy-p4d24xlarge-6:23318:23484 [7] NCCL INFO 8 coll channels, 8 p2p channels, 1 p2p channels per peer
 0: queue1-dy-p4d24xlarge-5:26818:26988 [0] NCCL INFO Connected all trees
 0: queue1-dy-p4d24xlarge-5:26818:26988 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
 0: queue1-dy-p4d24xlarge-5:26818:26988 [0] NCCL INFO 8 coll channels, 8 p2p channels, 1 p2p channels per peer
 8: queue1-dy-p4d24xlarge-6:23311:23482 [0] NCCL INFO Connected all trees
 8: queue1-dy-p4d24xlarge-6:23311:23482 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
 8: queue1-dy-p4d24xlarge-6:23311:23482 [0] NCCL INFO 8 coll channels, 8 p2p channels, 1 p2p channels per peer
 1: queue1-dy-p4d24xlarge-5:26819:26991 [1] NCCL INFO Connected all trees
 1: queue1-dy-p4d24xlarge-5:26819:26991 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
 1: queue1-dy-p4d24xlarge-5:26819:26991 [1] NCCL INFO 8 coll channels, 8 p2p channels, 1 p2p channels per peer
 0: queue1-dy-p4d24xlarge-5:26818:26988 [0] NCCL INFO comm 0x7fd070002fb0 rank 0 nranks 16 cudaDev 0 busId 101c0 - Init COMPLETE
 9: queue1-dy-p4d24xlarge-6:23312:23485 [1] NCCL INFO Connected all trees
 5: queue1-dy-p4d24xlarge-5:26823:26989 [5] NCCL INFO comm 0x7f598c002fb0 rank 5 nranks 16 cudaDev 5 busId 901d0 - Init COMPLETE
 7: queue1-dy-p4d24xlarge-5:26825:26995 [7] NCCL INFO comm 0x7f79c0002fb0 rank 7 nranks 16 cudaDev 7 busId a01d0 - Init COMPLETE
 1: queue1-dy-p4d24xlarge-5:26819:26991 [1] NCCL INFO comm 0x7f929c002fb0 rank 1 nranks 16 cudaDev 1 busId 101d0 - Init COMPLETE
 4: queue1-dy-p4d24xlarge-5:26822:26990 [4] NCCL INFO comm 0x7f53ec002fb0 rank 4 nranks 16 cudaDev 4 busId 901c0 - Init COMPLETE
 6: queue1-dy-p4d24xlarge-5:26824:26994 [6] NCCL INFO comm 0x7f1060002fb0 rank 6 nranks 16 cudaDev 6 busId a01c0 - Init COMPLETE
 2: queue1-dy-p4d24xlarge-5:26820:26992 [2] NCCL INFO comm 0x7f7fc0002fb0 rank 2 nranks 16 cudaDev 2 busId 201c0 - Init COMPLETE
 3: queue1-dy-p4d24xlarge-5:26821:26993 [3] NCCL INFO comm 0x7f440c002fb0 rank 3 nranks 16 cudaDev 3 busId 201d0 - Init COMPLETE
 9: queue1-dy-p4d24xlarge-6:23312:23485 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
 9: queue1-dy-p4d24xlarge-6:23312:23485 [1] NCCL INFO 8 coll channels, 8 p2p channels, 1 p2p channels per peer
 0: queue1-dy-p4d24xlarge-5:26818:26818 [0] NCCL INFO Launch mode Parallel
 0: 2022-11-03 06:13:15 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.1, activation_fn='silu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=True, adaptive_input_cutoff='20000,60000', adaptive_input_factor=4, adaptive_softmax_cutoff='20000,60000', adaptive_softmax_dropout=0.2, adaptive_softmax_factor=4, add_bos_token=False, all_gather_list_size=16384, arch='mega_lm_adaptive_big', attention_activation_fn='softmax', attention_dropout=0.1, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', clip_mode='total', clip_norm=0.25, cpu=False, criterion='adaptive_loss', curriculum=0, data='/fsx/datasets/wikitext-103', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', decoder_chunk_size=1024, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_hidden_dim=2048, decoder_input_dim=1024, decoder_layers=16, decoder_n_dim=16, decoder_z_dim=256, device_id=0, disable_validation=False, distributed_b
 0: ackend='nccl', distributed_init_method='env://', distributed_no_spawn=True, distributed_port=-1, distributed_rank=0, distributed_world_size=16, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, end_learning_rate=0.0, fast_stat_sync=False, feature_dropout=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, future_target=False, hidden_dropout=0.1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, localsgd_frequency=3, log_format='simple', log_interval=100, lr=[0.005], lr_scheduler='linear_decay', max_epoch=0, max_sentences=None, max_sentences_valid=None, max_target_positions=8096, max_tokens=6144, max_tokens_valid=6144, max_update=400000, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_affine_final_norm=True
 0: , no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, normalization_type='layernorm', normalize_before=True, normalize_embedding=False, nprocs_per_node=8, num_workers=0, optimizer='adam', optimizer_overrides='{}', output_dictionary_size=-1, past_target=False, patience=-1, profile=False, quantization_config_path=None, rel_pos_bias='rotary', report_ema_alpha=False, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sample_break_mode='complete', save_dir='saved_models/cu116_oss_1121', save_interval=1, save_interval_updates=0, seed=42, self_target=False, sentence_avg=False, share_all_embeddings=False, share_decoder_input_output_embed=False, shorten_data_split_list='', shorten_method='none', skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop
 0: _min_lr=-1, stop_time_hours=0, task='language_modeling', tensorboard_logdir='', test_subset='test', threshold_loss_scale=None, tie_adaptive_proj=True, tie_adaptive_weights=True, tokenizer=None, tokens_per_sample=2048, total_num_update=400000, tpu=False, train_subset='train', truncation_length=8192, update_freq=[1], use_bmuf=False, use_old_adam=False, user_dir=None, valid_block='splits:10', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, variant_block_multiple_max=6, variant_block_multiple_min=2, wandb_entity=None, wandb_id=None, wandb_project=None, warmup_init_lr=1e-07, warmup_power=1, warmup_updates=24000, weight_decay=0.1, write_out_alpha=False)
12: queue1-dy-p4d24xlarge-6:23315:23479 [4] NCCL INFO Connected all trees
12: queue1-dy-p4d24xlarge-6:23315:23479 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
12: queue1-dy-p4d24xlarge-6:23315:23479 [4] NCCL INFO 8 coll channels, 8 p2p channels, 1 p2p channels per peer
13: queue1-dy-p4d24xlarge-6:23316:23483 [5] NCCL INFO Connected all trees
13: queue1-dy-p4d24xlarge-6:23316:23483 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
13: queue1-dy-p4d24xlarge-6:23316:23483 [5] NCCL INFO 8 coll channels, 8 p2p channels, 1 p2p channels per peer
 8: queue1-dy-p4d24xlarge-6:23311:23482 [0] NCCL INFO comm 0x7f2104002fb0 rank 8 nranks 16 cudaDev 0 busId 101c0 - Init COMPLETE
12: queue1-dy-p4d24xlarge-6:23315:23479 [4] NCCL INFO comm 0x7fdf08002fb0 rank 12 nranks 16 cudaDev 4 busId 901c0 - Init COMPLETE
13: queue1-dy-p4d24xlarge-6:23316:23483 [5] NCCL INFO comm 0x7fe8a4002fb0 rank 13 nranks 16 cudaDev 5 busId 901d0 - Init COMPLETE
14: queue1-dy-p4d24xlarge-6:23317:23480 [6] NCCL INFO comm 0x7f40a0002fb0 rank 14 nranks 16 cudaDev 6 busId a01c0 - Init COMPLETE
15: queue1-dy-p4d24xlarge-6:23318:23484 [7] NCCL INFO comm 0x7f5990002fb0 rank 15 nranks 16 cudaDev 7 busId a01d0 - Init COMPLETE
 9: queue1-dy-p4d24xlarge-6:23312:23485 [1] NCCL INFO comm 0x7fa498002fb0 rank 9 nranks 16 cudaDev 1 busId 101d0 - Init COMPLETE
10: queue1-dy-p4d24xlarge-6:23313:23478 [2] NCCL INFO comm 0x7f97b0002fb0 rank 10 nranks 16 cudaDev 2 busId 201c0 - Init COMPLETE
11: queue1-dy-p4d24xlarge-6:23314:23481 [3] NCCL INFO comm 0x7fb790002fb0 rank 11 nranks 16 cudaDev 3 busId 201d0 - Init COMPLETE
 0: 2022-11-03 06:13:15 | INFO | fairseq.tasks.language_modeling | dictionary: 267744 types
 0: 2022-11-03 06:13:15 | INFO | fairseq.data.data_utils | loaded 3760 examples from: /fsx/datasets/wikitext-103/valid
 0: 2022-11-03 06:13:20 | INFO | fairseq_cli.train | MegaLanguageModel(
 0:   (decoder): MegaDecoderNoCrossAttn(
 0:     (embedding_dropout): FairseqDropout(p=0.3)
 0:     (embed_tokens): AdaptiveInput(
 0:       (embeddings): ModuleList(
 0:         (0): Sequential(
 0:           (0): Embedding(20000, 1024, padding_idx=1)
 0:           (1): Linear(in_features=1024, out_features=1024, bias=False)
 0:         )
 0:         (1): Sequential(
 0:           (0): Embedding(40000, 256)
 0:           (1): Linear(in_features=256, out_features=1024, bias=False)
 0:         )
 0:         (2): Sequential(
 0:           (0): Embedding(207744, 64)
 0:           (1): Linear(in_features=64, out_features=1024, bias=False)
 0:         )
 0:       )
 0:     )
 0:     (layers): ModuleList(
 0:       (0): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:       (1): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:       (2): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:       (3): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:       (4): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:       (5): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:       (6): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:       (7): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:       (8): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:       (9): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:       (10): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:       (11): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:       (12): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:       (13): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:       (14): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:       (15): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:     )
 0:     (final_norm): SequenceNorm(
 0:       (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=False)
 0:     )
 0:     (adaptive_softmax): AdaptiveSoftmax(
 0:       (dropout_module): FairseqDropout(p=0.2)
 0:       (lsm): LogSoftmax(dim=1)
 0:       (head): TiedHeadModule(
 0:         (word_proj): TiedLinear()
 0:         (class_proj): Linear(in_features=1024, out_features=2, bias=False)
 0:       )
 0:       (tail): ModuleList(
 0:         (0): Sequential(
 0:           (0): TiedLinear()
 0:           (1): Dropout(p=0.2, inplace=False)
 0:           (2): TiedLinear()
 0:         )
 0:         (1): Sequential(
 0:           (0): TiedLinear()
 0:           (1): Dropout(p=0.2, inplace=False)
 0:           (2): TiedLinear()
 0:         )
 0:       )
 0:     )
 0:   )
 0: )
 0: 2022-11-03 06:13:20 | INFO | fairseq_cli.train | task: language_modeling (LanguageModelingTask)
 0: 2022-11-03 06:13:20 | INFO | fairseq_cli.train | model: mega_lm_adaptive_big (MegaLanguageModel)
 0: 2022-11-03 06:13:20 | INFO | fairseq_cli.train | criterion: adaptive_loss (AdaptiveLoss)
 0: 2022-11-03 06:13:20 | INFO | fairseq_cli.train | num. model params: 252237824 (num. trained: 252237824)
 0: 2022-11-03 06:13:20 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.embeddings.0.0.weight <- decoder.adaptive_softmax.head.word_proj.weight
 0: 2022-11-03 06:13:20 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.embeddings.0.1.bias <- decoder.embed_tokens.embeddings.1.1.bias
 0: 2022-11-03 06:13:20 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.embeddings.0.1.bias <- decoder.embed_tokens.embeddings.2.1.bias
 0: 2022-11-03 06:13:20 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.embeddings.0.1.bias <- decoder.final_norm.norm.weight
 0: 2022-11-03 06:13:20 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.embeddings.0.1.bias <- decoder.final_norm.norm.bias
 0: 2022-11-03 06:13:20 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.embeddings.0.1.bias <- decoder.adaptive_softmax.head.class_proj.bias
 0: 2022-11-03 06:13:20 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.embeddings.1.0.weight <- decoder.adaptive_softmax.tail.0.2.weight
 0: 2022-11-03 06:13:20 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.embeddings.1.1.weight <- decoder.adaptive_softmax.tail.0.0.weight
 0: 2022-11-03 06:13:20 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.embeddings.2.0.weight <- decoder.adaptive_softmax.tail.1.2.weight
 0: 2022-11-03 06:13:20 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.embeddings.2.1.weight <- decoder.adaptive_softmax.tail.1.0.weight
 0: 2022-11-03 06:13:20 | INFO | fairseq.utils | ***********************CUDA enviroments for all 16 workers***********************
 0: 2022-11-03 06:13:20 | INFO | fairseq.utils | rank   0: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-03 06:13:20 | INFO | fairseq.utils | rank   1: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-03 06:13:20 | INFO | fairseq.utils | rank   2: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-03 06:13:20 | INFO | fairseq.utils | rank   3: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-03 06:13:20 | INFO | fairseq.utils | rank   4: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-03 06:13:20 | INFO | fairseq.utils | rank   5: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-03 06:13:20 | INFO | fairseq.utils | rank   6: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-03 06:13:20 | INFO | fairseq.utils | rank   7: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-03 06:13:20 | INFO | fairseq.utils | rank   8: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-03 06:13:20 | INFO | fairseq.utils | rank   9: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-03 06:13:20 | INFO | fairseq.utils | rank  10: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-03 06:13:20 | INFO | fairseq.utils | rank  11: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-03 06:13:20 | INFO | fairseq.utils | rank  12: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-03 06:13:20 | INFO | fairseq.utils | rank  13: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-03 06:13:20 | INFO | fairseq.utils | rank  14: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-03 06:13:20 | INFO | fairseq.utils | rank  15: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-03 06:13:20 | INFO | fairseq.utils | ***********************CUDA enviroments for all 16 workers***********************
 0: 2022-11-03 06:13:20 | INFO | fairseq_cli.train | training on 16 devices (GPUs/TPUs)
 0: 2022-11-03 06:13:20 | INFO | fairseq_cli.train | max tokens per GPU = 6144 and max sentences per GPU = None
 0: 2022-11-03 06:13:20 | INFO | fairseq.trainer | no existing checkpoint found saved_models/cu116_oss_1121/checkpoint_last.pt
 0: 2022-11-03 06:13:20 | INFO | fairseq.trainer | loading train data for epoch 1
 0: 2022-11-03 06:13:20 | INFO | fairseq.data.data_utils | loaded 1801350 examples from: /fsx/datasets/wikitext-103/train
 0: 2022-11-03 06:13:21 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16
 0: 2022-11-03 06:13:21 | INFO | fairseq.optim.adam | using FusedAdam
 0: 2022-11-03 06:13:21 | INFO | fairseq.trainer | begin training epoch 1
 0: 2022-11-03 06:15:05 | INFO | train_inner | epoch 001:    100 / 1230 loss=13.75, ppl=13779, wps=81118.1, ups=0.97, wpb=83611.2, bsz=20.6, num_updates=100, lr=2.09329e-05, gnorm=2.509, clip=100, train_wall=105, wall=106
 0: 2022-11-03 06:16:46 | INFO | train_inner | epoch 001:    200 / 1230 loss=11.124, ppl=2232.32, wps=83966.4, ups=1, wpb=84167.9, bsz=20.8, num_updates=200, lr=4.17658e-05, gnorm=0.55, clip=100, train_wall=100, wall=206
 0: 2022-11-03 06:18:26 | INFO | train_inner | epoch 001:    300 / 1230 loss=10.574, ppl=1524.54, wps=83782.7, ups=1, wpb=84139.5, bsz=20.9, num_updates=300, lr=6.25987e-05, gnorm=0.544, clip=100, train_wall=100, wall=306
 0: 2022-11-03 06:20:07 | INFO | train_inner | epoch 001:    400 / 1230 loss=10.12, ppl=1112.54, wps=83264, ups=0.99, wpb=83759.7, bsz=20.9, num_updates=400, lr=8.34317e-05, gnorm=0.586, clip=100, train_wall=100, wall=407
 0: 2022-11-03 06:21:47 | INFO | train_inner | epoch 001:    500 / 1230 loss=9.681, ppl=820.92, wps=83038.2, ups=1, wpb=83446.3, bsz=20.7, num_updates=500, lr=0.000104265, gnorm=0.568, clip=100, train_wall=100, wall=507
 0: 2022-11-03 06:23:28 | INFO | train_inner | epoch 001:    600 / 1230 loss=9.304, ppl=632.01, wps=83262.8, ups=0.99, wpb=83961, bsz=21.2, num_updates=600, lr=0.000125097, gnorm=0.55, clip=100, train_wall=101, wall=608
 0: 2022-11-03 06:25:08 | INFO | train_inner | epoch 001:    700 / 1230 loss=9.023, ppl=520.4, wps=83750, ups=1, wpb=84005.2, bsz=20.7, num_updates=700, lr=0.00014593, gnorm=0.578, clip=100, train_wall=100, wall=708
