 2: 2022-11-03 07:37:13 | INFO | fairseq.distributed_utils | distributed init (rank 2): env://
 5: 2022-11-03 07:37:13 | INFO | fairseq.distributed_utils | distributed init (rank 5): env://
 0: 2022-11-03 07:37:13 | INFO | fairseq.distributed_utils | distributed init (rank 0): env://
 4: 2022-11-03 07:37:13 | INFO | fairseq.distributed_utils | distributed init (rank 4): env://
 4: 2022-11-03 07:37:13 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
 1: 2022-11-03 07:37:13 | INFO | fairseq.distributed_utils | distributed init (rank 1): env://
 1: 2022-11-03 07:37:13 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
 6: 2022-11-03 07:37:13 | INFO | fairseq.distributed_utils | distributed init (rank 6): env://
 3: 2022-11-03 07:37:13 | INFO | fairseq.distributed_utils | distributed init (rank 3): env://
 6: 2022-11-03 07:37:13 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
 7: 2022-11-03 07:37:13 | INFO | fairseq.distributed_utils | distributed init (rank 7): env://
 3: 2022-11-03 07:37:13 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
 7: 2022-11-03 07:37:13 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
 2: 2022-11-03 07:37:14 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
 5: 2022-11-03 07:37:14 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
14: 2022-11-03 07:37:18 | INFO | fairseq.distributed_utils | distributed init (rank 14): env://
10: 2022-11-03 07:37:18 | INFO | fairseq.distributed_utils | distributed init (rank 10): env://
15: 2022-11-03 07:37:18 | INFO | fairseq.distributed_utils | distributed init (rank 15): env://
 9: 2022-11-03 07:37:18 | INFO | fairseq.distributed_utils | distributed init (rank 9): env://
10: 2022-11-03 07:37:18 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 10
14: 2022-11-03 07:37:18 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 14
15: 2022-11-03 07:37:18 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 15
 9: 2022-11-03 07:37:18 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 9
11: 2022-11-03 07:37:18 | INFO | fairseq.distributed_utils | distributed init (rank 11): env://
13: 2022-11-03 07:37:18 | INFO | fairseq.distributed_utils | distributed init (rank 13): env://
 8: 2022-11-03 07:37:18 | INFO | fairseq.distributed_utils | distributed init (rank 8): env://
12: 2022-11-03 07:37:18 | INFO | fairseq.distributed_utils | distributed init (rank 12): env://
11: 2022-11-03 07:37:18 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 11
13: 2022-11-03 07:37:18 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 13
 8: 2022-11-03 07:37:18 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 8
12: 2022-11-03 07:37:18 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 12
 0: 2022-11-03 07:37:18 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
 0: 2022-11-03 07:37:18 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
 0: 2022-11-03 07:37:18 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-5 as rank 0
 1: 2022-11-03 07:37:18 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
 4: 2022-11-03 07:37:18 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
 3: 2022-11-03 07:37:18 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
 2: 2022-11-03 07:37:18 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
 6: 2022-11-03 07:37:18 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
 7: 2022-11-03 07:37:18 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
 1: 2022-11-03 07:37:18 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-5 as rank 1
10: 2022-11-03 07:37:18 | INFO | torch.distributed.distributed_c10d | Rank 10: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
 5: 2022-11-03 07:37:18 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
 3: 2022-11-03 07:37:18 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-5 as rank 3
 4: 2022-11-03 07:37:18 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-5 as rank 4
 2: 2022-11-03 07:37:18 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-5 as rank 2
14: 2022-11-03 07:37:18 | INFO | torch.distributed.distributed_c10d | Rank 14: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
15: 2022-11-03 07:37:18 | INFO | torch.distributed.distributed_c10d | Rank 15: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
 9: 2022-11-03 07:37:18 | INFO | torch.distributed.distributed_c10d | Rank 9: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
11: 2022-11-03 07:37:18 | INFO | torch.distributed.distributed_c10d | Rank 11: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
13: 2022-11-03 07:37:18 | INFO | torch.distributed.distributed_c10d | Rank 13: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
10: 2022-11-03 07:37:18 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-6 as rank 10
15: 2022-11-03 07:37:18 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-6 as rank 15
 6: 2022-11-03 07:37:18 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-5 as rank 6
 9: 2022-11-03 07:37:18 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-6 as rank 9
11: 2022-11-03 07:37:18 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-6 as rank 11
14: 2022-11-03 07:37:18 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-6 as rank 14
13: 2022-11-03 07:37:18 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-6 as rank 13
 7: 2022-11-03 07:37:18 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-5 as rank 7
 5: 2022-11-03 07:37:18 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-5 as rank 5
 8: 2022-11-03 07:37:18 | INFO | torch.distributed.distributed_c10d | Rank 8: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
12: 2022-11-03 07:37:18 | INFO | torch.distributed.distributed_c10d | Rank 12: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
 8: 2022-11-03 07:37:18 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-6 as rank 8
12: 2022-11-03 07:37:18 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-6 as rank 12
 0: queue1-dy-p4d24xlarge-5:10957:10957 [0] NCCL INFO Bootstrap : Using ens32:10.0.27.216<0>
 0: queue1-dy-p4d24xlarge-5:10957:10957 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v5 symbol.
 0: queue1-dy-p4d24xlarge-5:10957:10957 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.
 0: queue1-dy-p4d24xlarge-5:10957:10957 [0] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws
 0: queue1-dy-p4d24xlarge-5:10957:10957 [0] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /fsx/conda/envs/fairseq_aws_1110/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml
 0: queue1-dy-p4d24xlarge-5:10957:10957 [0] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1
 0: queue1-dy-p4d24xlarge-5:10957:10957 [0] NCCL INFO NET/OFI Selected Provider is efa
 0: queue1-dy-p4d24xlarge-5:10957:10957 [0] NCCL INFO Using network AWS Libfabric
 0: NCCL version 2.12.12+cuda11.5
 1: queue1-dy-p4d24xlarge-5:10958:10958 [1] NCCL INFO Bootstrap : Using ens32:10.0.27.216<0>
 2: queue1-dy-p4d24xlarge-5:10959:10959 [2] NCCL INFO Bootstrap : Using ens32:10.0.27.216<0>
 3: queue1-dy-p4d24xlarge-5:10960:10960 [3] NCCL INFO Bootstrap : Using ens32:10.0.27.216<0>
 4: queue1-dy-p4d24xlarge-5:10961:10961 [4] NCCL INFO Bootstrap : Using ens32:10.0.27.216<0>
 5: queue1-dy-p4d24xlarge-5:10962:10962 [5] NCCL INFO Bootstrap : Using ens32:10.0.27.216<0>
 6: queue1-dy-p4d24xlarge-5:10963:10963 [6] NCCL INFO Bootstrap : Using ens32:10.0.27.216<0>
 7: queue1-dy-p4d24xlarge-5:10964:10964 [7] NCCL INFO Bootstrap : Using ens32:10.0.27.216<0>
 1: queue1-dy-p4d24xlarge-5:10958:10958 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v5 symbol.
 1: queue1-dy-p4d24xlarge-5:10958:10958 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.
 1: queue1-dy-p4d24xlarge-5:10958:10958 [1] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws
 3: queue1-dy-p4d24xlarge-5:10960:10960 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v5 symbol.
 3: queue1-dy-p4d24xlarge-5:10960:10960 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.
 1: queue1-dy-p4d24xlarge-5:10958:10958 [1] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /fsx/conda/envs/fairseq_aws_1110/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml
 1: queue1-dy-p4d24xlarge-5:10958:10958 [1] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1
 3: queue1-dy-p4d24xlarge-5:10960:10960 [3] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws
 3: queue1-dy-p4d24xlarge-5:10960:10960 [3] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /fsx/conda/envs/fairseq_aws_1110/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml
 3: queue1-dy-p4d24xlarge-5:10960:10960 [3] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1
 2: queue1-dy-p4d24xlarge-5:10959:10959 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v5 symbol.
 2: queue1-dy-p4d24xlarge-5:10959:10959 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.
 2: queue1-dy-p4d24xlarge-5:10959:10959 [2] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws
 2: queue1-dy-p4d24xlarge-5:10959:10959 [2] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /fsx/conda/envs/fairseq_aws_1110/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml
 2: queue1-dy-p4d24xlarge-5:10959:10959 [2] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1
 4: queue1-dy-p4d24xlarge-5:10961:10961 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v5 symbol.
 4: queue1-dy-p4d24xlarge-5:10961:10961 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.
 4: queue1-dy-p4d24xlarge-5:10961:10961 [4] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws
15: queue1-dy-p4d24xlarge-6:10022:10022 [7] NCCL INFO Bootstrap : Using ens32:10.0.23.67<0>
 5: queue1-dy-p4d24xlarge-5:10962:10962 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v5 symbol.
 5: queue1-dy-p4d24xlarge-5:10962:10962 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.
 5: queue1-dy-p4d24xlarge-5:10962:10962 [5] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws
 6: queue1-dy-p4d24xlarge-5:10963:10963 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v5 symbol.
 6: queue1-dy-p4d24xlarge-5:10963:10963 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.
 6: queue1-dy-p4d24xlarge-5:10963:10963 [6] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws
 6: queue1-dy-p4d24xlarge-5:10963:10963 [6] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /fsx/conda/envs/fairseq_aws_1110/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml
 7: queue1-dy-p4d24xlarge-5:10964:10964 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v5 symbol.
 7: queue1-dy-p4d24xlarge-5:10964:10964 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.
 7: queue1-dy-p4d24xlarge-5:10964:10964 [7] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws
 7: queue1-dy-p4d24xlarge-5:10964:10964 [7] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /fsx/conda/envs/fairseq_aws_1110/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml
 7: queue1-dy-p4d24xlarge-5:10964:10964 [7] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1
 8: queue1-dy-p4d24xlarge-6:10015:10015 [0] NCCL INFO Bootstrap : Using ens32:10.0.23.67<0>
 4: queue1-dy-p4d24xlarge-5:10961:10961 [4] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /fsx/conda/envs/fairseq_aws_1110/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml
 4: queue1-dy-p4d24xlarge-5:10961:10961 [4] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1
 9: queue1-dy-p4d24xlarge-6:10016:10016 [1] NCCL INFO Bootstrap : Using ens32:10.0.23.67<0>
 5: queue1-dy-p4d24xlarge-5:10962:10962 [5] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /fsx/conda/envs/fairseq_aws_1110/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml
 5: queue1-dy-p4d24xlarge-5:10962:10962 [5] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1
10: queue1-dy-p4d24xlarge-6:10017:10017 [2] NCCL INFO Bootstrap : Using ens32:10.0.23.67<0>
 6: queue1-dy-p4d24xlarge-5:10963:10963 [6] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1
11: queue1-dy-p4d24xlarge-6:10018:10018 [3] NCCL INFO Bootstrap : Using ens32:10.0.23.67<0>
12: queue1-dy-p4d24xlarge-6:10019:10019 [4] NCCL INFO Bootstrap : Using ens32:10.0.23.67<0>
13: queue1-dy-p4d24xlarge-6:10020:10020 [5] NCCL INFO Bootstrap : Using ens32:10.0.23.67<0>
14: queue1-dy-p4d24xlarge-6:10021:10021 [6] NCCL INFO Bootstrap : Using ens32:10.0.23.67<0>
 1: queue1-dy-p4d24xlarge-5:10958:10958 [1] NCCL INFO NET/OFI Selected Provider is efa
 1: queue1-dy-p4d24xlarge-5:10958:10958 [1] NCCL INFO Using network AWS Libfabric
 3: queue1-dy-p4d24xlarge-5:10960:10960 [3] NCCL INFO NET/OFI Selected Provider is efa
 3: queue1-dy-p4d24xlarge-5:10960:10960 [3] NCCL INFO Using network AWS Libfabric
 2: queue1-dy-p4d24xlarge-5:10959:10959 [2] NCCL INFO NET/OFI Selected Provider is efa
 2: queue1-dy-p4d24xlarge-5:10959:10959 [2] NCCL INFO Using network AWS Libfabric
 6: queue1-dy-p4d24xlarge-5:10963:10963 [6] NCCL INFO NET/OFI Selected Provider is efa
 6: queue1-dy-p4d24xlarge-5:10963:10963 [6] NCCL INFO Using network AWS Libfabric
 5: queue1-dy-p4d24xlarge-5:10962:10962 [5] NCCL INFO NET/OFI Selected Provider is efa
 5: queue1-dy-p4d24xlarge-5:10962:10962 [5] NCCL INFO Using network AWS Libfabric
 4: queue1-dy-p4d24xlarge-5:10961:10961 [4] NCCL INFO NET/OFI Selected Provider is efa
 4: queue1-dy-p4d24xlarge-5:10961:10961 [4] NCCL INFO Using network AWS Libfabric
 7: queue1-dy-p4d24xlarge-5:10964:10964 [7] NCCL INFO NET/OFI Selected Provider is efa
 7: queue1-dy-p4d24xlarge-5:10964:10964 [7] NCCL INFO Using network AWS Libfabric
 8: queue1-dy-p4d24xlarge-6:10015:10015 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v5 symbol.
 8: queue1-dy-p4d24xlarge-6:10015:10015 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.
 9: queue1-dy-p4d24xlarge-6:10016:10016 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v5 symbol.
 9: queue1-dy-p4d24xlarge-6:10016:10016 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.
11: queue1-dy-p4d24xlarge-6:10018:10018 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v5 symbol.
11: queue1-dy-p4d24xlarge-6:10018:10018 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.
10: queue1-dy-p4d24xlarge-6:10017:10017 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v5 symbol.
10: queue1-dy-p4d24xlarge-6:10017:10017 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.
12: queue1-dy-p4d24xlarge-6:10019:10019 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v5 symbol.
12: queue1-dy-p4d24xlarge-6:10019:10019 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.
13: queue1-dy-p4d24xlarge-6:10020:10020 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v5 symbol.
13: queue1-dy-p4d24xlarge-6:10020:10020 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.
14: queue1-dy-p4d24xlarge-6:10021:10021 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v5 symbol.
14: queue1-dy-p4d24xlarge-6:10021:10021 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.
15: queue1-dy-p4d24xlarge-6:10022:10022 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v5 symbol.
15: queue1-dy-p4d24xlarge-6:10022:10022 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.
11: queue1-dy-p4d24xlarge-6:10018:10018 [3] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws
13: queue1-dy-p4d24xlarge-6:10020:10020 [5] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws
 8: queue1-dy-p4d24xlarge-6:10015:10015 [0] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws
10: queue1-dy-p4d24xlarge-6:10017:10017 [2] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws
12: queue1-dy-p4d24xlarge-6:10019:10019 [4] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws
14: queue1-dy-p4d24xlarge-6:10021:10021 [6] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws
15: queue1-dy-p4d24xlarge-6:10022:10022 [7] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws
 9: queue1-dy-p4d24xlarge-6:10016:10016 [1] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws
13: queue1-dy-p4d24xlarge-6:10020:10020 [5] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /fsx/conda/envs/fairseq_aws_1110/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml
13: queue1-dy-p4d24xlarge-6:10020:10020 [5] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1
10: queue1-dy-p4d24xlarge-6:10017:10017 [2] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /fsx/conda/envs/fairseq_aws_1110/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml
10: queue1-dy-p4d24xlarge-6:10017:10017 [2] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1
11: queue1-dy-p4d24xlarge-6:10018:10018 [3] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /fsx/conda/envs/fairseq_aws_1110/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml
11: queue1-dy-p4d24xlarge-6:10018:10018 [3] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1
12: queue1-dy-p4d24xlarge-6:10019:10019 [4] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /fsx/conda/envs/fairseq_aws_1110/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml
12: queue1-dy-p4d24xlarge-6:10019:10019 [4] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1
14: queue1-dy-p4d24xlarge-6:10021:10021 [6] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /fsx/conda/envs/fairseq_aws_1110/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml
14: queue1-dy-p4d24xlarge-6:10021:10021 [6] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1
15: queue1-dy-p4d24xlarge-6:10022:10022 [7] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /fsx/conda/envs/fairseq_aws_1110/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml
15: queue1-dy-p4d24xlarge-6:10022:10022 [7] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1
 8: queue1-dy-p4d24xlarge-6:10015:10015 [0] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /fsx/conda/envs/fairseq_aws_1110/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml
 8: queue1-dy-p4d24xlarge-6:10015:10015 [0] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1
 9: queue1-dy-p4d24xlarge-6:10016:10016 [1] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /fsx/conda/envs/fairseq_aws_1110/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml
 9: queue1-dy-p4d24xlarge-6:10016:10016 [1] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1
 8: queue1-dy-p4d24xlarge-6:10015:10015 [0] NCCL INFO NET/OFI Selected Provider is efa
 8: queue1-dy-p4d24xlarge-6:10015:10015 [0] NCCL INFO Using network AWS Libfabric
 9: queue1-dy-p4d24xlarge-6:10016:10016 [1] NCCL INFO NET/OFI Selected Provider is efa
 9: queue1-dy-p4d24xlarge-6:10016:10016 [1] NCCL INFO Using network AWS Libfabric
10: queue1-dy-p4d24xlarge-6:10017:10017 [2] NCCL INFO NET/OFI Selected Provider is efa
10: queue1-dy-p4d24xlarge-6:10017:10017 [2] NCCL INFO Using network AWS Libfabric
11: queue1-dy-p4d24xlarge-6:10018:10018 [3] NCCL INFO NET/OFI Selected Provider is efa
11: queue1-dy-p4d24xlarge-6:10018:10018 [3] NCCL INFO Using network AWS Libfabric
12: queue1-dy-p4d24xlarge-6:10019:10019 [4] NCCL INFO NET/OFI Selected Provider is efa
12: queue1-dy-p4d24xlarge-6:10019:10019 [4] NCCL INFO Using network AWS Libfabric
13: queue1-dy-p4d24xlarge-6:10020:10020 [5] NCCL INFO NET/OFI Selected Provider is efa
13: queue1-dy-p4d24xlarge-6:10020:10020 [5] NCCL INFO Using network AWS Libfabric
14: queue1-dy-p4d24xlarge-6:10021:10021 [6] NCCL INFO NET/OFI Selected Provider is efa
14: queue1-dy-p4d24xlarge-6:10021:10021 [6] NCCL INFO Using network AWS Libfabric
15: queue1-dy-p4d24xlarge-6:10022:10022 [7] NCCL INFO NET/OFI Selected Provider is efa
15: queue1-dy-p4d24xlarge-6:10022:10022 [7] NCCL INFO Using network AWS Libfabric
15: queue1-dy-p4d24xlarge-6:10022:10295 [7] NCCL INFO Setting affinity for GPU 7 to fc000000,0000fc00,00000000
13: queue1-dy-p4d24xlarge-6:10020:10292 [5] NCCL INFO Setting affinity for GPU 5 to 0fc000,0000000f,c0000000
 6: queue1-dy-p4d24xlarge-5:10963:11230 [6] NCCL INFO Setting affinity for GPU 6 to 03f00000,000003f0,00000000
 7: queue1-dy-p4d24xlarge-5:10964:11233 [7] NCCL INFO Setting affinity for GPU 7 to fc000000,0000fc00,00000000
 5: queue1-dy-p4d24xlarge-5:10962:11231 [5] NCCL INFO Setting affinity for GPU 5 to 0fc000,0000000f,c0000000
 0: queue1-dy-p4d24xlarge-5:10957:11225 [0] NCCL INFO Setting affinity for GPU 0 to 3f0000,0000003f
14: queue1-dy-p4d24xlarge-6:10021:10299 [6] NCCL INFO Setting affinity for GPU 6 to 03f00000,000003f0,00000000
 4: queue1-dy-p4d24xlarge-5:10961:11232 [4] NCCL INFO Setting affinity for GPU 4 to 3f00,00000000,3f000000
12: queue1-dy-p4d24xlarge-6:10019:10293 [4] NCCL INFO Setting affinity for GPU 4 to 3f00,00000000,3f000000
 9: queue1-dy-p4d24xlarge-6:10016:10296 [1] NCCL INFO Setting affinity for GPU 1 to 0fc00000,00000fc0
 2: queue1-dy-p4d24xlarge-5:10959:11229 [2] NCCL INFO Setting affinity for GPU 2 to 03,f0000000,0003f000
 1: queue1-dy-p4d24xlarge-5:10958:11227 [1] NCCL INFO Setting affinity for GPU 1 to 0fc00000,00000fc0
 3: queue1-dy-p4d24xlarge-5:10960:11228 [3] NCCL INFO Setting affinity for GPU 3 to fc,00000000,00fc0000
10: queue1-dy-p4d24xlarge-6:10017:10298 [2] NCCL INFO Setting affinity for GPU 2 to 03,f0000000,0003f000
 8: queue1-dy-p4d24xlarge-6:10015:10297 [0] NCCL INFO Setting affinity for GPU 0 to 3f0000,0000003f
11: queue1-dy-p4d24xlarge-6:10018:10294 [3] NCCL INFO Setting affinity for GPU 3 to fc,00000000,00fc0000
11: queue1-dy-p4d24xlarge-6:10018:10294 [3] NCCL INFO Trees [0] 12/-1/-1->11->10 [1] 12/-1/-1->11->10 [2] -1/-1/-1->11->10 [3] 12/-1/-1->11->10 [4] 12/-1/-1->11->10 [5] 12/-1/-1->11->10 [6] -1/-1/-1->11->10 [7] 12/-1/-1->11->10
15: queue1-dy-p4d24xlarge-6:10022:10295 [7] NCCL INFO Trees [0] -1/-1/-1->15->14 [1] 8/-1/-1->15->14 [2] 8/-1/-1->15->14 [3] 8/-1/-1->15->14 [4] -1/-1/-1->15->14 [5] 8/-1/-1->15->14 [6] 8/-1/-1->15->14 [7] 8/-1/-1->15->14
12: queue1-dy-p4d24xlarge-6:10019:10293 [4] NCCL INFO Trees [0] 13/-1/-1->12->11 [1] 13/-1/-1->12->11 [2] 13/-1/-1->12->4 [3] 13/-1/-1->12->11 [4] 13/-1/-1->12->11 [5] 13/-1/-1->12->11 [6] 13/4/-1->12->-1 [7] 13/-1/-1->12->11
13: queue1-dy-p4d24xlarge-6:10020:10292 [5] NCCL INFO Trees [0] 14/-1/-1->13->12 [1] 14/-1/-1->13->12 [2] 14/-1/-1->13->12 [3] -1/-1/-1->13->12 [4] 14/-1/-1->13->12 [5] 14/-1/-1->13->12 [6] 14/-1/-1->13->12 [7] -1/-1/-1->13->12
10: queue1-dy-p4d24xlarge-6:10017:10298 [2] NCCL INFO Trees [0] 11/-1/-1->10->9 [1] 11/-1/-1->10->2 [2] 11/-1/-1->10->9 [3] 11/-1/-1->10->9 [4] 11/-1/-1->10->9 [5] 11/2/-1->10->-1 [6] 11/-1/-1->10->9 [7] 11/-1/-1->10->9
14: queue1-dy-p4d24xlarge-6:10021:10299 [6] NCCL INFO Trees [0] 15/-1/-1->14->13 [1] 15/-1/-1->14->13 [2] 15/-1/-1->14->13 [3] 15/-1/-1->14->6 [4] 15/-1/-1->14->13 [5] 15/-1/-1->14->13 [6] 15/-1/-1->14->13 [7] 15/6/-1->14->-1
 9: queue1-dy-p4d24xlarge-6:10016:10296 [1] NCCL INFO Trees [0] 10/-1/-1->9->8 [1] -1/-1/-1->9->8 [2] 10/-1/-1->9->8 [3] 10/-1/-1->9->8 [4] 10/-1/-1->9->8 [5] -1/-1/-1->9->8 [6] 10/-1/-1->9->8 [7] 10/-1/-1->9->8
 6: queue1-dy-p4d24xlarge-5:10963:11230 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 7/-1/-1->6->5 [3] 7/14/-1->6->-1 [4] 7/-1/-1->6->5 [5] 7/-1/-1->6->5 [6] 7/-1/-1->6->5 [7] 7/-1/-1->6->14
 8: queue1-dy-p4d24xlarge-6:10015:10297 [0] NCCL INFO Trees [0] 9/-1/-1->8->0 [1] 9/-1/-1->8->15 [2] 9/-1/-1->8->15 [3] 9/-1/-1->8->15 [4] 9/0/-1->8->-1 [5] 9/-1/-1->8->15 [6] 9/-1/-1->8->15 [7] 9/-1/-1->8->15
 7: queue1-dy-p4d24xlarge-5:10964:11233 [7] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] 0/-1/-1->7->6 [2] 0/-1/-1->7->6 [3] 0/-1/-1->7->6 [4] -1/-1/-1->7->6 [5] 0/-1/-1->7->6 [6] 0/-1/-1->7->6 [7] 0/-1/-1->7->6
 0: queue1-dy-p4d24xlarge-5:10957:11225 [0] NCCL INFO Channel 00/08 :    0   7   6   5   4   3   2   1   8  15  14  13  12  11  10   9
 0: queue1-dy-p4d24xlarge-5:10957:11225 [0] NCCL INFO Channel 01/08 :    0   3  10  15  14  13  12   9   8  11   2   7   6   5   4   1
 0: queue1-dy-p4d24xlarge-5:10957:11225 [0] NCCL INFO Channel 02/08 :    0   7   6   5  12  11  10   9   8  15  14  13   4   3   2   1
 0: queue1-dy-p4d24xlarge-5:10957:11225 [0] NCCL INFO Channel 03/08 :    0   5   4   7  14  11  10   9   8  13  12  15   6   3   2   1
 0: queue1-dy-p4d24xlarge-5:10957:11225 [0] NCCL INFO Channel 04/08 :    0   7   6   5   4   3   2   1   8  15  14  13  12  11  10   9
 1: queue1-dy-p4d24xlarge-5:10958:11227 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] -1/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] -1/-1/-1->1->0 [6] 2/-1/-1->1->0 [7] 2/-1/-1->1->0
 2: queue1-dy-p4d24xlarge-5:10959:11229 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/10/-1->2->-1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->10 [6] 3/-1/-1->2->1 [7] 3/-1/-1->2->1
 3: queue1-dy-p4d24xlarge-5:10960:11228 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2 [2] -1/-1/-1->3->2 [3] 4/-1/-1->3->2 [4] 4/-1/-1->3->2 [5] 4/-1/-1->3->2 [6] -1/-1/-1->3->2 [7] 4/-1/-1->3->2
 4: queue1-dy-p4d24xlarge-5:10961:11232 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3 [2] 5/12/-1->4->-1 [3] 5/-1/-1->4->3 [4] 5/-1/-1->4->3 [5] 5/-1/-1->4->3 [6] 5/-1/-1->4->12 [7] 5/-1/-1->4->3
 5: queue1-dy-p4d24xlarge-5:10962:11231 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4 [2] 6/-1/-1->5->4 [3] -1/-1/-1->5->4 [4] 6/-1/-1->5->4 [5] 6/-1/-1->5->4 [6] 6/-1/-1->5->4 [7] -1/-1/-1->5->4
 0: queue1-dy-p4d24xlarge-5:10957:11225 [0] NCCL INFO Channel 05/08 :    0   3  10  15  14  13  12   9   8  11   2   7   6   5   4   1
 0: queue1-dy-p4d24xlarge-5:10957:11225 [0] NCCL INFO Channel 06/08 :    0   7   6   5  12  11  10   9   8  15  14  13   4   3   2   1
 0: queue1-dy-p4d24xlarge-5:10957:11225 [0] NCCL INFO Channel 07/08 :    0   5   4   7  14  11  10   9   8  13  12  15   6   3   2   1
 0: queue1-dy-p4d24xlarge-5:10957:11225 [0] NCCL INFO Trees [0] 1/8/-1->0->-1 [1] 1/-1/-1->0->7 [2] 1/-1/-1->0->7 [3] 1/-1/-1->0->7 [4] 1/-1/-1->0->8 [5] 1/-1/-1->0->7 [6] 1/-1/-1->0->7 [7] 1/-1/-1->0->7
12: queue1-dy-p4d24xlarge-6:10019:10293 [4] NCCL INFO Channel 03 : 12[901c0] -> 15[a01d0] via P2P/IPC/read
 2: queue1-dy-p4d24xlarge-5:10959:11229 [2] NCCL INFO Channel 01 : 2[201c0] -> 7[a01d0] via P2P/IPC/read
10: queue1-dy-p4d24xlarge-6:10017:10298 [2] NCCL INFO Channel 01 : 10[201c0] -> 15[a01d0] via P2P/IPC/read
 8: queue1-dy-p4d24xlarge-6:10015:10297 [0] NCCL INFO Channel 01 : 8[101c0] -> 11[201d0] via P2P/IPC/read
 2: queue1-dy-p4d24xlarge-5:10959:11229 [2] NCCL INFO Channel 05 : 2[201c0] -> 7[a01d0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-6:10019:10293 [4] NCCL INFO Channel 07 : 12[901c0] -> 15[a01d0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-5:10961:11232 [4] NCCL INFO Channel 03 : 4[901c0] -> 7[a01d0] via P2P/IPC/read
 0: queue1-dy-p4d24xlarge-5:10957:11225 [0] NCCL INFO Channel 01 : 0[101c0] -> 3[201d0] via P2P/IPC/read
10: queue1-dy-p4d24xlarge-6:10017:10298 [2] NCCL INFO Channel 05 : 10[201c0] -> 15[a01d0] via P2P/IPC/read
 8: queue1-dy-p4d24xlarge-6:10015:10297 [0] NCCL INFO Channel 05 : 8[101c0] -> 11[201d0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-5:10961:11232 [4] NCCL INFO Channel 07 : 4[901c0] -> 7[a01d0] via P2P/IPC/read
 0: queue1-dy-p4d24xlarge-5:10957:11225 [0] NCCL INFO Channel 05 : 0[101c0] -> 3[201d0] via P2P/IPC/read
 1: queue1-dy-p4d24xlarge-5:10958:11227 [1] NCCL INFO Channel 00/0 : 1[101d0] -> 8[101c0] [send] via NET/AWS Libfabric/0/GDRDMA
 1: queue1-dy-p4d24xlarge-5:10958:11227 [1] NCCL INFO Channel 04/0 : 1[101d0] -> 8[101c0] [send] via NET/AWS Libfabric/0/GDRDMA
 9: queue1-dy-p4d24xlarge-6:10016:10296 [1] NCCL INFO Channel 00/0 : 9[101d0] -> 0[101c0] [send] via NET/AWS Libfabric/0/GDRDMA
 9: queue1-dy-p4d24xlarge-6:10016:10296 [1] NCCL INFO Channel 04/0 : 9[101d0] -> 0[101c0] [send] via NET/AWS Libfabric/0/GDRDMA
 8: queue1-dy-p4d24xlarge-6:10015:10297 [0] NCCL INFO Channel 03 : 8[101c0] -> 13[901d0] via P2P/IPC/read
 8: queue1-dy-p4d24xlarge-6:10015:10297 [0] NCCL INFO Channel 07 : 8[101c0] -> 13[901d0] via P2P/IPC/read
 0: queue1-dy-p4d24xlarge-5:10957:11225 [0] NCCL INFO Channel 03 : 0[101c0] -> 5[901d0] via P2P/IPC/read
 0: queue1-dy-p4d24xlarge-5:10957:11225 [0] NCCL INFO Channel 07 : 0[101c0] -> 5[901d0] via P2P/IPC/read
11: queue1-dy-p4d24xlarge-6:10018:10294 [3] NCCL INFO Channel 01/0 : 11[201d0] -> 2[201c0] [send] via NET/AWS Libfabric/1/GDRDMA
11: queue1-dy-p4d24xlarge-6:10018:10294 [3] NCCL INFO Channel 05/0 : 11[201d0] -> 2[201c0] [send] via NET/AWS Libfabric/1/GDRDMA
 3: queue1-dy-p4d24xlarge-5:10960:11228 [3] NCCL INFO Channel 01/0 : 3[201d0] -> 10[201c0] [send] via NET/AWS Libfabric/1/GDRDMA
 3: queue1-dy-p4d24xlarge-5:10960:11228 [3] NCCL INFO Channel 05/0 : 3[201d0] -> 10[201c0] [send] via NET/AWS Libfabric/1/GDRDMA
13: queue1-dy-p4d24xlarge-6:10020:10292 [5] NCCL INFO Channel 02/0 : 13[901d0] -> 4[901c0] [send] via NET/AWS Libfabric/2/GDRDMA
13: queue1-dy-p4d24xlarge-6:10020:10292 [5] NCCL INFO Channel 06/0 : 13[901d0] -> 4[901c0] [send] via NET/AWS Libfabric/2/GDRDMA
 5: queue1-dy-p4d24xlarge-5:10962:11231 [5] NCCL INFO Channel 02/0 : 5[901d0] -> 12[901c0] [send] via NET/AWS Libfabric/2/GDRDMA
 5: queue1-dy-p4d24xlarge-5:10962:11231 [5] NCCL INFO Channel 06/0 : 5[901d0] -> 12[901c0] [send] via NET/AWS Libfabric/2/GDRDMA
15: queue1-dy-p4d24xlarge-6:10022:10295 [7] NCCL INFO Channel 03/0 : 15[a01d0] -> 6[a01c0] [send] via NET/AWS Libfabric/3/GDRDMA
 7: queue1-dy-p4d24xlarge-5:10964:11233 [7] NCCL INFO Channel 03/0 : 7[a01d0] -> 14[a01c0] [send] via NET/AWS Libfabric/3/GDRDMA
 7: queue1-dy-p4d24xlarge-5:10964:11233 [7] NCCL INFO Channel 07/0 : 7[a01d0] -> 14[a01c0] [send] via NET/AWS Libfabric/3/GDRDMA
 6: queue1-dy-p4d24xlarge-5:10963:11230 [6] NCCL INFO Channel 03/0 : 15[a01d0] -> 6[a01c0] [receive] via NET/AWS Libfabric/3/GDRDMA
 6: queue1-dy-p4d24xlarge-5:10963:11230 [6] NCCL INFO Channel 07/0 : 15[a01d0] -> 6[a01c0] [receive] via NET/AWS Libfabric/3/GDRDMA
14: queue1-dy-p4d24xlarge-6:10021:10299 [6] NCCL INFO Channel 03/0 : 7[a01d0] -> 14[a01c0] [receive] via NET/AWS Libfabric/3/GDRDMA
14: queue1-dy-p4d24xlarge-6:10021:10299 [6] NCCL INFO Channel 07/0 : 7[a01d0] -> 14[a01c0] [receive] via NET/AWS Libfabric/3/GDRDMA
12: queue1-dy-p4d24xlarge-6:10019:10293 [4] NCCL INFO Channel 02/0 : 5[901d0] -> 12[901c0] [receive] via NET/AWS Libfabric/2/GDRDMA
12: queue1-dy-p4d24xlarge-6:10019:10293 [4] NCCL INFO Channel 06/0 : 5[901d0] -> 12[901c0] [receive] via NET/AWS Libfabric/2/GDRDMA
 4: queue1-dy-p4d24xlarge-5:10961:11232 [4] NCCL INFO Channel 02/0 : 13[901d0] -> 4[901c0] [receive] via NET/AWS Libfabric/2/GDRDMA
 4: queue1-dy-p4d24xlarge-5:10961:11232 [4] NCCL INFO Channel 06/0 : 13[901d0] -> 4[901c0] [receive] via NET/AWS Libfabric/2/GDRDMA
10: queue1-dy-p4d24xlarge-6:10017:10298 [2] NCCL INFO Channel 01/0 : 3[201d0] -> 10[201c0] [receive] via NET/AWS Libfabric/1/GDRDMA
 2: queue1-dy-p4d24xlarge-5:10959:11229 [2] NCCL INFO Channel 01/0 : 11[201d0] -> 2[201c0] [receive] via NET/AWS Libfabric/1/GDRDMA
 8: queue1-dy-p4d24xlarge-6:10015:10297 [0] NCCL INFO Channel 00/0 : 1[101d0] -> 8[101c0] [receive] via NET/AWS Libfabric/0/GDRDMA
 2: queue1-dy-p4d24xlarge-5:10959:11229 [2] NCCL INFO Channel 05/0 : 11[201d0] -> 2[201c0] [receive] via NET/AWS Libfabric/1/GDRDMA
 0: queue1-dy-p4d24xlarge-5:10957:11225 [0] NCCL INFO Channel 00/0 : 9[101d0] -> 0[101c0] [receive] via NET/AWS Libfabric/0/GDRDMA
15: queue1-dy-p4d24xlarge-6:10022:10295 [7] NCCL INFO Channel 07/0 : 15[a01d0] -> 6[a01c0] [send] via NET/AWS Libfabric/3/GDRDMA
10: queue1-dy-p4d24xlarge-6:10017:10298 [2] NCCL INFO Channel 05/0 : 3[201d0] -> 10[201c0] [receive] via NET/AWS Libfabric/1/GDRDMA
 8: queue1-dy-p4d24xlarge-6:10015:10297 [0] NCCL INFO Channel 04/0 : 1[101d0] -> 8[101c0] [receive] via NET/AWS Libfabric/0/GDRDMA
 8: queue1-dy-p4d24xlarge-6:10015:10297 [0] NCCL INFO Channel 00 : 8[101c0] -> 15[a01d0] via P2P/IPC/read
 8: queue1-dy-p4d24xlarge-6:10015:10297 [0] NCCL INFO Channel 02 : 8[101c0] -> 15[a01d0] via P2P/IPC/read
 8: queue1-dy-p4d24xlarge-6:10015:10297 [0] NCCL INFO Channel 04 : 8[101c0] -> 15[a01d0] via P2P/IPC/read
 8: queue1-dy-p4d24xlarge-6:10015:10297 [0] NCCL INFO Channel 06 : 8[101c0] -> 15[a01d0] via P2P/IPC/read
 0: queue1-dy-p4d24xlarge-5:10957:11225 [0] NCCL INFO Channel 04/0 : 9[101d0] -> 0[101c0] [receive] via NET/AWS Libfabric/0/GDRDMA
 0: queue1-dy-p4d24xlarge-5:10957:11225 [0] NCCL INFO Channel 00 : 0[101c0] -> 7[a01d0] via P2P/IPC/read
 0: queue1-dy-p4d24xlarge-5:10957:11225 [0] NCCL INFO Channel 02 : 0[101c0] -> 7[a01d0] via P2P/IPC/read
 0: queue1-dy-p4d24xlarge-5:10957:11225 [0] NCCL INFO Channel 04 : 0[101c0] -> 7[a01d0] via P2P/IPC/read
 0: queue1-dy-p4d24xlarge-5:10957:11225 [0] NCCL INFO Channel 06 : 0[101c0] -> 7[a01d0] via P2P/IPC/read
 5: queue1-dy-p4d24xlarge-5:10962:11231 [5] NCCL INFO Channel 00 : 5[901d0] -> 4[901c0] via P2P/IPC/read
 5: queue1-dy-p4d24xlarge-5:10962:11231 [5] NCCL INFO Channel 01 : 5[901d0] -> 4[901c0] via P2P/IPC/read
 5: queue1-dy-p4d24xlarge-5:10962:11231 [5] NCCL INFO Channel 03 : 5[901d0] -> 4[901c0] via P2P/IPC/read
 5: queue1-dy-p4d24xlarge-5:10962:11231 [5] NCCL INFO Channel 04 : 5[901d0] -> 4[901c0] via P2P/IPC/read
 5: queue1-dy-p4d24xlarge-5:10962:11231 [5] NCCL INFO Channel 05 : 5[901d0] -> 4[901c0] via P2P/IPC/read
 5: queue1-dy-p4d24xlarge-5:10962:11231 [5] NCCL INFO Channel 07 : 5[901d0] -> 4[901c0] via P2P/IPC/read
13: queue1-dy-p4d24xlarge-6:10020:10292 [5] NCCL INFO Channel 00 : 13[901d0] -> 12[901c0] via P2P/IPC/read
13: queue1-dy-p4d24xlarge-6:10020:10292 [5] NCCL INFO Channel 01 : 13[901d0] -> 12[901c0] via P2P/IPC/read
13: queue1-dy-p4d24xlarge-6:10020:10292 [5] NCCL INFO Channel 03 : 13[901d0] -> 12[901c0] via P2P/IPC/read
13: queue1-dy-p4d24xlarge-6:10020:10292 [5] NCCL INFO Channel 04 : 13[901d0] -> 12[901c0] via P2P/IPC/read
13: queue1-dy-p4d24xlarge-6:10020:10292 [5] NCCL INFO Channel 05 : 13[901d0] -> 12[901c0] via P2P/IPC/read
13: queue1-dy-p4d24xlarge-6:10020:10292 [5] NCCL INFO Channel 07 : 13[901d0] -> 12[901c0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-6:10019:10293 [4] NCCL INFO Channel 01 : 12[901c0] -> 9[101d0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-6:10019:10293 [4] NCCL INFO Channel 05 : 12[901c0] -> 9[101d0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-5:10961:11232 [4] NCCL INFO Channel 01 : 4[901c0] -> 1[101d0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-5:10961:11232 [4] NCCL INFO Channel 05 : 4[901c0] -> 1[101d0] via P2P/IPC/read
 2: queue1-dy-p4d24xlarge-5:10959:11229 [2] NCCL INFO Channel 00 : 2[201c0] -> 1[101d0] via P2P/IPC/read
 2: queue1-dy-p4d24xlarge-5:10959:11229 [2] NCCL INFO Channel 02 : 2[201c0] -> 1[101d0] via P2P/IPC/read
 2: queue1-dy-p4d24xlarge-5:10959:11229 [2] NCCL INFO Channel 03 : 2[201c0] -> 1[101d0] via P2P/IPC/read
 2: queue1-dy-p4d24xlarge-5:10959:11229 [2] NCCL INFO Channel 04 : 2[201c0] -> 1[101d0] via P2P/IPC/read
 2: queue1-dy-p4d24xlarge-5:10959:11229 [2] NCCL INFO Channel 06 : 2[201c0] -> 1[101d0] via P2P/IPC/read
 2: queue1-dy-p4d24xlarge-5:10959:11229 [2] NCCL INFO Channel 07 : 2[201c0] -> 1[101d0] via P2P/IPC/read
14: queue1-dy-p4d24xlarge-6:10021:10299 [6] NCCL INFO Channel 03 : 14[a01c0] -> 11[201d0] via P2P/IPC/read
14: queue1-dy-p4d24xlarge-6:10021:10299 [6] NCCL INFO Channel 07 : 14[a01c0] -> 11[201d0] via P2P/IPC/read
 7: queue1-dy-p4d24xlarge-5:10964:11233 [7] NCCL INFO Channel 00 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
 7: queue1-dy-p4d24xlarge-5:10964:11233 [7] NCCL INFO Channel 01 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
 7: queue1-dy-p4d24xlarge-5:10964:11233 [7] NCCL INFO Channel 02 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
 6: queue1-dy-p4d24xlarge-5:10963:11230 [6] NCCL INFO Channel 03 : 6[a01c0] -> 3[201d0] via P2P/IPC/read
 1: queue1-dy-p4d24xlarge-5:10958:11227 [1] NCCL INFO Channel 01 : 1[101d0] -> 0[101c0] via P2P/IPC/read
 7: queue1-dy-p4d24xlarge-5:10964:11233 [7] NCCL INFO Channel 04 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-5:10961:11232 [4] NCCL INFO Channel 00 : 4[901c0] -> 3[201d0] via P2P/IPC/read
 6: queue1-dy-p4d24xlarge-5:10963:11230 [6] NCCL INFO Channel 07 : 6[a01c0] -> 3[201d0] via P2P/IPC/read
 1: queue1-dy-p4d24xlarge-5:10958:11227 [1] NCCL INFO Channel 02 : 1[101d0] -> 0[101c0] via P2P/IPC/read
 7: queue1-dy-p4d24xlarge-5:10964:11233 [7] NCCL INFO Channel 05 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-5:10961:11232 [4] NCCL INFO Channel 02 : 4[901c0] -> 3[201d0] via P2P/IPC/read
 1: queue1-dy-p4d24xlarge-5:10958:11227 [1] NCCL INFO Channel 03 : 1[101d0] -> 0[101c0] via P2P/IPC/read
 7: queue1-dy-p4d24xlarge-5:10964:11233 [7] NCCL INFO Channel 06 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-5:10961:11232 [4] NCCL INFO Channel 04 : 4[901c0] -> 3[201d0] via P2P/IPC/read
11: queue1-dy-p4d24xlarge-6:10018:10294 [3] NCCL INFO Channel 00 : 11[201d0] -> 10[201c0] via P2P/IPC/read
 1: queue1-dy-p4d24xlarge-5:10958:11227 [1] NCCL INFO Channel 05 : 1[101d0] -> 0[101c0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-5:10961:11232 [4] NCCL INFO Channel 06 : 4[901c0] -> 3[201d0] via P2P/IPC/read
 1: queue1-dy-p4d24xlarge-5:10958:11227 [1] NCCL INFO Channel 06 : 1[101d0] -> 0[101c0] via P2P/IPC/read
11: queue1-dy-p4d24xlarge-6:10018:10294 [3] NCCL INFO Channel 02 : 11[201d0] -> 10[201c0] via P2P/IPC/read
 1: queue1-dy-p4d24xlarge-5:10958:11227 [1] NCCL INFO Channel 07 : 1[101d0] -> 0[101c0] via P2P/IPC/read
15: queue1-dy-p4d24xlarge-6:10022:10295 [7] NCCL INFO Channel 00 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read
14: queue1-dy-p4d24xlarge-6:10021:10299 [6] NCCL INFO Channel 00 : 14[a01c0] -> 13[901d0] via P2P/IPC/read
 3: queue1-dy-p4d24xlarge-5:10960:11228 [3] NCCL INFO Channel 00 : 3[201d0] -> 2[201c0] via P2P/IPC/read
 3: queue1-dy-p4d24xlarge-5:10960:11228 [3] NCCL INFO Channel 02 : 3[201d0] -> 2[201c0] via P2P/IPC/read
11: queue1-dy-p4d24xlarge-6:10018:10294 [3] NCCL INFO Channel 03 : 11[201d0] -> 10[201c0] via P2P/IPC/read
 3: queue1-dy-p4d24xlarge-5:10960:11228 [3] NCCL INFO Channel 03 : 3[201d0] -> 2[201c0] via P2P/IPC/read
15: queue1-dy-p4d24xlarge-6:10022:10295 [7] NCCL INFO Channel 01 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read
 6: queue1-dy-p4d24xlarge-5:10963:11230 [6] NCCL INFO Channel 00 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
14: queue1-dy-p4d24xlarge-6:10021:10299 [6] NCCL INFO Channel 01 : 14[a01c0] -> 13[901d0] via P2P/IPC/read
 3: queue1-dy-p4d24xlarge-5:10960:11228 [3] NCCL INFO Channel 04 : 3[201d0] -> 2[201c0] via P2P/IPC/read
 6: queue1-dy-p4d24xlarge-5:10963:11230 [6] NCCL INFO Channel 01 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
 3: queue1-dy-p4d24xlarge-5:10960:11228 [3] NCCL INFO Channel 06 : 3[201d0] -> 2[201c0] via P2P/IPC/read
 6: queue1-dy-p4d24xlarge-5:10963:11230 [6] NCCL INFO Channel 02 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
11: queue1-dy-p4d24xlarge-6:10018:10294 [3] NCCL INFO Channel 04 : 11[201d0] -> 10[201c0] via P2P/IPC/read
 3: queue1-dy-p4d24xlarge-5:10960:11228 [3] NCCL INFO Channel 07 : 3[201d0] -> 2[201c0] via P2P/IPC/read
 6: queue1-dy-p4d24xlarge-5:10963:11230 [6] NCCL INFO Channel 04 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
15: queue1-dy-p4d24xlarge-6:10022:10295 [7] NCCL INFO Channel 02 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read
14: queue1-dy-p4d24xlarge-6:10021:10299 [6] NCCL INFO Channel 02 : 14[a01c0] -> 13[901d0] via P2P/IPC/read
 6: queue1-dy-p4d24xlarge-5:10963:11230 [6] NCCL INFO Channel 05 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
11: queue1-dy-p4d24xlarge-6:10018:10294 [3] NCCL INFO Channel 06 : 11[201d0] -> 10[201c0] via P2P/IPC/read
 9: queue1-dy-p4d24xlarge-6:10016:10296 [1] NCCL INFO Channel 01 : 9[101d0] -> 8[101c0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-6:10019:10293 [4] NCCL INFO Channel 00 : 12[901c0] -> 11[201d0] via P2P/IPC/read
15: queue1-dy-p4d24xlarge-6:10022:10295 [7] NCCL INFO Channel 04 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read
14: queue1-dy-p4d24xlarge-6:10021:10299 [6] NCCL INFO Channel 04 : 14[a01c0] -> 13[901d0] via P2P/IPC/read
11: queue1-dy-p4d24xlarge-6:10018:10294 [3] NCCL INFO Channel 07 : 11[201d0] -> 10[201c0] via P2P/IPC/read
 9: queue1-dy-p4d24xlarge-6:10016:10296 [1] NCCL INFO Channel 02 : 9[101d0] -> 8[101c0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-6:10019:10293 [4] NCCL INFO Channel 02 : 12[901c0] -> 11[201d0] via P2P/IPC/read
 6: queue1-dy-p4d24xlarge-5:10963:11230 [6] NCCL INFO Channel 06 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
15: queue1-dy-p4d24xlarge-6:10022:10295 [7] NCCL INFO Channel 05 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read
14: queue1-dy-p4d24xlarge-6:10021:10299 [6] NCCL INFO Channel 05 : 14[a01c0] -> 13[901d0] via P2P/IPC/read
 9: queue1-dy-p4d24xlarge-6:10016:10296 [1] NCCL INFO Channel 03 : 9[101d0] -> 8[101c0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-6:10019:10293 [4] NCCL INFO Channel 04 : 12[901c0] -> 11[201d0] via P2P/IPC/read
15: queue1-dy-p4d24xlarge-6:10022:10295 [7] NCCL INFO Channel 06 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read
14: queue1-dy-p4d24xlarge-6:10021:10299 [6] NCCL INFO Channel 06 : 14[a01c0] -> 13[901d0] via P2P/IPC/read
 9: queue1-dy-p4d24xlarge-6:10016:10296 [1] NCCL INFO Channel 05 : 9[101d0] -> 8[101c0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-6:10019:10293 [4] NCCL INFO Channel 06 : 12[901c0] -> 11[201d0] via P2P/IPC/read
 9: queue1-dy-p4d24xlarge-6:10016:10296 [1] NCCL INFO Channel 06 : 9[101d0] -> 8[101c0] via P2P/IPC/read
 9: queue1-dy-p4d24xlarge-6:10016:10296 [1] NCCL INFO Channel 07 : 9[101d0] -> 8[101c0] via P2P/IPC/read
15: queue1-dy-p4d24xlarge-6:10022:10295 [7] NCCL INFO Connected all rings
10: queue1-dy-p4d24xlarge-6:10017:10298 [2] NCCL INFO Channel 00 : 10[201c0] -> 9[101d0] via P2P/IPC/read
 7: queue1-dy-p4d24xlarge-5:10964:11233 [7] NCCL INFO Connected all rings
10: queue1-dy-p4d24xlarge-6:10017:10298 [2] NCCL INFO Channel 02 : 10[201c0] -> 9[101d0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-5:10961:11232 [4] NCCL INFO Connected all rings
 3: queue1-dy-p4d24xlarge-5:10960:11228 [3] NCCL INFO Connected all rings
10: queue1-dy-p4d24xlarge-6:10017:10298 [2] NCCL INFO Channel 03 : 10[201c0] -> 9[101d0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-6:10019:10293 [4] NCCL INFO Connected all rings
 2: queue1-dy-p4d24xlarge-5:10959:11229 [2] NCCL INFO Connected all rings
10: queue1-dy-p4d24xlarge-6:10017:10298 [2] NCCL INFO Channel 04 : 10[201c0] -> 9[101d0] via P2P/IPC/read
14: queue1-dy-p4d24xlarge-6:10021:10299 [6] NCCL INFO Connected all rings
13: queue1-dy-p4d24xlarge-6:10020:10292 [5] NCCL INFO Connected all rings
10: queue1-dy-p4d24xlarge-6:10017:10298 [2] NCCL INFO Channel 06 : 10[201c0] -> 9[101d0] via P2P/IPC/read
10: queue1-dy-p4d24xlarge-6:10017:10298 [2] NCCL INFO Channel 07 : 10[201c0] -> 9[101d0] via P2P/IPC/read
 8: queue1-dy-p4d24xlarge-6:10015:10297 [0] NCCL INFO Connected all rings
 8: queue1-dy-p4d24xlarge-6:10015:10297 [0] NCCL INFO Channel 00 : 8[101c0] -> 9[101d0] via P2P/IPC/read
 6: queue1-dy-p4d24xlarge-5:10963:11230 [6] NCCL INFO Connected all rings
 5: queue1-dy-p4d24xlarge-5:10962:11231 [5] NCCL INFO Connected all rings
 8: queue1-dy-p4d24xlarge-6:10015:10297 [0] NCCL INFO Channel 01 : 8[101c0] -> 9[101d0] via P2P/IPC/read
 8: queue1-dy-p4d24xlarge-6:10015:10297 [0] NCCL INFO Channel 02 : 8[101c0] -> 9[101d0] via P2P/IPC/read
 0: queue1-dy-p4d24xlarge-5:10957:11225 [0] NCCL INFO Connected all rings
 0: queue1-dy-p4d24xlarge-5:10957:11225 [0] NCCL INFO Channel 00 : 0[101c0] -> 1[101d0] via P2P/IPC/read
 0: queue1-dy-p4d24xlarge-5:10957:11225 [0] NCCL INFO Channel 01 : 0[101c0] -> 1[101d0] via P2P/IPC/read
14: queue1-dy-p4d24xlarge-6:10021:10299 [6] NCCL INFO Channel 00 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read
 8: queue1-dy-p4d24xlarge-6:10015:10297 [0] NCCL INFO Channel 03 : 8[101c0] -> 9[101d0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-5:10961:11232 [4] NCCL INFO Channel 00 : 4[901c0] -> 5[901d0] via P2P/IPC/read
 0: queue1-dy-p4d24xlarge-5:10957:11225 [0] NCCL INFO Channel 02 : 0[101c0] -> 1[101d0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-6:10019:10293 [4] NCCL INFO Channel 00 : 12[901c0] -> 13[901d0] via P2P/IPC/read
 3: queue1-dy-p4d24xlarge-5:10960:11228 [3] NCCL INFO Channel 00 : 3[201d0] -> 4[901c0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-5:10961:11232 [4] NCCL INFO Channel 01 : 4[901c0] -> 5[901d0] via P2P/IPC/read
 0: queue1-dy-p4d24xlarge-5:10957:11225 [0] NCCL INFO Channel 03 : 0[101c0] -> 1[101d0] via P2P/IPC/read
 3: queue1-dy-p4d24xlarge-5:10960:11228 [3] NCCL INFO Channel 01 : 3[201d0] -> 4[901c0] via P2P/IPC/read
 6: queue1-dy-p4d24xlarge-5:10963:11230 [6] NCCL INFO Channel 00 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
 1: queue1-dy-p4d24xlarge-5:10958:11227 [1] NCCL INFO Connected all rings
 4: queue1-dy-p4d24xlarge-5:10961:11232 [4] NCCL INFO Channel 02 : 4[901c0] -> 5[901d0] via P2P/IPC/read
 0: queue1-dy-p4d24xlarge-5:10957:11225 [0] NCCL INFO Channel 04 : 0[101c0] -> 1[101d0] via P2P/IPC/read
 2: queue1-dy-p4d24xlarge-5:10959:11229 [2] NCCL INFO Channel 00 : 2[201c0] -> 3[201d0] via P2P/IPC/read
14: queue1-dy-p4d24xlarge-6:10021:10299 [6] NCCL INFO Channel 01 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read
 6: queue1-dy-p4d24xlarge-5:10963:11230 [6] NCCL INFO Channel 01 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
 8: queue1-dy-p4d24xlarge-6:10015:10297 [0] NCCL INFO Channel 04 : 8[101c0] -> 9[101d0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-5:10961:11232 [4] NCCL INFO Channel 03 : 4[901c0] -> 5[901d0] via P2P/IPC/read
 2: queue1-dy-p4d24xlarge-5:10959:11229 [2] NCCL INFO Channel 01 : 2[201c0] -> 3[201d0] via P2P/IPC/read
 3: queue1-dy-p4d24xlarge-5:10960:11228 [3] NCCL INFO Channel 03 : 3[201d0] -> 4[901c0] via P2P/IPC/read
 0: queue1-dy-p4d24xlarge-5:10957:11225 [0] NCCL INFO Channel 05 : 0[101c0] -> 1[101d0] via P2P/IPC/read
 6: queue1-dy-p4d24xlarge-5:10963:11230 [6] NCCL INFO Channel 02 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-5:10961:11232 [4] NCCL INFO Channel 04 : 4[901c0] -> 5[901d0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-6:10019:10293 [4] NCCL INFO Channel 01 : 12[901c0] -> 13[901d0] via P2P/IPC/read
 2: queue1-dy-p4d24xlarge-5:10959:11229 [2] NCCL INFO Channel 02 : 2[201c0] -> 3[201d0] via P2P/IPC/read
 3: queue1-dy-p4d24xlarge-5:10960:11228 [3] NCCL INFO Channel 04 : 3[201d0] -> 4[901c0] via P2P/IPC/read
 0: queue1-dy-p4d24xlarge-5:10957:11225 [0] NCCL INFO Channel 06 : 0[101c0] -> 1[101d0] via P2P/IPC/read
 6: queue1-dy-p4d24xlarge-5:10963:11230 [6] NCCL INFO Channel 03 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-5:10961:11232 [4] NCCL INFO Channel 05 : 4[901c0] -> 5[901d0] via P2P/IPC/read
 2: queue1-dy-p4d24xlarge-5:10959:11229 [2] NCCL INFO Channel 03 : 2[201c0] -> 3[201d0] via P2P/IPC/read
 3: queue1-dy-p4d24xlarge-5:10960:11228 [3] NCCL INFO Channel 05 : 3[201d0] -> 4[901c0] via P2P/IPC/read
 0: queue1-dy-p4d24xlarge-5:10957:11225 [0] NCCL INFO Channel 07 : 0[101c0] -> 1[101d0] via P2P/IPC/read
 6: queue1-dy-p4d24xlarge-5:10963:11230 [6] NCCL INFO Channel 04 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
14: queue1-dy-p4d24xlarge-6:10021:10299 [6] NCCL INFO Channel 02 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-5:10961:11232 [4] NCCL INFO Channel 06 : 4[901c0] -> 5[901d0] via P2P/IPC/read
 2: queue1-dy-p4d24xlarge-5:10959:11229 [2] NCCL INFO Channel 04 : 2[201c0] -> 3[201d0] via P2P/IPC/read
 8: queue1-dy-p4d24xlarge-6:10015:10297 [0] NCCL INFO Channel 05 : 8[101c0] -> 9[101d0] via P2P/IPC/read
 3: queue1-dy-p4d24xlarge-5:10960:11228 [3] NCCL INFO Channel 07 : 3[201d0] -> 4[901c0] via P2P/IPC/read
 6: queue1-dy-p4d24xlarge-5:10963:11230 [6] NCCL INFO Channel 05 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-6:10019:10293 [4] NCCL INFO Channel 02 : 12[901c0] -> 13[901d0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-5:10961:11232 [4] NCCL INFO Channel 07 : 4[901c0] -> 5[901d0] via P2P/IPC/read
 5: queue1-dy-p4d24xlarge-5:10962:11231 [5] NCCL INFO Channel 00 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
 2: queue1-dy-p4d24xlarge-5:10959:11229 [2] NCCL INFO Channel 05 : 2[201c0] -> 3[201d0] via P2P/IPC/read
 6: queue1-dy-p4d24xlarge-5:10963:11230 [6] NCCL INFO Channel 06 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
 5: queue1-dy-p4d24xlarge-5:10962:11231 [5] NCCL INFO Channel 01 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
 2: queue1-dy-p4d24xlarge-5:10959:11229 [2] NCCL INFO Channel 06 : 2[201c0] -> 3[201d0] via P2P/IPC/read
11: queue1-dy-p4d24xlarge-6:10018:10294 [3] NCCL INFO Connected all rings
 6: queue1-dy-p4d24xlarge-5:10963:11230 [6] NCCL INFO Channel 07 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
 5: queue1-dy-p4d24xlarge-5:10962:11231 [5] NCCL INFO Channel 02 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
 2: queue1-dy-p4d24xlarge-5:10959:11229 [2] NCCL INFO Channel 07 : 2[201c0] -> 3[201d0] via P2P/IPC/read
14: queue1-dy-p4d24xlarge-6:10021:10299 [6] NCCL INFO Channel 03 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read
 8: queue1-dy-p4d24xlarge-6:10015:10297 [0] NCCL INFO Channel 06 : 8[101c0] -> 9[101d0] via P2P/IPC/read
 1: queue1-dy-p4d24xlarge-5:10958:11227 [1] NCCL INFO Channel 00 : 1[101d0] -> 2[201c0] via P2P/IPC/read
 5: queue1-dy-p4d24xlarge-5:10962:11231 [5] NCCL INFO Channel 04 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-6:10019:10293 [4] NCCL INFO Channel 03 : 12[901c0] -> 13[901d0] via P2P/IPC/read
 5: queue1-dy-p4d24xlarge-5:10962:11231 [5] NCCL INFO Channel 05 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
 1: queue1-dy-p4d24xlarge-5:10958:11227 [1] NCCL INFO Channel 02 : 1[101d0] -> 2[201c0] via P2P/IPC/read
 9: queue1-dy-p4d24xlarge-6:10016:10296 [1] NCCL INFO Connected all rings
14: queue1-dy-p4d24xlarge-6:10021:10299 [6] NCCL INFO Channel 04 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read
 8: queue1-dy-p4d24xlarge-6:10015:10297 [0] NCCL INFO Channel 07 : 8[101c0] -> 9[101d0] via P2P/IPC/read
10: queue1-dy-p4d24xlarge-6:10017:10298 [2] NCCL INFO Connected all rings
12: queue1-dy-p4d24xlarge-6:10019:10293 [4] NCCL INFO Channel 04 : 12[901c0] -> 13[901d0] via P2P/IPC/read
14: queue1-dy-p4d24xlarge-6:10021:10299 [6] NCCL INFO Channel 05 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read
 5: queue1-dy-p4d24xlarge-5:10962:11231 [5] NCCL INFO Channel 06 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-6:10019:10293 [4] NCCL INFO Channel 05 : 12[901c0] -> 13[901d0] via P2P/IPC/read
 1: queue1-dy-p4d24xlarge-5:10958:11227 [1] NCCL INFO Channel 03 : 1[101d0] -> 2[201c0] via P2P/IPC/read
14: queue1-dy-p4d24xlarge-6:10021:10299 [6] NCCL INFO Channel 06 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read
13: queue1-dy-p4d24xlarge-6:10020:10292 [5] NCCL INFO Channel 00 : 13[901d0] -> 14[a01c0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-6:10019:10293 [4] NCCL INFO Channel 06 : 12[901c0] -> 13[901d0] via P2P/IPC/read
14: queue1-dy-p4d24xlarge-6:10021:10299 [6] NCCL INFO Channel 07 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read
13: queue1-dy-p4d24xlarge-6:10020:10292 [5] NCCL INFO Channel 01 : 13[901d0] -> 14[a01c0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-6:10019:10293 [4] NCCL INFO Channel 07 : 12[901c0] -> 13[901d0] via P2P/IPC/read
13: queue1-dy-p4d24xlarge-6:10020:10292 [5] NCCL INFO Channel 02 : 13[901d0] -> 14[a01c0] via P2P/IPC/read
13: queue1-dy-p4d24xlarge-6:10020:10292 [5] NCCL INFO Channel 04 : 13[901d0] -> 14[a01c0] via P2P/IPC/read
 1: queue1-dy-p4d24xlarge-5:10958:11227 [1] NCCL INFO Channel 04 : 1[101d0] -> 2[201c0] via P2P/IPC/read
10: queue1-dy-p4d24xlarge-6:10017:10298 [2] NCCL INFO Channel 00 : 10[201c0] -> 11[201d0] via P2P/IPC/read
13: queue1-dy-p4d24xlarge-6:10020:10292 [5] NCCL INFO Channel 05 : 13[901d0] -> 14[a01c0] via P2P/IPC/read
11: queue1-dy-p4d24xlarge-6:10018:10294 [3] NCCL INFO Channel 00 : 11[201d0] -> 12[901c0] via P2P/IPC/read
10: queue1-dy-p4d24xlarge-6:10017:10298 [2] NCCL INFO Channel 01 : 10[201c0] -> 11[201d0] via P2P/IPC/read
13: queue1-dy-p4d24xlarge-6:10020:10292 [5] NCCL INFO Channel 06 : 13[901d0] -> 14[a01c0] via P2P/IPC/read
11: queue1-dy-p4d24xlarge-6:10018:10294 [3] NCCL INFO Channel 01 : 11[201d0] -> 12[901c0] via P2P/IPC/read
 1: queue1-dy-p4d24xlarge-5:10958:11227 [1] NCCL INFO Channel 06 : 1[101d0] -> 2[201c0] via P2P/IPC/read
10: queue1-dy-p4d24xlarge-6:10017:10298 [2] NCCL INFO Channel 02 : 10[201c0] -> 11[201d0] via P2P/IPC/read
11: queue1-dy-p4d24xlarge-6:10018:10294 [3] NCCL INFO Channel 03 : 11[201d0] -> 12[901c0] via P2P/IPC/read
 9: queue1-dy-p4d24xlarge-6:10016:10296 [1] NCCL INFO Channel 00 : 9[101d0] -> 10[201c0] via P2P/IPC/read
10: queue1-dy-p4d24xlarge-6:10017:10298 [2] NCCL INFO Channel 03 : 10[201c0] -> 11[201d0] via P2P/IPC/read
11: queue1-dy-p4d24xlarge-6:10018:10294 [3] NCCL INFO Channel 04 : 11[201d0] -> 12[901c0] via P2P/IPC/read
 9: queue1-dy-p4d24xlarge-6:10016:10296 [1] NCCL INFO Channel 02 : 9[101d0] -> 10[201c0] via P2P/IPC/read
10: queue1-dy-p4d24xlarge-6:10017:10298 [2] NCCL INFO Channel 04 : 10[201c0] -> 11[201d0] via P2P/IPC/read
 1: queue1-dy-p4d24xlarge-5:10958:11227 [1] NCCL INFO Channel 07 : 1[101d0] -> 2[201c0] via P2P/IPC/read
11: queue1-dy-p4d24xlarge-6:10018:10294 [3] NCCL INFO Channel 05 : 11[201d0] -> 12[901c0] via P2P/IPC/read
 9: queue1-dy-p4d24xlarge-6:10016:10296 [1] NCCL INFO Channel 03 : 9[101d0] -> 10[201c0] via P2P/IPC/read
10: queue1-dy-p4d24xlarge-6:10017:10298 [2] NCCL INFO Channel 05 : 10[201c0] -> 11[201d0] via P2P/IPC/read
 9: queue1-dy-p4d24xlarge-6:10016:10296 [1] NCCL INFO Channel 04 : 9[101d0] -> 10[201c0] via P2P/IPC/read
11: queue1-dy-p4d24xlarge-6:10018:10294 [3] NCCL INFO Channel 07 : 11[201d0] -> 12[901c0] via P2P/IPC/read
10: queue1-dy-p4d24xlarge-6:10017:10298 [2] NCCL INFO Channel 06 : 10[201c0] -> 11[201d0] via P2P/IPC/read
 9: queue1-dy-p4d24xlarge-6:10016:10296 [1] NCCL INFO Channel 06 : 9[101d0] -> 10[201c0] via P2P/IPC/read
10: queue1-dy-p4d24xlarge-6:10017:10298 [2] NCCL INFO Channel 07 : 10[201c0] -> 11[201d0] via P2P/IPC/read
 9: queue1-dy-p4d24xlarge-6:10016:10296 [1] NCCL INFO Channel 07 : 9[101d0] -> 10[201c0] via P2P/IPC/read
13: queue1-dy-p4d24xlarge-6:10020:10292 [5] NCCL INFO Channel 02 : 13[901d0] -> 12[901c0] via P2P/IPC/read
13: queue1-dy-p4d24xlarge-6:10020:10292 [5] NCCL INFO Channel 06 : 13[901d0] -> 12[901c0] via P2P/IPC/read
 5: queue1-dy-p4d24xlarge-5:10962:11231 [5] NCCL INFO Channel 02 : 5[901d0] -> 4[901c0] via P2P/IPC/read
 0: queue1-dy-p4d24xlarge-5:10957:11225 [0] NCCL INFO Channel 01 : 0[101c0] -> 7[a01d0] via P2P/IPC/read
 3: queue1-dy-p4d24xlarge-5:10960:11228 [3] NCCL INFO Channel 01 : 3[201d0] -> 2[201c0] via P2P/IPC/read
 5: queue1-dy-p4d24xlarge-5:10962:11231 [5] NCCL INFO Channel 06 : 5[901d0] -> 4[901c0] via P2P/IPC/read
 0: queue1-dy-p4d24xlarge-5:10957:11225 [0] NCCL INFO Channel 03 : 0[101c0] -> 7[a01d0] via P2P/IPC/read
 3: queue1-dy-p4d24xlarge-5:10960:11228 [3] NCCL INFO Channel 05 : 3[201d0] -> 2[201c0] via P2P/IPC/read
 0: queue1-dy-p4d24xlarge-5:10957:11225 [0] NCCL INFO Channel 05 : 0[101c0] -> 7[a01d0] via P2P/IPC/read
 8: queue1-dy-p4d24xlarge-6:10015:10297 [0] NCCL INFO Channel 01 : 8[101c0] -> 15[a01d0] via P2P/IPC/read
 0: queue1-dy-p4d24xlarge-5:10957:11225 [0] NCCL INFO Channel 07 : 0[101c0] -> 7[a01d0] via P2P/IPC/read
 1: queue1-dy-p4d24xlarge-5:10958:11227 [1] NCCL INFO Channel 00 : 1[101d0] -> 0[101c0] via P2P/IPC/read
 1: queue1-dy-p4d24xlarge-5:10958:11227 [1] NCCL INFO Channel 04 : 1[101d0] -> 0[101c0] via P2P/IPC/read
 8: queue1-dy-p4d24xlarge-6:10015:10297 [0] NCCL INFO Channel 03 : 8[101c0] -> 15[a01d0] via P2P/IPC/read
 7: queue1-dy-p4d24xlarge-5:10964:11233 [7] NCCL INFO Channel 01 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
 7: queue1-dy-p4d24xlarge-5:10964:11233 [7] NCCL INFO Channel 02 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
 7: queue1-dy-p4d24xlarge-5:10964:11233 [7] NCCL INFO Channel 03 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
 8: queue1-dy-p4d24xlarge-6:10015:10297 [0] NCCL INFO Channel 05 : 8[101c0] -> 15[a01d0] via P2P/IPC/read
 7: queue1-dy-p4d24xlarge-5:10964:11233 [7] NCCL INFO Channel 05 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
 7: queue1-dy-p4d24xlarge-5:10964:11233 [7] NCCL INFO Channel 06 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
 7: queue1-dy-p4d24xlarge-5:10964:11233 [7] NCCL INFO Channel 07 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
 8: queue1-dy-p4d24xlarge-6:10015:10297 [0] NCCL INFO Channel 07 : 8[101c0] -> 15[a01d0] via P2P/IPC/read
 9: queue1-dy-p4d24xlarge-6:10016:10296 [1] NCCL INFO Channel 00 : 9[101d0] -> 8[101c0] via P2P/IPC/read
 9: queue1-dy-p4d24xlarge-6:10016:10296 [1] NCCL INFO Channel 04 : 9[101d0] -> 8[101c0] via P2P/IPC/read
15: queue1-dy-p4d24xlarge-6:10022:10295 [7] NCCL INFO Channel 01 : 15[a01d0] -> 8[101c0] via P2P/IPC/read
11: queue1-dy-p4d24xlarge-6:10018:10294 [3] NCCL INFO Channel 01 : 11[201d0] -> 10[201c0] via P2P/IPC/read
15: queue1-dy-p4d24xlarge-6:10022:10295 [7] NCCL INFO Channel 02 : 15[a01d0] -> 8[101c0] via P2P/IPC/read
11: queue1-dy-p4d24xlarge-6:10018:10294 [3] NCCL INFO Channel 05 : 11[201d0] -> 10[201c0] via P2P/IPC/read
15: queue1-dy-p4d24xlarge-6:10022:10295 [7] NCCL INFO Channel 03 : 15[a01d0] -> 8[101c0] via P2P/IPC/read
15: queue1-dy-p4d24xlarge-6:10022:10295 [7] NCCL INFO Channel 05 : 15[a01d0] -> 8[101c0] via P2P/IPC/read
15: queue1-dy-p4d24xlarge-6:10022:10295 [7] NCCL INFO Channel 06 : 15[a01d0] -> 8[101c0] via P2P/IPC/read
15: queue1-dy-p4d24xlarge-6:10022:10295 [7] NCCL INFO Channel 07 : 15[a01d0] -> 8[101c0] via P2P/IPC/read
14: queue1-dy-p4d24xlarge-6:10021:10299 [6] NCCL INFO Channel 03/0 : 6[a01c0] -> 14[a01c0] [receive] via NET/AWS Libfabric/3/GDRDMA
14: queue1-dy-p4d24xlarge-6:10021:10299 [6] NCCL INFO Channel 07/0 : 6[a01c0] -> 14[a01c0] [receive] via NET/AWS Libfabric/3/GDRDMA
14: queue1-dy-p4d24xlarge-6:10021:10299 [6] NCCL INFO Channel 03/0 : 14[a01c0] -> 6[a01c0] [send] via NET/AWS Libfabric/3/GDRDMA
14: queue1-dy-p4d24xlarge-6:10021:10299 [6] NCCL INFO Channel 07/0 : 14[a01c0] -> 6[a01c0] [send] via NET/AWS Libfabric/3/GDRDMA
 4: queue1-dy-p4d24xlarge-5:10961:11232 [4] NCCL INFO Channel 02/0 : 12[901c0] -> 4[901c0] [receive] via NET/AWS Libfabric/2/GDRDMA
 6: queue1-dy-p4d24xlarge-5:10963:11230 [6] NCCL INFO Channel 03/0 : 14[a01c0] -> 6[a01c0] [receive] via NET/AWS Libfabric/3/GDRDMA
 4: queue1-dy-p4d24xlarge-5:10961:11232 [4] NCCL INFO Channel 06/0 : 12[901c0] -> 4[901c0] [receive] via NET/AWS Libfabric/2/GDRDMA
 6: queue1-dy-p4d24xlarge-5:10963:11230 [6] NCCL INFO Channel 07/0 : 14[a01c0] -> 6[a01c0] [receive] via NET/AWS Libfabric/3/GDRDMA
 4: queue1-dy-p4d24xlarge-5:10961:11232 [4] NCCL INFO Channel 02/0 : 4[901c0] -> 12[901c0] [send] via NET/AWS Libfabric/2/GDRDMA
 6: queue1-dy-p4d24xlarge-5:10963:11230 [6] NCCL INFO Channel 03/0 : 6[a01c0] -> 14[a01c0] [send] via NET/AWS Libfabric/3/GDRDMA
 4: queue1-dy-p4d24xlarge-5:10961:11232 [4] NCCL INFO Channel 06/0 : 4[901c0] -> 12[901c0] [send] via NET/AWS Libfabric/2/GDRDMA
 6: queue1-dy-p4d24xlarge-5:10963:11230 [6] NCCL INFO Channel 07/0 : 6[a01c0] -> 14[a01c0] [send] via NET/AWS Libfabric/3/GDRDMA
 2: queue1-dy-p4d24xlarge-5:10959:11229 [2] NCCL INFO Channel 01/0 : 10[201c0] -> 2[201c0] [receive] via NET/AWS Libfabric/1/GDRDMA
 2: queue1-dy-p4d24xlarge-5:10959:11229 [2] NCCL INFO Channel 05/0 : 10[201c0] -> 2[201c0] [receive] via NET/AWS Libfabric/1/GDRDMA
 2: queue1-dy-p4d24xlarge-5:10959:11229 [2] NCCL INFO Channel 01/0 : 2[201c0] -> 10[201c0] [send] via NET/AWS Libfabric/1/GDRDMA
 2: queue1-dy-p4d24xlarge-5:10959:11229 [2] NCCL INFO Channel 05/0 : 2[201c0] -> 10[201c0] [send] via NET/AWS Libfabric/1/GDRDMA
 0: queue1-dy-p4d24xlarge-5:10957:11225 [0] NCCL INFO Channel 00/0 : 8[101c0] -> 0[101c0] [receive] via NET/AWS Libfabric/0/GDRDMA
 0: queue1-dy-p4d24xlarge-5:10957:11225 [0] NCCL INFO Channel 04/0 : 8[101c0] -> 0[101c0] [receive] via NET/AWS Libfabric/0/GDRDMA
 0: queue1-dy-p4d24xlarge-5:10957:11225 [0] NCCL INFO Channel 00/0 : 0[101c0] -> 8[101c0] [send] via NET/AWS Libfabric/0/GDRDMA
 0: queue1-dy-p4d24xlarge-5:10957:11225 [0] NCCL INFO Channel 04/0 : 0[101c0] -> 8[101c0] [send] via NET/AWS Libfabric/0/GDRDMA
12: queue1-dy-p4d24xlarge-6:10019:10293 [4] NCCL INFO Channel 02/0 : 4[901c0] -> 12[901c0] [receive] via NET/AWS Libfabric/2/GDRDMA
12: queue1-dy-p4d24xlarge-6:10019:10293 [4] NCCL INFO Channel 06/0 : 4[901c0] -> 12[901c0] [receive] via NET/AWS Libfabric/2/GDRDMA
12: queue1-dy-p4d24xlarge-6:10019:10293 [4] NCCL INFO Channel 02/0 : 12[901c0] -> 4[901c0] [send] via NET/AWS Libfabric/2/GDRDMA
12: queue1-dy-p4d24xlarge-6:10019:10293 [4] NCCL INFO Channel 06/0 : 12[901c0] -> 4[901c0] [send] via NET/AWS Libfabric/2/GDRDMA
10: queue1-dy-p4d24xlarge-6:10017:10298 [2] NCCL INFO Channel 01/0 : 2[201c0] -> 10[201c0] [receive] via NET/AWS Libfabric/1/GDRDMA
10: queue1-dy-p4d24xlarge-6:10017:10298 [2] NCCL INFO Channel 05/0 : 2[201c0] -> 10[201c0] [receive] via NET/AWS Libfabric/1/GDRDMA
10: queue1-dy-p4d24xlarge-6:10017:10298 [2] NCCL INFO Channel 01/0 : 10[201c0] -> 2[201c0] [send] via NET/AWS Libfabric/1/GDRDMA
10: queue1-dy-p4d24xlarge-6:10017:10298 [2] NCCL INFO Channel 05/0 : 10[201c0] -> 2[201c0] [send] via NET/AWS Libfabric/1/GDRDMA
 8: queue1-dy-p4d24xlarge-6:10015:10297 [0] NCCL INFO Channel 00/0 : 0[101c0] -> 8[101c0] [receive] via NET/AWS Libfabric/0/GDRDMA
 8: queue1-dy-p4d24xlarge-6:10015:10297 [0] NCCL INFO Channel 04/0 : 0[101c0] -> 8[101c0] [receive] via NET/AWS Libfabric/0/GDRDMA
 8: queue1-dy-p4d24xlarge-6:10015:10297 [0] NCCL INFO Channel 00/0 : 8[101c0] -> 0[101c0] [send] via NET/AWS Libfabric/0/GDRDMA
 8: queue1-dy-p4d24xlarge-6:10015:10297 [0] NCCL INFO Channel 04/0 : 8[101c0] -> 0[101c0] [send] via NET/AWS Libfabric/0/GDRDMA
 4: queue1-dy-p4d24xlarge-5:10961:11232 [4] NCCL INFO Channel 01 : 4[901c0] -> 3[201d0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-5:10961:11232 [4] NCCL INFO Channel 03 : 4[901c0] -> 3[201d0] via P2P/IPC/read
 2: queue1-dy-p4d24xlarge-5:10959:11229 [2] NCCL INFO Connected all trees
 2: queue1-dy-p4d24xlarge-5:10959:11229 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
 2: queue1-dy-p4d24xlarge-5:10959:11229 [2] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer
 4: queue1-dy-p4d24xlarge-5:10961:11232 [4] NCCL INFO Channel 05 : 4[901c0] -> 3[201d0] via P2P/IPC/read
10: queue1-dy-p4d24xlarge-6:10017:10298 [2] NCCL INFO Connected all trees
10: queue1-dy-p4d24xlarge-6:10017:10298 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
10: queue1-dy-p4d24xlarge-6:10017:10298 [2] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer
12: queue1-dy-p4d24xlarge-6:10019:10293 [4] NCCL INFO Channel 01 : 12[901c0] -> 11[201d0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-5:10961:11232 [4] NCCL INFO Channel 07 : 4[901c0] -> 3[201d0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-6:10019:10293 [4] NCCL INFO Channel 03 : 12[901c0] -> 11[201d0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-6:10019:10293 [4] NCCL INFO Channel 05 : 12[901c0] -> 11[201d0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-6:10019:10293 [4] NCCL INFO Channel 07 : 12[901c0] -> 11[201d0] via P2P/IPC/read
13: queue1-dy-p4d24xlarge-6:10020:10292 [5] NCCL INFO Connected all trees
13: queue1-dy-p4d24xlarge-6:10020:10292 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
13: queue1-dy-p4d24xlarge-6:10020:10292 [5] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer
 4: queue1-dy-p4d24xlarge-5:10961:11232 [4] NCCL INFO Connected all trees
 4: queue1-dy-p4d24xlarge-5:10961:11232 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
 4: queue1-dy-p4d24xlarge-5:10961:11232 [4] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer
 5: queue1-dy-p4d24xlarge-5:10962:11231 [5] NCCL INFO Connected all trees
 5: queue1-dy-p4d24xlarge-5:10962:11231 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
 5: queue1-dy-p4d24xlarge-5:10962:11231 [5] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer
11: queue1-dy-p4d24xlarge-6:10018:10294 [3] NCCL INFO Connected all trees
11: queue1-dy-p4d24xlarge-6:10018:10294 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
11: queue1-dy-p4d24xlarge-6:10018:10294 [3] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer
 3: queue1-dy-p4d24xlarge-5:10960:11228 [3] NCCL INFO Connected all trees
 3: queue1-dy-p4d24xlarge-5:10960:11228 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
 3: queue1-dy-p4d24xlarge-5:10960:11228 [3] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer
12: queue1-dy-p4d24xlarge-6:10019:10293 [4] NCCL INFO Connected all trees
12: queue1-dy-p4d24xlarge-6:10019:10293 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
12: queue1-dy-p4d24xlarge-6:10019:10293 [4] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer
 7: queue1-dy-p4d24xlarge-5:10964:11233 [7] NCCL INFO Channel 03 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
 7: queue1-dy-p4d24xlarge-5:10964:11233 [7] NCCL INFO Channel 07 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
15: queue1-dy-p4d24xlarge-6:10022:10295 [7] NCCL INFO Channel 03 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read
15: queue1-dy-p4d24xlarge-6:10022:10295 [7] NCCL INFO Channel 07 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read
 6: queue1-dy-p4d24xlarge-5:10963:11230 [6] NCCL INFO Connected all trees
 6: queue1-dy-p4d24xlarge-5:10963:11230 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
 6: queue1-dy-p4d24xlarge-5:10963:11230 [6] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer
 7: queue1-dy-p4d24xlarge-5:10964:11233 [7] NCCL INFO Connected all trees
 7: queue1-dy-p4d24xlarge-5:10964:11233 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
 7: queue1-dy-p4d24xlarge-5:10964:11233 [7] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer
 0: queue1-dy-p4d24xlarge-5:10957:11225 [0] NCCL INFO Connected all trees
 0: queue1-dy-p4d24xlarge-5:10957:11225 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
 0: queue1-dy-p4d24xlarge-5:10957:11225 [0] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer
14: queue1-dy-p4d24xlarge-6:10021:10299 [6] NCCL INFO Connected all trees
14: queue1-dy-p4d24xlarge-6:10021:10299 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
14: queue1-dy-p4d24xlarge-6:10021:10299 [6] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer
15: queue1-dy-p4d24xlarge-6:10022:10295 [7] NCCL INFO Connected all trees
15: queue1-dy-p4d24xlarge-6:10022:10295 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
15: queue1-dy-p4d24xlarge-6:10022:10295 [7] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer
 9: queue1-dy-p4d24xlarge-6:10016:10296 [1] NCCL INFO Connected all trees
 9: queue1-dy-p4d24xlarge-6:10016:10296 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
 9: queue1-dy-p4d24xlarge-6:10016:10296 [1] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer
 8: queue1-dy-p4d24xlarge-6:10015:10297 [0] NCCL INFO Connected all trees
 8: queue1-dy-p4d24xlarge-6:10015:10297 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
 8: queue1-dy-p4d24xlarge-6:10015:10297 [0] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer
 1: queue1-dy-p4d24xlarge-5:10958:11227 [1] NCCL INFO Connected all trees
 1: queue1-dy-p4d24xlarge-5:10958:11227 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
 1: queue1-dy-p4d24xlarge-5:10958:11227 [1] NCCL INFO 8 coll channels, 8 p2p channels, 2 p2p channels per peer
 8: queue1-dy-p4d24xlarge-6:10015:10297 [0] NCCL INFO comm 0x7f3084002fb0 rank 8 nranks 16 cudaDev 0 busId 101c0 - Init COMPLETE
10: queue1-dy-p4d24xlarge-6:10017:10298 [2] NCCL INFO comm 0x7feab8002fb0 rank 10 nranks 16 cudaDev 2 busId 201c0 - Init COMPLETE
12: queue1-dy-p4d24xlarge-6:10019:10293 [4] NCCL INFO comm 0x7f4724002fb0 rank 12 nranks 16 cudaDev 4 busId 901c0 - Init COMPLETE
 9: queue1-dy-p4d24xlarge-6:10016:10296 [1] NCCL INFO comm 0x7fe830002fb0 rank 9 nranks 16 cudaDev 1 busId 101d0 - Init COMPLETE
13: queue1-dy-p4d24xlarge-6:10020:10292 [5] NCCL INFO comm 0x7f6b60002fb0 rank 13 nranks 16 cudaDev 5 busId 901d0 - Init COMPLETE
14: queue1-dy-p4d24xlarge-6:10021:10299 [6] NCCL INFO comm 0x7efd58002fb0 rank 14 nranks 16 cudaDev 6 busId a01c0 - Init COMPLETE
11: queue1-dy-p4d24xlarge-6:10018:10294 [3] NCCL INFO comm 0x7facb8002fb0 rank 11 nranks 16 cudaDev 3 busId 201d0 - Init COMPLETE
15: queue1-dy-p4d24xlarge-6:10022:10295 [7] NCCL INFO comm 0x7f9b3c002fb0 rank 15 nranks 16 cudaDev 7 busId a01d0 - Init COMPLETE
 2: queue1-dy-p4d24xlarge-5:10959:11229 [2] NCCL INFO comm 0x7f7018002fb0 rank 2 nranks 16 cudaDev 2 busId 201c0 - Init COMPLETE
 4: queue1-dy-p4d24xlarge-5:10961:11232 [4] NCCL INFO comm 0x7f32e0002fb0 rank 4 nranks 16 cudaDev 4 busId 901c0 - Init COMPLETE
 6: queue1-dy-p4d24xlarge-5:10963:11230 [6] NCCL INFO comm 0x7fbab0002fb0 rank 6 nranks 16 cudaDev 6 busId a01c0 - Init COMPLETE
 0: queue1-dy-p4d24xlarge-5:10957:11225 [0] NCCL INFO comm 0x7fa96c002fb0 rank 0 nranks 16 cudaDev 0 busId 101c0 - Init COMPLETE
 0: queue1-dy-p4d24xlarge-5:10957:10957 [0] NCCL INFO Launch mode Parallel
 1: queue1-dy-p4d24xlarge-5:10958:11227 [1] NCCL INFO comm 0x7fb1f4002fb0 rank 1 nranks 16 cudaDev 1 busId 101d0 - Init COMPLETE
 5: queue1-dy-p4d24xlarge-5:10962:11231 [5] NCCL INFO comm 0x7fbf04002fb0 rank 5 nranks 16 cudaDev 5 busId 901d0 - Init COMPLETE
 7: queue1-dy-p4d24xlarge-5:10964:11233 [7] NCCL INFO comm 0x7fcdbc002fb0 rank 7 nranks 16 cudaDev 7 busId a01d0 - Init COMPLETE
 3: queue1-dy-p4d24xlarge-5:10960:11228 [3] NCCL INFO comm 0x7f836c002fb0 rank 3 nranks 16 cudaDev 3 busId 201d0 - Init COMPLETE
 0: 2022-11-03 07:37:25 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.1, activation_fn='silu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=True, adaptive_input_cutoff='20000,60000', adaptive_input_factor=4, adaptive_softmax_cutoff='20000,60000', adaptive_softmax_dropout=0.2, adaptive_softmax_factor=4, add_bos_token=False, all_gather_list_size=16384, arch='mega_lm_adaptive_big', attention_activation_fn='softmax', attention_dropout=0.1, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', clip_mode='total', clip_norm=0.25, cpu=False, criterion='adaptive_loss', curriculum=0, data='/fsx/datasets/wikitext-103', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', decoder_chunk_size=1024, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_hidden_dim=2048, decoder_input_dim=1024, decoder_layers=16, decoder_n_dim=16, decoder_z_dim=256, device_id=0, disable_validation=False, distributed_b
 0: ackend='nccl', distributed_init_method='env://', distributed_no_spawn=True, distributed_port=-1, distributed_rank=0, distributed_world_size=16, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, end_learning_rate=0.0, fast_stat_sync=False, feature_dropout=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, future_target=False, hidden_dropout=0.1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, localsgd_frequency=3, log_format='simple', log_interval=100, lr=[0.005], lr_scheduler='linear_decay', max_epoch=0, max_sentences=None, max_sentences_valid=None, max_target_positions=8096, max_tokens=6144, max_tokens_valid=6144, max_update=400000, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_affine_final_norm=True
 0: , no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, normalization_type='layernorm', normalize_before=True, normalize_embedding=False, nprocs_per_node=8, num_workers=0, optimizer='adam', optimizer_overrides='{}', output_dictionary_size=-1, past_target=False, patience=-1, profile=False, quantization_config_path=None, rel_pos_bias='rotary', report_ema_alpha=False, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sample_break_mode='complete', save_dir='saved_models/aws_pytorch_111_cu115', save_interval=1, save_interval_updates=0, seed=42, self_target=False, sentence_avg=False, share_all_embeddings=False, share_decoder_input_output_embed=False, shorten_data_split_list='', shorten_method='none', skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=Non
 0: e, stop_min_lr=-1, stop_time_hours=0, task='language_modeling', tensorboard_logdir='', test_subset='test', threshold_loss_scale=None, tie_adaptive_proj=True, tie_adaptive_weights=True, tokenizer=None, tokens_per_sample=2048, total_num_update=400000, tpu=False, train_subset='train', truncation_length=8192, update_freq=[1], use_bmuf=False, use_old_adam=False, user_dir=None, valid_block='splits:10', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, variant_block_multiple_max=6, variant_block_multiple_min=2, wandb_entity=None, wandb_id=None, wandb_project=None, warmup_init_lr=1e-07, warmup_power=1, warmup_updates=24000, weight_decay=0.1, write_out_alpha=False)
 0: 2022-11-03 07:37:25 | INFO | fairseq.tasks.language_modeling | dictionary: 267744 types
 0: 2022-11-03 07:37:25 | INFO | fairseq.data.data_utils | loaded 3760 examples from: /fsx/datasets/wikitext-103/valid
 0: 2022-11-03 07:37:30 | INFO | fairseq_cli.train | MegaLanguageModel(
 0:   (decoder): MegaDecoderNoCrossAttn(
 0:     (embedding_dropout): FairseqDropout(p=0.3)
 0:     (embed_tokens): AdaptiveInput(
 0:       (embeddings): ModuleList(
 0:         (0): Sequential(
 0:           (0): Embedding(20000, 1024, padding_idx=1)
 0:           (1): Linear(in_features=1024, out_features=1024, bias=False)
 0:         )
 0:         (1): Sequential(
 0:           (0): Embedding(40000, 256)
 0:           (1): Linear(in_features=256, out_features=1024, bias=False)
 0:         )
 0:         (2): Sequential(
 0:           (0): Embedding(207744, 64)
 0:           (1): Linear(in_features=64, out_features=1024, bias=False)
 0:         )
 0:       )
 0:     )
 0:     (layers): ModuleList(
 0:       (0): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:       (1): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:       (2): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:       (3): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:       (4): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:       (5): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:       (6): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:       (7): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:       (8): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:       (9): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:       (10): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:       (11): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:       (12): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:       (13): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:       (14): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:       (15): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:     )
 0:     (final_norm): SequenceNorm(
 0:       (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=False)
 0:     )
 0:     (adaptive_softmax): AdaptiveSoftmax(
 0:       (dropout_module): FairseqDropout(p=0.2)
 0:       (lsm): LogSoftmax(dim=1)
 0:       (head): TiedHeadModule(
 0:         (word_proj): TiedLinear()
 0:         (class_proj): Linear(in_features=1024, out_features=2, bias=False)
 0:       )
 0:       (tail): ModuleList(
 0:         (0): Sequential(
 0:           (0): TiedLinear()
 0:           (1): Dropout(p=0.2, inplace=False)
 0:           (2): TiedLinear()
 0:         )
 0:         (1): Sequential(
 0:           (0): TiedLinear()
 0:           (1): Dropout(p=0.2, inplace=False)
 0:           (2): TiedLinear()
 0:         )
 0:       )
 0:     )
 0:   )
 0: )
 0: 2022-11-03 07:37:30 | INFO | fairseq_cli.train | task: language_modeling (LanguageModelingTask)
 0: 2022-11-03 07:37:30 | INFO | fairseq_cli.train | model: mega_lm_adaptive_big (MegaLanguageModel)
 0: 2022-11-03 07:37:30 | INFO | fairseq_cli.train | criterion: adaptive_loss (AdaptiveLoss)
 0: 2022-11-03 07:37:30 | INFO | fairseq_cli.train | num. model params: 252237824 (num. trained: 252237824)
 0: 2022-11-03 07:37:30 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.embeddings.0.0.weight <- decoder.adaptive_softmax.head.word_proj.weight
 0: 2022-11-03 07:37:30 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.embeddings.0.1.bias <- decoder.embed_tokens.embeddings.1.1.bias
 0: 2022-11-03 07:37:30 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.embeddings.0.1.bias <- decoder.embed_tokens.embeddings.2.1.bias
 0: 2022-11-03 07:37:30 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.embeddings.0.1.bias <- decoder.final_norm.norm.weight
 0: 2022-11-03 07:37:30 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.embeddings.0.1.bias <- decoder.final_norm.norm.bias
 0: 2022-11-03 07:37:30 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.embeddings.0.1.bias <- decoder.adaptive_softmax.head.class_proj.bias
 0: 2022-11-03 07:37:30 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.embeddings.1.0.weight <- decoder.adaptive_softmax.tail.0.2.weight
 0: 2022-11-03 07:37:30 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.embeddings.1.1.weight <- decoder.adaptive_softmax.tail.0.0.weight
 0: 2022-11-03 07:37:30 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.embeddings.2.0.weight <- decoder.adaptive_softmax.tail.1.2.weight
 0: 2022-11-03 07:37:30 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.embeddings.2.1.weight <- decoder.adaptive_softmax.tail.1.0.weight
 0: 2022-11-03 07:37:30 | INFO | fairseq.utils | ***********************CUDA enviroments for all 16 workers***********************
 0: 2022-11-03 07:37:30 | INFO | fairseq.utils | rank   0: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-03 07:37:30 | INFO | fairseq.utils | rank   1: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-03 07:37:30 | INFO | fairseq.utils | rank   2: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-03 07:37:30 | INFO | fairseq.utils | rank   3: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-03 07:37:30 | INFO | fairseq.utils | rank   4: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-03 07:37:30 | INFO | fairseq.utils | rank   5: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-03 07:37:30 | INFO | fairseq.utils | rank   6: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-03 07:37:30 | INFO | fairseq.utils | rank   7: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-03 07:37:30 | INFO | fairseq.utils | rank   8: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-03 07:37:30 | INFO | fairseq.utils | rank   9: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-03 07:37:30 | INFO | fairseq.utils | rank  10: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-03 07:37:30 | INFO | fairseq.utils | rank  11: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-03 07:37:30 | INFO | fairseq.utils | rank  12: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-03 07:37:30 | INFO | fairseq.utils | rank  13: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-03 07:37:30 | INFO | fairseq.utils | rank  14: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-03 07:37:30 | INFO | fairseq.utils | rank  15: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-03 07:37:30 | INFO | fairseq.utils | ***********************CUDA enviroments for all 16 workers***********************
 0: 2022-11-03 07:37:30 | INFO | fairseq_cli.train | training on 16 devices (GPUs/TPUs)
 0: 2022-11-03 07:37:30 | INFO | fairseq_cli.train | max tokens per GPU = 6144 and max sentences per GPU = None
 0: 2022-11-03 07:37:30 | INFO | fairseq.trainer | no existing checkpoint found saved_models/aws_pytorch_111_cu115/checkpoint_last.pt
 0: 2022-11-03 07:37:30 | INFO | fairseq.trainer | loading train data for epoch 1
 0: 2022-11-03 07:37:31 | INFO | fairseq.data.data_utils | loaded 1801350 examples from: /fsx/datasets/wikitext-103/train
 0: 2022-11-03 07:37:31 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16
 0: 2022-11-03 07:37:31 | INFO | fairseq.optim.adam | using FusedAdam
 0: 2022-11-03 07:37:31 | INFO | fairseq.trainer | begin training epoch 1
 0: 2022-11-03 07:38:18 | INFO | train_inner | epoch 001:    100 / 1230 loss=13.75, ppl=13779, wps=188902, ups=2.26, wpb=83611.2, bsz=20.6, num_updates=100, lr=2.09329e-05, gnorm=2.509, clip=100, train_wall=47, wall=48
 0: 2022-11-03 07:38:59 | INFO | train_inner | epoch 001:    200 / 1230 loss=11.124, ppl=2232.36, wps=208251, ups=2.47, wpb=84167.9, bsz=20.8, num_updates=200, lr=4.17658e-05, gnorm=0.55, clip=100, train_wall=40, wall=88
 0: 2022-11-03 07:39:39 | INFO | train_inner | epoch 001:    300 / 1230 loss=10.582, ppl=1532.73, wps=207424, ups=2.47, wpb=84139.5, bsz=20.9, num_updates=300, lr=6.25987e-05, gnorm=0.588, clip=100, train_wall=40, wall=129
 0: 2022-11-03 07:40:20 | INFO | train_inner | epoch 001:    400 / 1230 loss=10.134, ppl=1123.68, wps=205747, ups=2.46, wpb=83759.7, bsz=20.9, num_updates=400, lr=8.34317e-05, gnorm=0.57, clip=100, train_wall=41, wall=170
 0: 2022-11-03 07:41:01 | INFO | train_inner | epoch 001:    500 / 1230 loss=9.691, ppl=826.77, wps=206090, ups=2.47, wpb=83446.3, bsz=20.7, num_updates=500, lr=0.000104265, gnorm=0.6, clip=100, train_wall=40, wall=210
 0: 2022-11-03 07:41:41 | INFO | train_inner | epoch 001:    600 / 1230 loss=9.318, ppl=638.07, wps=205430, ups=2.45, wpb=83961, bsz=21.2, num_updates=600, lr=0.000125097, gnorm=0.57, clip=100, train_wall=41, wall=251
 0: 2022-11-03 07:42:22 | INFO | train_inner | epoch 001:    700 / 1230 loss=9.03, ppl=522.83, wps=207257, ups=2.47, wpb=84005.2, bsz=20.7, num_updates=700, lr=0.00014593, gnorm=0.561, clip=100, train_wall=40, wall=292
 0: 2022-11-03 07:43:03 | INFO | train_inner | epoch 001:    800 / 1230 loss=8.778, ppl=439.05, wps=207742, ups=2.46, wpb=84386.1, bsz=21, num_updates=800, lr=0.000166763, gnorm=0.564, clip=100, train_wall=40, wall=332
 0: 2022-11-03 07:43:43 | INFO | train_inner | epoch 001:    900 / 1230 loss=8.561, ppl=377.65, wps=207556, ups=2.46, wpb=84358.3, bsz=21.1, num_updates=900, lr=0.000187596, gnorm=0.553, clip=100, train_wall=41, wall=373
 0: 2022-11-03 07:44:24 | INFO | train_inner | epoch 001:   1000 / 1230 loss=8.349, ppl=326.12, wps=207460, ups=2.47, wpb=83970.9, bsz=20.8, num_updates=1000, lr=0.000208429, gnorm=0.532, clip=100, train_wall=40, wall=413
