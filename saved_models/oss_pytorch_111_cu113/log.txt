15: 2022-11-02 19:14:18 | INFO | fairseq.distributed_utils | distributed init (rank 15): env://
13: 2022-11-02 19:14:18 | INFO | fairseq.distributed_utils | distributed init (rank 13): env://
 0: 2022-11-02 19:14:18 | INFO | fairseq.distributed_utils | distributed init (rank 0): env://
14: 2022-11-02 19:14:18 | INFO | fairseq.distributed_utils | distributed init (rank 14): env://
14: 2022-11-02 19:14:18 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 14
11: 2022-11-02 19:14:18 | INFO | fairseq.distributed_utils | distributed init (rank 11): env://
 6: 2022-11-02 19:14:18 | INFO | fairseq.distributed_utils | distributed init (rank 6): env://
 6: 2022-11-02 19:14:18 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
10: 2022-11-02 19:14:18 | INFO | fairseq.distributed_utils | distributed init (rank 10): env://
11: 2022-11-02 19:14:18 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 11
12: 2022-11-02 19:14:18 | INFO | fairseq.distributed_utils | distributed init (rank 12): env://
10: 2022-11-02 19:14:18 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 10
 8: 2022-11-02 19:14:18 | INFO | fairseq.distributed_utils | distributed init (rank 8): env://
 9: 2022-11-02 19:14:18 | INFO | fairseq.distributed_utils | distributed init (rank 9): env://
 3: 2022-11-02 19:14:18 | INFO | fairseq.distributed_utils | distributed init (rank 3): env://
12: 2022-11-02 19:14:18 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 12
 2: 2022-11-02 19:14:18 | INFO | fairseq.distributed_utils | distributed init (rank 2): env://
 3: 2022-11-02 19:14:18 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
 2: 2022-11-02 19:14:18 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
 8: 2022-11-02 19:14:18 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 8
 9: 2022-11-02 19:14:18 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 9
 5: 2022-11-02 19:14:18 | INFO | fairseq.distributed_utils | distributed init (rank 5): env://
 5: 2022-11-02 19:14:18 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
 1: 2022-11-02 19:14:18 | INFO | fairseq.distributed_utils | distributed init (rank 1): env://
 1: 2022-11-02 19:14:18 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
 7: 2022-11-02 19:14:18 | INFO | fairseq.distributed_utils | distributed init (rank 7): env://
 7: 2022-11-02 19:14:18 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
 4: 2022-11-02 19:14:18 | INFO | fairseq.distributed_utils | distributed init (rank 4): env://
 4: 2022-11-02 19:14:18 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
13: 2022-11-02 19:14:19 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 13
15: 2022-11-02 19:14:19 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 15
 0: 2022-11-02 19:14:19 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
 0: 2022-11-02 19:14:19 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
 0: 2022-11-02 19:14:19 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-1 as rank 0
14: 2022-11-02 19:14:19 | INFO | torch.distributed.distributed_c10d | Rank 14: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
 6: 2022-11-02 19:14:19 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
11: 2022-11-02 19:14:19 | INFO | torch.distributed.distributed_c10d | Rank 11: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
10: 2022-11-02 19:14:19 | INFO | torch.distributed.distributed_c10d | Rank 10: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
12: 2022-11-02 19:14:19 | INFO | torch.distributed.distributed_c10d | Rank 12: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
 2: 2022-11-02 19:14:19 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
14: 2022-11-02 19:14:19 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-2 as rank 14
 3: 2022-11-02 19:14:19 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
 5: 2022-11-02 19:14:19 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
 6: 2022-11-02 19:14:19 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-1 as rank 6
 8: 2022-11-02 19:14:19 | INFO | torch.distributed.distributed_c10d | Rank 8: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
 9: 2022-11-02 19:14:19 | INFO | torch.distributed.distributed_c10d | Rank 9: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
 1: 2022-11-02 19:14:19 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
 3: 2022-11-02 19:14:19 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-1 as rank 3
 2: 2022-11-02 19:14:19 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-1 as rank 2
 7: 2022-11-02 19:14:19 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
 4: 2022-11-02 19:14:19 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
 5: 2022-11-02 19:14:19 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-1 as rank 5
 1: 2022-11-02 19:14:19 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-1 as rank 1
10: 2022-11-02 19:14:19 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-2 as rank 10
 7: 2022-11-02 19:14:19 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-1 as rank 7
11: 2022-11-02 19:14:19 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-2 as rank 11
12: 2022-11-02 19:14:19 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-2 as rank 12
 8: 2022-11-02 19:14:19 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-2 as rank 8
 9: 2022-11-02 19:14:19 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-2 as rank 9
 4: 2022-11-02 19:14:19 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-1 as rank 4
13: 2022-11-02 19:14:19 | INFO | torch.distributed.distributed_c10d | Rank 13: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
15: 2022-11-02 19:14:19 | INFO | torch.distributed.distributed_c10d | Rank 15: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.
13: 2022-11-02 19:14:19 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-2 as rank 13
15: 2022-11-02 19:14:19 | INFO | fairseq.distributed_utils | initialized host queue1-dy-p4d24xlarge-2 as rank 15
 0: queue1-dy-p4d24xlarge-1:10487:10487 [0] NCCL INFO Bootstrap : Using ens32:10.0.30.85<0>
 0: queue1-dy-p4d24xlarge-1:10487:10487 [0] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v4 symbol.
 0: queue1-dy-p4d24xlarge-1:10487:10487 [0] NCCL INFO NET/IB : No device found.
 0: queue1-dy-p4d24xlarge-1:10487:10487 [0] NCCL INFO NET/Socket : Using [0]ens32:10.0.30.85<0> [1]ens65:10.0.23.119<0> [2]ens129:10.0.30.197<0> [3]ens161:10.0.27.130<0>
 0: queue1-dy-p4d24xlarge-1:10487:10487 [0] NCCL INFO Using network Socket
 0: NCCL version 2.10.3+cuda11.3
15: queue1-dy-p4d24xlarge-2:10522:10522 [7] NCCL INFO Bootstrap : Using ens32:10.0.19.22<0>
15: queue1-dy-p4d24xlarge-2:10522:10522 [7] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v4 symbol.
14: queue1-dy-p4d24xlarge-2:10521:10521 [6] NCCL INFO Bootstrap : Using ens32:10.0.19.22<0>
15: queue1-dy-p4d24xlarge-2:10522:10522 [7] NCCL INFO NET/IB : No device found.
15: queue1-dy-p4d24xlarge-2:10522:10522 [7] NCCL INFO NET/Socket : Using [0]ens32:10.0.19.22<0> [1]ens65:10.0.26.219<0> [2]ens129:10.0.18.247<0> [3]ens161:10.0.26.184<0>
15: queue1-dy-p4d24xlarge-2:10522:10522 [7] NCCL INFO Using network Socket
14: queue1-dy-p4d24xlarge-2:10521:10521 [6] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v4 symbol.
 2: queue1-dy-p4d24xlarge-1:10489:10489 [2] NCCL INFO Bootstrap : Using ens32:10.0.30.85<0>
 2: queue1-dy-p4d24xlarge-1:10489:10489 [2] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v4 symbol.
14: queue1-dy-p4d24xlarge-2:10521:10521 [6] NCCL INFO NET/IB : No device found.
14: queue1-dy-p4d24xlarge-2:10521:10521 [6] NCCL INFO NET/Socket : Using [0]ens32:10.0.19.22<0> [1]ens65:10.0.26.219<0> [2]ens129:10.0.18.247<0> [3]ens161:10.0.26.184<0>
14: queue1-dy-p4d24xlarge-2:10521:10521 [6] NCCL INFO Using network Socket
12: queue1-dy-p4d24xlarge-2:10519:10519 [4] NCCL INFO Bootstrap : Using ens32:10.0.19.22<0>
 7: queue1-dy-p4d24xlarge-1:10494:10494 [7] NCCL INFO Bootstrap : Using ens32:10.0.30.85<0>
12: queue1-dy-p4d24xlarge-2:10519:10519 [4] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v4 symbol.
 2: queue1-dy-p4d24xlarge-1:10489:10489 [2] NCCL INFO NET/IB : No device found.
 2: queue1-dy-p4d24xlarge-1:10489:10489 [2] NCCL INFO NET/Socket : Using [0]ens32:10.0.30.85<0> [1]ens65:10.0.23.119<0> [2]ens129:10.0.30.197<0> [3]ens161:10.0.27.130<0>
 2: queue1-dy-p4d24xlarge-1:10489:10489 [2] NCCL INFO Using network Socket
 7: queue1-dy-p4d24xlarge-1:10494:10494 [7] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v4 symbol.
 6: queue1-dy-p4d24xlarge-1:10493:10493 [6] NCCL INFO Bootstrap : Using ens32:10.0.30.85<0>
 1: queue1-dy-p4d24xlarge-1:10488:10488 [1] NCCL INFO Bootstrap : Using ens32:10.0.30.85<0>
12: queue1-dy-p4d24xlarge-2:10519:10519 [4] NCCL INFO NET/IB : No device found.
 6: queue1-dy-p4d24xlarge-1:10493:10493 [6] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v4 symbol.
12: queue1-dy-p4d24xlarge-2:10519:10519 [4] NCCL INFO NET/Socket : Using [0]ens32:10.0.19.22<0> [1]ens65:10.0.26.219<0> [2]ens129:10.0.18.247<0> [3]ens161:10.0.26.184<0>
12: queue1-dy-p4d24xlarge-2:10519:10519 [4] NCCL INFO Using network Socket
13: queue1-dy-p4d24xlarge-2:10520:10520 [5] NCCL INFO Bootstrap : Using ens32:10.0.19.22<0>
 1: queue1-dy-p4d24xlarge-1:10488:10488 [1] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v4 symbol.
 7: queue1-dy-p4d24xlarge-1:10494:10494 [7] NCCL INFO NET/IB : No device found.
 7: queue1-dy-p4d24xlarge-1:10494:10494 [7] NCCL INFO NET/Socket : Using [0]ens32:10.0.30.85<0> [1]ens65:10.0.23.119<0> [2]ens129:10.0.30.197<0> [3]ens161:10.0.27.130<0>
 7: queue1-dy-p4d24xlarge-1:10494:10494 [7] NCCL INFO Using network Socket
13: queue1-dy-p4d24xlarge-2:10520:10520 [5] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v4 symbol.
 3: queue1-dy-p4d24xlarge-1:10490:10490 [3] NCCL INFO Bootstrap : Using ens32:10.0.30.85<0>
10: queue1-dy-p4d24xlarge-2:10517:10517 [2] NCCL INFO Bootstrap : Using ens32:10.0.19.22<0>
 6: queue1-dy-p4d24xlarge-1:10493:10493 [6] NCCL INFO NET/IB : No device found.
 6: queue1-dy-p4d24xlarge-1:10493:10493 [6] NCCL INFO NET/Socket : Using [0]ens32:10.0.30.85<0> [1]ens65:10.0.23.119<0> [2]ens129:10.0.30.197<0> [3]ens161:10.0.27.130<0>
 6: queue1-dy-p4d24xlarge-1:10493:10493 [6] NCCL INFO Using network Socket
 1: queue1-dy-p4d24xlarge-1:10488:10488 [1] NCCL INFO NET/IB : No device found.
 1: queue1-dy-p4d24xlarge-1:10488:10488 [1] NCCL INFO NET/Socket : Using [0]ens32:10.0.30.85<0> [1]ens65:10.0.23.119<0> [2]ens129:10.0.30.197<0> [3]ens161:10.0.27.130<0>
 1: queue1-dy-p4d24xlarge-1:10488:10488 [1] NCCL INFO Using network Socket
 3: queue1-dy-p4d24xlarge-1:10490:10490 [3] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v4 symbol.
10: queue1-dy-p4d24xlarge-2:10517:10517 [2] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v4 symbol.
13: queue1-dy-p4d24xlarge-2:10520:10520 [5] NCCL INFO NET/IB : No device found.
 8: queue1-dy-p4d24xlarge-2:10515:10515 [0] NCCL INFO Bootstrap : Using ens32:10.0.19.22<0>
13: queue1-dy-p4d24xlarge-2:10520:10520 [5] NCCL INFO NET/Socket : Using [0]ens32:10.0.19.22<0> [1]ens65:10.0.26.219<0> [2]ens129:10.0.18.247<0> [3]ens161:10.0.26.184<0>
13: queue1-dy-p4d24xlarge-2:10520:10520 [5] NCCL INFO Using network Socket
 8: queue1-dy-p4d24xlarge-2:10515:10515 [0] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v4 symbol.
 3: queue1-dy-p4d24xlarge-1:10490:10490 [3] NCCL INFO NET/IB : No device found.
 3: queue1-dy-p4d24xlarge-1:10490:10490 [3] NCCL INFO NET/Socket : Using [0]ens32:10.0.30.85<0> [1]ens65:10.0.23.119<0> [2]ens129:10.0.30.197<0> [3]ens161:10.0.27.130<0>
 3: queue1-dy-p4d24xlarge-1:10490:10490 [3] NCCL INFO Using network Socket
10: queue1-dy-p4d24xlarge-2:10517:10517 [2] NCCL INFO NET/IB : No device found.
10: queue1-dy-p4d24xlarge-2:10517:10517 [2] NCCL INFO NET/Socket : Using [0]ens32:10.0.19.22<0> [1]ens65:10.0.26.219<0> [2]ens129:10.0.18.247<0> [3]ens161:10.0.26.184<0>
10: queue1-dy-p4d24xlarge-2:10517:10517 [2] NCCL INFO Using network Socket
 8: queue1-dy-p4d24xlarge-2:10515:10515 [0] NCCL INFO NET/IB : No device found.
 8: queue1-dy-p4d24xlarge-2:10515:10515 [0] NCCL INFO NET/Socket : Using [0]ens32:10.0.19.22<0> [1]ens65:10.0.26.219<0> [2]ens129:10.0.18.247<0> [3]ens161:10.0.26.184<0>
 8: queue1-dy-p4d24xlarge-2:10515:10515 [0] NCCL INFO Using network Socket
 5: queue1-dy-p4d24xlarge-1:10492:10492 [5] NCCL INFO Bootstrap : Using ens32:10.0.30.85<0>
 5: queue1-dy-p4d24xlarge-1:10492:10492 [5] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v4 symbol.
 4: queue1-dy-p4d24xlarge-1:10491:10491 [4] NCCL INFO Bootstrap : Using ens32:10.0.30.85<0>
 5: queue1-dy-p4d24xlarge-1:10492:10492 [5] NCCL INFO NET/IB : No device found.
 5: queue1-dy-p4d24xlarge-1:10492:10492 [5] NCCL INFO NET/Socket : Using [0]ens32:10.0.30.85<0> [1]ens65:10.0.23.119<0> [2]ens129:10.0.30.197<0> [3]ens161:10.0.27.130<0>
 5: queue1-dy-p4d24xlarge-1:10492:10492 [5] NCCL INFO Using network Socket
 4: queue1-dy-p4d24xlarge-1:10491:10491 [4] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v4 symbol.
 4: queue1-dy-p4d24xlarge-1:10491:10491 [4] NCCL INFO NET/IB : No device found.
 4: queue1-dy-p4d24xlarge-1:10491:10491 [4] NCCL INFO NET/Socket : Using [0]ens32:10.0.30.85<0> [1]ens65:10.0.23.119<0> [2]ens129:10.0.30.197<0> [3]ens161:10.0.27.130<0>
 4: queue1-dy-p4d24xlarge-1:10491:10491 [4] NCCL INFO Using network Socket
 9: queue1-dy-p4d24xlarge-2:10516:10516 [1] NCCL INFO Bootstrap : Using ens32:10.0.19.22<0>
 9: queue1-dy-p4d24xlarge-2:10516:10516 [1] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v4 symbol.
 9: queue1-dy-p4d24xlarge-2:10516:10516 [1] NCCL INFO NET/IB : No device found.
 9: queue1-dy-p4d24xlarge-2:10516:10516 [1] NCCL INFO NET/Socket : Using [0]ens32:10.0.19.22<0> [1]ens65:10.0.26.219<0> [2]ens129:10.0.18.247<0> [3]ens161:10.0.26.184<0>
 9: queue1-dy-p4d24xlarge-2:10516:10516 [1] NCCL INFO Using network Socket
11: queue1-dy-p4d24xlarge-2:10518:10518 [3] NCCL INFO Bootstrap : Using ens32:10.0.19.22<0>
11: queue1-dy-p4d24xlarge-2:10518:10518 [3] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v4 symbol.
11: queue1-dy-p4d24xlarge-2:10518:10518 [3] NCCL INFO NET/IB : No device found.
11: queue1-dy-p4d24xlarge-2:10518:10518 [3] NCCL INFO NET/Socket : Using [0]ens32:10.0.19.22<0> [1]ens65:10.0.26.219<0> [2]ens129:10.0.18.247<0> [3]ens161:10.0.26.184<0>
11: queue1-dy-p4d24xlarge-2:10518:10518 [3] NCCL INFO Using network Socket
 3: queue1-dy-p4d24xlarge-1:10490:10660 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2 [2] -1/-1/-1->3->2 [3] -1/-1/-1->3->2 [4] 4/-1/-1->3->2 [5] 4/-1/-1->3->2 [6] -1/-1/-1->3->2 [7] -1/-1/-1->3->2
 3: queue1-dy-p4d24xlarge-1:10490:10660 [3] NCCL INFO Setting affinity for GPU 3 to fc,00000000,00fc0000
10: queue1-dy-p4d24xlarge-2:10517:10684 [2] NCCL INFO Trees [0] 11/-1/-1->10->9 [1] 11/-1/-1->10->9 [2] 11/-1/-1->10->9 [3] 11/-1/-1->10->9 [4] 11/-1/-1->10->9 [5] 11/-1/-1->10->9 [6] 11/-1/-1->10->9 [7] 11/-1/-1->10->9
10: queue1-dy-p4d24xlarge-2:10517:10684 [2] NCCL INFO Setting affinity for GPU 2 to 03,f0000000,0003f000
 4: queue1-dy-p4d24xlarge-1:10491:10662 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3 [2] 5/12/-1->4->-1 [3] 5/12/-1->4->-1 [4] 5/-1/-1->4->3 [5] 5/-1/-1->4->3 [6] 5/-1/-1->4->12 [7] 5/-1/-1->4->12
 4: queue1-dy-p4d24xlarge-1:10491:10662 [4] NCCL INFO Setting affinity for GPU 4 to 3f00,00000000,3f000000
 5: queue1-dy-p4d24xlarge-1:10492:10661 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4 [2] 6/-1/-1->5->4 [3] 6/-1/-1->5->4 [4] 6/-1/-1->5->4 [5] 6/-1/-1->5->4 [6] 6/-1/-1->5->4 [7] 6/-1/-1->5->4
 5: queue1-dy-p4d24xlarge-1:10492:10661 [5] NCCL INFO Setting affinity for GPU 5 to 0fc000,0000000f,c0000000
 6: queue1-dy-p4d24xlarge-1:10493:10658 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 7/-1/-1->6->5 [3] 7/-1/-1->6->5 [4] 7/-1/-1->6->5 [5] 7/-1/-1->6->5 [6] 7/-1/-1->6->5 [7] 7/-1/-1->6->5
 6: queue1-dy-p4d24xlarge-1:10493:10658 [6] NCCL INFO Setting affinity for GPU 6 to 03f00000,000003f0,00000000
 7: queue1-dy-p4d24xlarge-1:10494:10657 [7] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6 [2] 0/-1/-1->7->6 [3] 0/-1/-1->7->6 [4] -1/-1/-1->7->6 [5] -1/-1/-1->7->6 [6] 0/-1/-1->7->6 [7] 0/-1/-1->7->6
 7: queue1-dy-p4d24xlarge-1:10494:10657 [7] NCCL INFO Setting affinity for GPU 7 to fc000000,0000fc00,00000000
 0: queue1-dy-p4d24xlarge-1:10487:10655 [0] NCCL INFO Channel 00/08 :    0   7   6   5   4   3   2   1   8  15  14  13  12  11  10   9
 0: queue1-dy-p4d24xlarge-1:10487:10655 [0] NCCL INFO Channel 01/08 :    0   7   6   5   4   3   2   1   8  15  14  13  12  11  10   9
 2: queue1-dy-p4d24xlarge-1:10489:10656 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->1 [6] 3/-1/-1->2->1 [7] 3/-1/-1->2->1
 0: queue1-dy-p4d24xlarge-1:10487:10655 [0] NCCL INFO Channel 02/08 :    0   3   2   7   6   5  12   9   8  11  10  15  14  13   4   1
 0: queue1-dy-p4d24xlarge-1:10487:10655 [0] NCCL INFO Channel 03/08 :    0   3   2   7   6   5  12   9   8  11  10  15  14  13   4   1
 0: queue1-dy-p4d24xlarge-1:10487:10655 [0] NCCL INFO Channel 04/08 :    0   7   6   5   4   3   2   1   8  15  14  13  12  11  10   9
 1: queue1-dy-p4d24xlarge-1:10488:10659 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] 2/-1/-1->1->0 [6] 2/-1/-1->1->0 [7] 2/-1/-1->1->0
 1: queue1-dy-p4d24xlarge-1:10488:10659 [1] NCCL INFO Setting affinity for GPU 1 to 0fc00000,00000fc0
 2: queue1-dy-p4d24xlarge-1:10489:10656 [2] NCCL INFO Setting affinity for GPU 2 to 03,f0000000,0003f000
 0: queue1-dy-p4d24xlarge-1:10487:10655 [0] NCCL INFO Channel 05/08 :    0   7   6   5   4   3   2   1   8  15  14  13  12  11  10   9
 0: queue1-dy-p4d24xlarge-1:10487:10655 [0] NCCL INFO Channel 06/08 :    0   3   2   7   6   5  12   9   8  11  10  15  14  13   4   1
 0: queue1-dy-p4d24xlarge-1:10487:10655 [0] NCCL INFO Channel 07/08 :    0   3   2   7   6   5  12   9   8  11  10  15  14  13   4   1
 0: queue1-dy-p4d24xlarge-1:10487:10655 [0] NCCL INFO Trees [0] 1/8/-1->0->-1 [1] 1/8/-1->0->-1 [2] 1/-1/-1->0->7 [3] 1/-1/-1->0->7 [4] 1/-1/-1->0->8 [5] 1/-1/-1->0->8 [6] 1/-1/-1->0->7 [7] 1/-1/-1->0->7
 0: queue1-dy-p4d24xlarge-1:10487:10655 [0] NCCL INFO Setting affinity for GPU 0 to 3f0000,0000003f
15: queue1-dy-p4d24xlarge-2:10522:10680 [7] NCCL INFO Trees [0] -1/-1/-1->15->14 [1] -1/-1/-1->15->14 [2] 8/-1/-1->15->14 [3] 8/-1/-1->15->14 [4] -1/-1/-1->15->14 [5] -1/-1/-1->15->14 [6] 8/-1/-1->15->14 [7] 8/-1/-1->15->14
15: queue1-dy-p4d24xlarge-2:10522:10680 [7] NCCL INFO Setting affinity for GPU 7 to fc000000,0000fc00,00000000
 8: queue1-dy-p4d24xlarge-2:10515:10685 [0] NCCL INFO Trees [0] 9/-1/-1->8->0 [1] 9/-1/-1->8->0 [2] 9/-1/-1->8->15 [3] 9/-1/-1->8->15 [4] 9/0/-1->8->-1 [5] 9/0/-1->8->-1 [6] 9/-1/-1->8->15 [7] 9/-1/-1->8->15
 8: queue1-dy-p4d24xlarge-2:10515:10685 [0] NCCL INFO Setting affinity for GPU 0 to 3f0000,0000003f
 9: queue1-dy-p4d24xlarge-2:10516:10686 [1] NCCL INFO Trees [0] 10/-1/-1->9->8 [1] 10/-1/-1->9->8 [2] 10/-1/-1->9->8 [3] 10/-1/-1->9->8 [4] 10/-1/-1->9->8 [5] 10/-1/-1->9->8 [6] 10/-1/-1->9->8 [7] 10/-1/-1->9->8
 9: queue1-dy-p4d24xlarge-2:10516:10686 [1] NCCL INFO Setting affinity for GPU 1 to 0fc00000,00000fc0
11: queue1-dy-p4d24xlarge-2:10518:10687 [3] NCCL INFO Trees [0] 12/-1/-1->11->10 [1] 12/-1/-1->11->10 [2] -1/-1/-1->11->10 [3] -1/-1/-1->11->10 [4] 12/-1/-1->11->10 [5] 12/-1/-1->11->10 [6] -1/-1/-1->11->10 [7] -1/-1/-1->11->10
11: queue1-dy-p4d24xlarge-2:10518:10687 [3] NCCL INFO Setting affinity for GPU 3 to fc,00000000,00fc0000
12: queue1-dy-p4d24xlarge-2:10519:10682 [4] NCCL INFO Trees [0] 13/-1/-1->12->11 [1] 13/-1/-1->12->11 [2] 13/-1/-1->12->4 [3] 13/-1/-1->12->4 [4] 13/-1/-1->12->11 [5] 13/-1/-1->12->11 [6] 13/4/-1->12->-1 [7] 13/4/-1->12->-1
12: queue1-dy-p4d24xlarge-2:10519:10682 [4] NCCL INFO Setting affinity for GPU 4 to 3f00,00000000,3f000000
13: queue1-dy-p4d24xlarge-2:10520:10683 [5] NCCL INFO Trees [0] 14/-1/-1->13->12 [1] 14/-1/-1->13->12 [2] 14/-1/-1->13->12 [3] 14/-1/-1->13->12 [4] 14/-1/-1->13->12 [5] 14/-1/-1->13->12 [6] 14/-1/-1->13->12 [7] 14/-1/-1->13->12
13: queue1-dy-p4d24xlarge-2:10520:10683 [5] NCCL INFO Setting affinity for GPU 5 to 0fc000,0000000f,c0000000
14: queue1-dy-p4d24xlarge-2:10521:10681 [6] NCCL INFO Trees [0] 15/-1/-1->14->13 [1] 15/-1/-1->14->13 [2] 15/-1/-1->14->13 [3] 15/-1/-1->14->13 [4] 15/-1/-1->14->13 [5] 15/-1/-1->14->13 [6] 15/-1/-1->14->13 [7] 15/-1/-1->14->13
14: queue1-dy-p4d24xlarge-2:10521:10681 [6] NCCL INFO Setting affinity for GPU 6 to 03f00000,000003f0,00000000
10: queue1-dy-p4d24xlarge-2:10517:10684 [2] NCCL INFO Channel 02 : 10[201c0] -> 15[a01d0] via P2P/IPC/read
10: queue1-dy-p4d24xlarge-2:10517:10684 [2] NCCL INFO Channel 03 : 10[201c0] -> 15[a01d0] via P2P/IPC/read
 2: queue1-dy-p4d24xlarge-1:10489:10656 [2] NCCL INFO Channel 02 : 2[201c0] -> 7[a01d0] via P2P/IPC/read
 8: queue1-dy-p4d24xlarge-2:10515:10685 [0] NCCL INFO Channel 02 : 8[101c0] -> 11[201d0] via P2P/IPC/read
 0: queue1-dy-p4d24xlarge-1:10487:10655 [0] NCCL INFO Channel 02 : 0[101c0] -> 3[201d0] via P2P/IPC/read
10: queue1-dy-p4d24xlarge-2:10517:10684 [2] NCCL INFO Channel 06 : 10[201c0] -> 15[a01d0] via P2P/IPC/read
 8: queue1-dy-p4d24xlarge-2:10515:10685 [0] NCCL INFO Channel 03 : 8[101c0] -> 11[201d0] via P2P/IPC/read
 2: queue1-dy-p4d24xlarge-1:10489:10656 [2] NCCL INFO Channel 03 : 2[201c0] -> 7[a01d0] via P2P/IPC/read
 0: queue1-dy-p4d24xlarge-1:10487:10655 [0] NCCL INFO Channel 03 : 0[101c0] -> 3[201d0] via P2P/IPC/read
10: queue1-dy-p4d24xlarge-2:10517:10684 [2] NCCL INFO Channel 07 : 10[201c0] -> 15[a01d0] via P2P/IPC/read
 8: queue1-dy-p4d24xlarge-2:10515:10685 [0] NCCL INFO Channel 06 : 8[101c0] -> 11[201d0] via P2P/IPC/read
 2: queue1-dy-p4d24xlarge-1:10489:10656 [2] NCCL INFO Channel 06 : 2[201c0] -> 7[a01d0] via P2P/IPC/read
 0: queue1-dy-p4d24xlarge-1:10487:10655 [0] NCCL INFO Channel 06 : 0[101c0] -> 3[201d0] via P2P/IPC/read
 5: queue1-dy-p4d24xlarge-1:10492:10661 [5] NCCL INFO Channel 02 : 5[901d0] -> 12[901c0] [send] via NET/Socket/2
 2: queue1-dy-p4d24xlarge-1:10489:10656 [2] NCCL INFO Channel 07 : 2[201c0] -> 7[a01d0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-1:10491:10662 [4] NCCL INFO Channel 02 : 13[901d0] -> 4[901c0] [receive] via NET/Socket/2
 4: queue1-dy-p4d24xlarge-1:10491:10662 [4] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
 0: queue1-dy-p4d24xlarge-1:10487:10655 [0] NCCL INFO Channel 07 : 0[101c0] -> 3[201d0] via P2P/IPC/read
 1: queue1-dy-p4d24xlarge-1:10488:10659 [1] NCCL INFO Channel 00 : 1[101d0] -> 8[101c0] [send] via NET/Socket/0
13: queue1-dy-p4d24xlarge-2:10520:10683 [5] NCCL INFO Channel 02 : 13[901d0] -> 4[901c0] [send] via NET/Socket/2
 8: queue1-dy-p4d24xlarge-2:10515:10685 [0] NCCL INFO Channel 07 : 8[101c0] -> 11[201d0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-2:10519:10682 [4] NCCL INFO Channel 02 : 5[901d0] -> 12[901c0] [receive] via NET/Socket/2
12: queue1-dy-p4d24xlarge-2:10519:10682 [4] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
 9: queue1-dy-p4d24xlarge-2:10516:10686 [1] NCCL INFO Channel 00 : 9[101d0] -> 0[101c0] [send] via NET/Socket/0
 5: queue1-dy-p4d24xlarge-1:10492:10661 [5] NCCL INFO Channel 03 : 5[901d0] -> 12[901c0] [send] via NET/Socket/3
 4: queue1-dy-p4d24xlarge-1:10491:10662 [4] NCCL INFO Channel 03 : 13[901d0] -> 4[901c0] [receive] via NET/Socket/3
 4: queue1-dy-p4d24xlarge-1:10491:10662 [4] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
13: queue1-dy-p4d24xlarge-2:10520:10683 [5] NCCL INFO Channel 03 : 13[901d0] -> 4[901c0] [send] via NET/Socket/3
 1: queue1-dy-p4d24xlarge-1:10488:10659 [1] NCCL INFO Channel 01 : 1[101d0] -> 8[101c0] [send] via NET/Socket/1
12: queue1-dy-p4d24xlarge-2:10519:10682 [4] NCCL INFO Channel 03 : 5[901d0] -> 12[901c0] [receive] via NET/Socket/3
12: queue1-dy-p4d24xlarge-2:10519:10682 [4] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
 9: queue1-dy-p4d24xlarge-2:10516:10686 [1] NCCL INFO Channel 01 : 9[101d0] -> 0[101c0] [send] via NET/Socket/1
 6: queue1-dy-p4d24xlarge-1:10493:10658 [6] NCCL INFO Channel 00 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
14: queue1-dy-p4d24xlarge-2:10521:10681 [6] NCCL INFO Channel 00 : 14[a01c0] -> 13[901d0] via P2P/IPC/read
 5: queue1-dy-p4d24xlarge-1:10492:10661 [5] NCCL INFO Channel 06 : 5[901d0] -> 12[901c0] [send] via NET/Socket/2
13: queue1-dy-p4d24xlarge-2:10520:10683 [5] NCCL INFO Channel 06 : 13[901d0] -> 4[901c0] [send] via NET/Socket/2
14: queue1-dy-p4d24xlarge-2:10521:10681 [6] NCCL INFO Channel 01 : 14[a01c0] -> 13[901d0] via P2P/IPC/read
 6: queue1-dy-p4d24xlarge-1:10493:10658 [6] NCCL INFO Channel 01 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-1:10491:10662 [4] NCCL INFO Channel 06 : 13[901d0] -> 4[901c0] [receive] via NET/Socket/2
 4: queue1-dy-p4d24xlarge-1:10491:10662 [4] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
 1: queue1-dy-p4d24xlarge-1:10488:10659 [1] NCCL INFO Channel 04 : 1[101d0] -> 8[101c0] [send] via NET/Socket/0
 6: queue1-dy-p4d24xlarge-1:10493:10658 [6] NCCL INFO Channel 02 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
14: queue1-dy-p4d24xlarge-2:10521:10681 [6] NCCL INFO Channel 02 : 14[a01c0] -> 13[901d0] via P2P/IPC/read
 6: queue1-dy-p4d24xlarge-1:10493:10658 [6] NCCL INFO Channel 03 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-2:10519:10682 [4] NCCL INFO Channel 06 : 5[901d0] -> 12[901c0] [receive] via NET/Socket/2
12: queue1-dy-p4d24xlarge-2:10519:10682 [4] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
14: queue1-dy-p4d24xlarge-2:10521:10681 [6] NCCL INFO Channel 03 : 14[a01c0] -> 13[901d0] via P2P/IPC/read
 9: queue1-dy-p4d24xlarge-2:10516:10686 [1] NCCL INFO Channel 04 : 9[101d0] -> 0[101c0] [send] via NET/Socket/0
 3: queue1-dy-p4d24xlarge-1:10490:10660 [3] NCCL INFO Channel 00 : 3[201d0] -> 2[201c0] via P2P/IPC/read
 6: queue1-dy-p4d24xlarge-1:10493:10658 [6] NCCL INFO Channel 04 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
11: queue1-dy-p4d24xlarge-2:10518:10687 [3] NCCL INFO Channel 00 : 11[201d0] -> 10[201c0] via P2P/IPC/read
14: queue1-dy-p4d24xlarge-2:10521:10681 [6] NCCL INFO Channel 04 : 14[a01c0] -> 13[901d0] via P2P/IPC/read
 0: queue1-dy-p4d24xlarge-1:10487:10655 [0] NCCL INFO Channel 00 : 9[101d0] -> 0[101c0] [receive] via NET/Socket/0
 8: queue1-dy-p4d24xlarge-2:10515:10685 [0] NCCL INFO Channel 00 : 1[101d0] -> 8[101c0] [receive] via NET/Socket/0
 3: queue1-dy-p4d24xlarge-1:10490:10660 [3] NCCL INFO Channel 01 : 3[201d0] -> 2[201c0] via P2P/IPC/read
 0: queue1-dy-p4d24xlarge-1:10487:10655 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
 8: queue1-dy-p4d24xlarge-2:10515:10685 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
 6: queue1-dy-p4d24xlarge-1:10493:10658 [6] NCCL INFO Channel 05 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
11: queue1-dy-p4d24xlarge-2:10518:10687 [3] NCCL INFO Channel 01 : 11[201d0] -> 10[201c0] via P2P/IPC/read
14: queue1-dy-p4d24xlarge-2:10521:10681 [6] NCCL INFO Channel 05 : 14[a01c0] -> 13[901d0] via P2P/IPC/read
 3: queue1-dy-p4d24xlarge-1:10490:10660 [3] NCCL INFO Channel 02 : 3[201d0] -> 2[201c0] via P2P/IPC/read
 6: queue1-dy-p4d24xlarge-1:10493:10658 [6] NCCL INFO Channel 06 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
11: queue1-dy-p4d24xlarge-2:10518:10687 [3] NCCL INFO Channel 02 : 11[201d0] -> 10[201c0] via P2P/IPC/read
14: queue1-dy-p4d24xlarge-2:10521:10681 [6] NCCL INFO Channel 06 : 14[a01c0] -> 13[901d0] via P2P/IPC/read
10: queue1-dy-p4d24xlarge-2:10517:10684 [2] NCCL INFO Channel 00 : 10[201c0] -> 9[101d0] via P2P/IPC/read
 3: queue1-dy-p4d24xlarge-1:10490:10660 [3] NCCL INFO Channel 03 : 3[201d0] -> 2[201c0] via P2P/IPC/read
 6: queue1-dy-p4d24xlarge-1:10493:10658 [6] NCCL INFO Channel 07 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
11: queue1-dy-p4d24xlarge-2:10518:10687 [3] NCCL INFO Channel 03 : 11[201d0] -> 10[201c0] via P2P/IPC/read
14: queue1-dy-p4d24xlarge-2:10521:10681 [6] NCCL INFO Channel 07 : 14[a01c0] -> 13[901d0] via P2P/IPC/read
10: queue1-dy-p4d24xlarge-2:10517:10684 [2] NCCL INFO Channel 01 : 10[201c0] -> 9[101d0] via P2P/IPC/read
 2: queue1-dy-p4d24xlarge-1:10489:10656 [2] NCCL INFO Channel 00 : 2[201c0] -> 1[101d0] via P2P/IPC/read
11: queue1-dy-p4d24xlarge-2:10518:10687 [3] NCCL INFO Channel 04 : 11[201d0] -> 10[201c0] via P2P/IPC/read
 5: queue1-dy-p4d24xlarge-1:10492:10661 [5] NCCL INFO Channel 07 : 5[901d0] -> 12[901c0] [send] via NET/Socket/3
13: queue1-dy-p4d24xlarge-2:10520:10683 [5] NCCL INFO Channel 07 : 13[901d0] -> 4[901c0] [send] via NET/Socket/3
10: queue1-dy-p4d24xlarge-2:10517:10684 [2] NCCL INFO Channel 04 : 10[201c0] -> 9[101d0] via P2P/IPC/read
 3: queue1-dy-p4d24xlarge-1:10490:10660 [3] NCCL INFO Channel 04 : 3[201d0] -> 2[201c0] via P2P/IPC/read
11: queue1-dy-p4d24xlarge-2:10518:10687 [3] NCCL INFO Channel 05 : 11[201d0] -> 10[201c0] via P2P/IPC/read
10: queue1-dy-p4d24xlarge-2:10517:10684 [2] NCCL INFO Channel 05 : 10[201c0] -> 9[101d0] via P2P/IPC/read
11: queue1-dy-p4d24xlarge-2:10518:10687 [3] NCCL INFO Channel 06 : 11[201d0] -> 10[201c0] via P2P/IPC/read
 2: queue1-dy-p4d24xlarge-1:10489:10656 [2] NCCL INFO Channel 01 : 2[201c0] -> 1[101d0] via P2P/IPC/read
 3: queue1-dy-p4d24xlarge-1:10490:10660 [3] NCCL INFO Channel 05 : 3[201d0] -> 2[201c0] via P2P/IPC/read
11: queue1-dy-p4d24xlarge-2:10518:10687 [3] NCCL INFO Channel 07 : 11[201d0] -> 10[201c0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-1:10491:10662 [4] NCCL INFO Channel 07 : 13[901d0] -> 4[901c0] [receive] via NET/Socket/3
 4: queue1-dy-p4d24xlarge-1:10491:10662 [4] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
 2: queue1-dy-p4d24xlarge-1:10489:10656 [2] NCCL INFO Channel 04 : 2[201c0] -> 1[101d0] via P2P/IPC/read
 1: queue1-dy-p4d24xlarge-1:10488:10659 [1] NCCL INFO Channel 05 : 1[101d0] -> 8[101c0] [send] via NET/Socket/1
 3: queue1-dy-p4d24xlarge-1:10490:10660 [3] NCCL INFO Channel 06 : 3[201d0] -> 2[201c0] via P2P/IPC/read
 2: queue1-dy-p4d24xlarge-1:10489:10656 [2] NCCL INFO Channel 05 : 2[201c0] -> 1[101d0] via P2P/IPC/read
 3: queue1-dy-p4d24xlarge-1:10490:10660 [3] NCCL INFO Channel 07 : 3[201d0] -> 2[201c0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-2:10519:10682 [4] NCCL INFO Channel 07 : 5[901d0] -> 12[901c0] [receive] via NET/Socket/3
12: queue1-dy-p4d24xlarge-2:10519:10682 [4] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
 9: queue1-dy-p4d24xlarge-2:10516:10686 [1] NCCL INFO Channel 05 : 9[101d0] -> 0[101c0] [send] via NET/Socket/1
 0: queue1-dy-p4d24xlarge-1:10487:10655 [0] NCCL INFO Channel 01 : 9[101d0] -> 0[101c0] [receive] via NET/Socket/1
 0: queue1-dy-p4d24xlarge-1:10487:10655 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
 8: queue1-dy-p4d24xlarge-2:10515:10685 [0] NCCL INFO Channel 01 : 1[101d0] -> 8[101c0] [receive] via NET/Socket/1
 8: queue1-dy-p4d24xlarge-2:10515:10685 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
 0: queue1-dy-p4d24xlarge-1:10487:10655 [0] NCCL INFO Channel 04 : 9[101d0] -> 0[101c0] [receive] via NET/Socket/0
 0: queue1-dy-p4d24xlarge-1:10487:10655 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
 8: queue1-dy-p4d24xlarge-2:10515:10685 [0] NCCL INFO Channel 04 : 1[101d0] -> 8[101c0] [receive] via NET/Socket/0
 8: queue1-dy-p4d24xlarge-2:10515:10685 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
 0: queue1-dy-p4d24xlarge-1:10487:10655 [0] NCCL INFO Channel 05 : 9[101d0] -> 0[101c0] [receive] via NET/Socket/1
 0: queue1-dy-p4d24xlarge-1:10487:10655 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
 8: queue1-dy-p4d24xlarge-2:10515:10685 [0] NCCL INFO Channel 05 : 1[101d0] -> 8[101c0] [receive] via NET/Socket/1
 8: queue1-dy-p4d24xlarge-2:10515:10685 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
 0: queue1-dy-p4d24xlarge-1:10487:10655 [0] NCCL INFO Channel 00 : 0[101c0] -> 7[a01d0] via P2P/IPC/read
 8: queue1-dy-p4d24xlarge-2:10515:10685 [0] NCCL INFO Channel 00 : 8[101c0] -> 15[a01d0] via P2P/IPC/read
 0: queue1-dy-p4d24xlarge-1:10487:10655 [0] NCCL INFO Channel 01 : 0[101c0] -> 7[a01d0] via P2P/IPC/read
 8: queue1-dy-p4d24xlarge-2:10515:10685 [0] NCCL INFO Channel 01 : 8[101c0] -> 15[a01d0] via P2P/IPC/read
 0: queue1-dy-p4d24xlarge-1:10487:10655 [0] NCCL INFO Channel 04 : 0[101c0] -> 7[a01d0] via P2P/IPC/read
 8: queue1-dy-p4d24xlarge-2:10515:10685 [0] NCCL INFO Channel 04 : 8[101c0] -> 15[a01d0] via P2P/IPC/read
 0: queue1-dy-p4d24xlarge-1:10487:10655 [0] NCCL INFO Channel 05 : 0[101c0] -> 7[a01d0] via P2P/IPC/read
 8: queue1-dy-p4d24xlarge-2:10515:10685 [0] NCCL INFO Channel 05 : 8[101c0] -> 15[a01d0] via P2P/IPC/read
 7: queue1-dy-p4d24xlarge-1:10494:10657 [7] NCCL INFO Channel 00 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
15: queue1-dy-p4d24xlarge-2:10522:10680 [7] NCCL INFO Channel 00 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read
15: queue1-dy-p4d24xlarge-2:10522:10680 [7] NCCL INFO Channel 01 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read
 7: queue1-dy-p4d24xlarge-1:10494:10657 [7] NCCL INFO Channel 01 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
15: queue1-dy-p4d24xlarge-2:10522:10680 [7] NCCL INFO Channel 02 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read
 7: queue1-dy-p4d24xlarge-1:10494:10657 [7] NCCL INFO Channel 02 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
15: queue1-dy-p4d24xlarge-2:10522:10680 [7] NCCL INFO Channel 03 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read
 7: queue1-dy-p4d24xlarge-1:10494:10657 [7] NCCL INFO Channel 03 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
15: queue1-dy-p4d24xlarge-2:10522:10680 [7] NCCL INFO Channel 04 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read
 7: queue1-dy-p4d24xlarge-1:10494:10657 [7] NCCL INFO Channel 04 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
15: queue1-dy-p4d24xlarge-2:10522:10680 [7] NCCL INFO Channel 05 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read
 7: queue1-dy-p4d24xlarge-1:10494:10657 [7] NCCL INFO Channel 05 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
15: queue1-dy-p4d24xlarge-2:10522:10680 [7] NCCL INFO Channel 06 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read
 7: queue1-dy-p4d24xlarge-1:10494:10657 [7] NCCL INFO Channel 06 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
15: queue1-dy-p4d24xlarge-2:10522:10680 [7] NCCL INFO Channel 07 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read
 7: queue1-dy-p4d24xlarge-1:10494:10657 [7] NCCL INFO Channel 07 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
15: queue1-dy-p4d24xlarge-2:10522:10680 [7] NCCL INFO Connected all rings
 7: queue1-dy-p4d24xlarge-1:10494:10657 [7] NCCL INFO Connected all rings
 4: queue1-dy-p4d24xlarge-1:10491:10662 [4] NCCL INFO Channel 02 : 4[901c0] -> 1[101d0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-1:10491:10662 [4] NCCL INFO Channel 03 : 4[901c0] -> 1[101d0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-1:10491:10662 [4] NCCL INFO Channel 06 : 4[901c0] -> 1[101d0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-1:10491:10662 [4] NCCL INFO Channel 07 : 4[901c0] -> 1[101d0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-2:10519:10682 [4] NCCL INFO Channel 02 : 12[901c0] -> 9[101d0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-2:10519:10682 [4] NCCL INFO Channel 03 : 12[901c0] -> 9[101d0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-2:10519:10682 [4] NCCL INFO Channel 06 : 12[901c0] -> 9[101d0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-2:10519:10682 [4] NCCL INFO Channel 07 : 12[901c0] -> 9[101d0] via P2P/IPC/read
13: queue1-dy-p4d24xlarge-2:10520:10683 [5] NCCL INFO Channel 00 : 13[901d0] -> 12[901c0] via P2P/IPC/read
13: queue1-dy-p4d24xlarge-2:10520:10683 [5] NCCL INFO Channel 01 : 13[901d0] -> 12[901c0] via P2P/IPC/read
13: queue1-dy-p4d24xlarge-2:10520:10683 [5] NCCL INFO Channel 04 : 13[901d0] -> 12[901c0] via P2P/IPC/read
13: queue1-dy-p4d24xlarge-2:10520:10683 [5] NCCL INFO Channel 05 : 13[901d0] -> 12[901c0] via P2P/IPC/read
 5: queue1-dy-p4d24xlarge-1:10492:10661 [5] NCCL INFO Channel 00 : 5[901d0] -> 4[901c0] via P2P/IPC/read
 5: queue1-dy-p4d24xlarge-1:10492:10661 [5] NCCL INFO Channel 01 : 5[901d0] -> 4[901c0] via P2P/IPC/read
 5: queue1-dy-p4d24xlarge-1:10492:10661 [5] NCCL INFO Channel 04 : 5[901d0] -> 4[901c0] via P2P/IPC/read
 5: queue1-dy-p4d24xlarge-1:10492:10661 [5] NCCL INFO Channel 05 : 5[901d0] -> 4[901c0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-1:10491:10662 [4] NCCL INFO Channel 00 : 4[901c0] -> 3[201d0] via P2P/IPC/read
 1: queue1-dy-p4d24xlarge-1:10488:10659 [1] NCCL INFO Channel 02 : 1[101d0] -> 0[101c0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-1:10491:10662 [4] NCCL INFO Channel 01 : 4[901c0] -> 3[201d0] via P2P/IPC/read
 9: queue1-dy-p4d24xlarge-2:10516:10686 [1] NCCL INFO Channel 02 : 9[101d0] -> 8[101c0] via P2P/IPC/read
 1: queue1-dy-p4d24xlarge-1:10488:10659 [1] NCCL INFO Channel 03 : 1[101d0] -> 0[101c0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-2:10519:10682 [4] NCCL INFO Channel 00 : 12[901c0] -> 11[201d0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-1:10491:10662 [4] NCCL INFO Channel 04 : 4[901c0] -> 3[201d0] via P2P/IPC/read
 9: queue1-dy-p4d24xlarge-2:10516:10686 [1] NCCL INFO Channel 03 : 9[101d0] -> 8[101c0] via P2P/IPC/read
 1: queue1-dy-p4d24xlarge-1:10488:10659 [1] NCCL INFO Channel 06 : 1[101d0] -> 0[101c0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-2:10519:10682 [4] NCCL INFO Channel 01 : 12[901c0] -> 11[201d0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-1:10491:10662 [4] NCCL INFO Channel 05 : 4[901c0] -> 3[201d0] via P2P/IPC/read
 9: queue1-dy-p4d24xlarge-2:10516:10686 [1] NCCL INFO Channel 06 : 9[101d0] -> 8[101c0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-2:10519:10682 [4] NCCL INFO Channel 04 : 12[901c0] -> 11[201d0] via P2P/IPC/read
 9: queue1-dy-p4d24xlarge-2:10516:10686 [1] NCCL INFO Channel 07 : 9[101d0] -> 8[101c0] via P2P/IPC/read
 1: queue1-dy-p4d24xlarge-1:10488:10659 [1] NCCL INFO Channel 07 : 1[101d0] -> 0[101c0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-2:10519:10682 [4] NCCL INFO Channel 05 : 12[901c0] -> 11[201d0] via P2P/IPC/read
14: queue1-dy-p4d24xlarge-2:10521:10681 [6] NCCL INFO Connected all rings
 0: queue1-dy-p4d24xlarge-1:10487:10655 [0] NCCL INFO Connected all rings
 0: queue1-dy-p4d24xlarge-1:10487:10655 [0] NCCL INFO Channel 00 : 0[101c0] -> 1[101d0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-2:10519:10682 [4] NCCL INFO Connected all rings
 0: queue1-dy-p4d24xlarge-1:10487:10655 [0] NCCL INFO Channel 01 : 0[101c0] -> 1[101d0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-1:10491:10662 [4] NCCL INFO Connected all rings
 8: queue1-dy-p4d24xlarge-2:10515:10685 [0] NCCL INFO Connected all rings
 6: queue1-dy-p4d24xlarge-1:10493:10658 [6] NCCL INFO Connected all rings
 1: queue1-dy-p4d24xlarge-1:10488:10659 [1] NCCL INFO Connected all rings
 0: queue1-dy-p4d24xlarge-1:10487:10655 [0] NCCL INFO Channel 02 : 0[101c0] -> 1[101d0] via P2P/IPC/read
 8: queue1-dy-p4d24xlarge-2:10515:10685 [0] NCCL INFO Channel 00 : 8[101c0] -> 9[101d0] via P2P/IPC/read
10: queue1-dy-p4d24xlarge-2:10517:10684 [2] NCCL INFO Connected all rings
13: queue1-dy-p4d24xlarge-2:10520:10683 [5] NCCL INFO Connected all rings
 0: queue1-dy-p4d24xlarge-1:10487:10655 [0] NCCL INFO Channel 03 : 0[101c0] -> 1[101d0] via P2P/IPC/read
11: queue1-dy-p4d24xlarge-2:10518:10687 [3] NCCL INFO Connected all rings
 8: queue1-dy-p4d24xlarge-2:10515:10685 [0] NCCL INFO Channel 01 : 8[101c0] -> 9[101d0] via P2P/IPC/read
 5: queue1-dy-p4d24xlarge-1:10492:10661 [5] NCCL INFO Connected all rings
 3: queue1-dy-p4d24xlarge-1:10490:10660 [3] NCCL INFO Connected all rings
 9: queue1-dy-p4d24xlarge-2:10516:10686 [1] NCCL INFO Connected all rings
 8: queue1-dy-p4d24xlarge-2:10515:10685 [0] NCCL INFO Channel 02 : 8[101c0] -> 9[101d0] via P2P/IPC/read
 2: queue1-dy-p4d24xlarge-1:10489:10656 [2] NCCL INFO Connected all rings
 0: queue1-dy-p4d24xlarge-1:10487:10655 [0] NCCL INFO Channel 04 : 0[101c0] -> 1[101d0] via P2P/IPC/read
 8: queue1-dy-p4d24xlarge-2:10515:10685 [0] NCCL INFO Channel 03 : 8[101c0] -> 9[101d0] via P2P/IPC/read
 0: queue1-dy-p4d24xlarge-1:10487:10655 [0] NCCL INFO Channel 05 : 0[101c0] -> 1[101d0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-2:10519:10682 [4] NCCL INFO Channel 00 : 12[901c0] -> 13[901d0] via P2P/IPC/read
14: queue1-dy-p4d24xlarge-2:10521:10681 [6] NCCL INFO Channel 00 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read
 8: queue1-dy-p4d24xlarge-2:10515:10685 [0] NCCL INFO Channel 04 : 8[101c0] -> 9[101d0] via P2P/IPC/read
 0: queue1-dy-p4d24xlarge-1:10487:10655 [0] NCCL INFO Channel 06 : 0[101c0] -> 1[101d0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-1:10491:10662 [4] NCCL INFO Channel 00 : 4[901c0] -> 5[901d0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-2:10519:10682 [4] NCCL INFO Channel 01 : 12[901c0] -> 13[901d0] via P2P/IPC/read
14: queue1-dy-p4d24xlarge-2:10521:10681 [6] NCCL INFO Channel 01 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read
 8: queue1-dy-p4d24xlarge-2:10515:10685 [0] NCCL INFO Channel 05 : 8[101c0] -> 9[101d0] via P2P/IPC/read
 0: queue1-dy-p4d24xlarge-1:10487:10655 [0] NCCL INFO Channel 07 : 0[101c0] -> 1[101d0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-2:10519:10682 [4] NCCL INFO Channel 02 : 12[901c0] -> 13[901d0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-1:10491:10662 [4] NCCL INFO Channel 01 : 4[901c0] -> 5[901d0] via P2P/IPC/read
14: queue1-dy-p4d24xlarge-2:10521:10681 [6] NCCL INFO Channel 02 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read
 8: queue1-dy-p4d24xlarge-2:10515:10685 [0] NCCL INFO Channel 06 : 8[101c0] -> 9[101d0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-1:10491:10662 [4] NCCL INFO Channel 02 : 4[901c0] -> 5[901d0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-2:10519:10682 [4] NCCL INFO Channel 03 : 12[901c0] -> 13[901d0] via P2P/IPC/read
14: queue1-dy-p4d24xlarge-2:10521:10681 [6] NCCL INFO Channel 03 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read
 8: queue1-dy-p4d24xlarge-2:10515:10685 [0] NCCL INFO Channel 07 : 8[101c0] -> 9[101d0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-1:10491:10662 [4] NCCL INFO Channel 03 : 4[901c0] -> 5[901d0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-2:10519:10682 [4] NCCL INFO Channel 04 : 12[901c0] -> 13[901d0] via P2P/IPC/read
14: queue1-dy-p4d24xlarge-2:10521:10681 [6] NCCL INFO Channel 04 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-1:10491:10662 [4] NCCL INFO Channel 04 : 4[901c0] -> 5[901d0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-2:10519:10682 [4] NCCL INFO Channel 05 : 12[901c0] -> 13[901d0] via P2P/IPC/read
 6: queue1-dy-p4d24xlarge-1:10493:10658 [6] NCCL INFO Channel 00 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
 1: queue1-dy-p4d24xlarge-1:10488:10659 [1] NCCL INFO Channel 00 : 1[101d0] -> 2[201c0] via P2P/IPC/read
14: queue1-dy-p4d24xlarge-2:10521:10681 [6] NCCL INFO Channel 05 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read
10: queue1-dy-p4d24xlarge-2:10517:10684 [2] NCCL INFO Channel 00 : 10[201c0] -> 11[201d0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-1:10491:10662 [4] NCCL INFO Channel 05 : 4[901c0] -> 5[901d0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-2:10519:10682 [4] NCCL INFO Channel 06 : 12[901c0] -> 13[901d0] via P2P/IPC/read
13: queue1-dy-p4d24xlarge-2:10520:10683 [5] NCCL INFO Channel 00 : 13[901d0] -> 14[a01c0] via P2P/IPC/read
 6: queue1-dy-p4d24xlarge-1:10493:10658 [6] NCCL INFO Channel 01 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
11: queue1-dy-p4d24xlarge-2:10518:10687 [3] NCCL INFO Channel 00 : 11[201d0] -> 12[901c0] via P2P/IPC/read
 1: queue1-dy-p4d24xlarge-1:10488:10659 [1] NCCL INFO Channel 01 : 1[101d0] -> 2[201c0] via P2P/IPC/read
14: queue1-dy-p4d24xlarge-2:10521:10681 [6] NCCL INFO Channel 06 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read
10: queue1-dy-p4d24xlarge-2:10517:10684 [2] NCCL INFO Channel 01 : 10[201c0] -> 11[201d0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-1:10491:10662 [4] NCCL INFO Channel 06 : 4[901c0] -> 5[901d0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-2:10519:10682 [4] NCCL INFO Channel 07 : 12[901c0] -> 13[901d0] via P2P/IPC/read
 5: queue1-dy-p4d24xlarge-1:10492:10661 [5] NCCL INFO Channel 00 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
13: queue1-dy-p4d24xlarge-2:10520:10683 [5] NCCL INFO Channel 01 : 13[901d0] -> 14[a01c0] via P2P/IPC/read
 3: queue1-dy-p4d24xlarge-1:10490:10660 [3] NCCL INFO Channel 00 : 3[201d0] -> 4[901c0] via P2P/IPC/read
 9: queue1-dy-p4d24xlarge-2:10516:10686 [1] NCCL INFO Channel 00 : 9[101d0] -> 10[201c0] via P2P/IPC/read
11: queue1-dy-p4d24xlarge-2:10518:10687 [3] NCCL INFO Channel 01 : 11[201d0] -> 12[901c0] via P2P/IPC/read
 6: queue1-dy-p4d24xlarge-1:10493:10658 [6] NCCL INFO Channel 02 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
 1: queue1-dy-p4d24xlarge-1:10488:10659 [1] NCCL INFO Channel 02 : 1[101d0] -> 2[201c0] via P2P/IPC/read
14: queue1-dy-p4d24xlarge-2:10521:10681 [6] NCCL INFO Channel 07 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read
 2: queue1-dy-p4d24xlarge-1:10489:10656 [2] NCCL INFO Channel 00 : 2[201c0] -> 3[201d0] via P2P/IPC/read
10: queue1-dy-p4d24xlarge-2:10517:10684 [2] NCCL INFO Channel 02 : 10[201c0] -> 11[201d0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-1:10491:10662 [4] NCCL INFO Channel 07 : 4[901c0] -> 5[901d0] via P2P/IPC/read
13: queue1-dy-p4d24xlarge-2:10520:10683 [5] NCCL INFO Channel 02 : 13[901d0] -> 14[a01c0] via P2P/IPC/read
 5: queue1-dy-p4d24xlarge-1:10492:10661 [5] NCCL INFO Channel 01 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
 9: queue1-dy-p4d24xlarge-2:10516:10686 [1] NCCL INFO Channel 01 : 9[101d0] -> 10[201c0] via P2P/IPC/read
 3: queue1-dy-p4d24xlarge-1:10490:10660 [3] NCCL INFO Channel 01 : 3[201d0] -> 4[901c0] via P2P/IPC/read
11: queue1-dy-p4d24xlarge-2:10518:10687 [3] NCCL INFO Channel 04 : 11[201d0] -> 12[901c0] via P2P/IPC/read
 1: queue1-dy-p4d24xlarge-1:10488:10659 [1] NCCL INFO Channel 03 : 1[101d0] -> 2[201c0] via P2P/IPC/read
 2: queue1-dy-p4d24xlarge-1:10489:10656 [2] NCCL INFO Channel 01 : 2[201c0] -> 3[201d0] via P2P/IPC/read
 6: queue1-dy-p4d24xlarge-1:10493:10658 [6] NCCL INFO Channel 03 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
10: queue1-dy-p4d24xlarge-2:10517:10684 [2] NCCL INFO Channel 03 : 10[201c0] -> 11[201d0] via P2P/IPC/read
 5: queue1-dy-p4d24xlarge-1:10492:10661 [5] NCCL INFO Channel 02 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
 3: queue1-dy-p4d24xlarge-1:10490:10660 [3] NCCL INFO Channel 04 : 3[201d0] -> 4[901c0] via P2P/IPC/read
 1: queue1-dy-p4d24xlarge-1:10488:10659 [1] NCCL INFO Channel 04 : 1[101d0] -> 2[201c0] via P2P/IPC/read
 2: queue1-dy-p4d24xlarge-1:10489:10656 [2] NCCL INFO Channel 02 : 2[201c0] -> 3[201d0] via P2P/IPC/read
 6: queue1-dy-p4d24xlarge-1:10493:10658 [6] NCCL INFO Channel 04 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
13: queue1-dy-p4d24xlarge-2:10520:10683 [5] NCCL INFO Channel 03 : 13[901d0] -> 14[a01c0] via P2P/IPC/read
 9: queue1-dy-p4d24xlarge-2:10516:10686 [1] NCCL INFO Channel 02 : 9[101d0] -> 10[201c0] via P2P/IPC/read
 5: queue1-dy-p4d24xlarge-1:10492:10661 [5] NCCL INFO Channel 03 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
11: queue1-dy-p4d24xlarge-2:10518:10687 [3] NCCL INFO Channel 05 : 11[201d0] -> 12[901c0] via P2P/IPC/read
 3: queue1-dy-p4d24xlarge-1:10490:10660 [3] NCCL INFO Channel 05 : 3[201d0] -> 4[901c0] via P2P/IPC/read
 1: queue1-dy-p4d24xlarge-1:10488:10659 [1] NCCL INFO Channel 05 : 1[101d0] -> 2[201c0] via P2P/IPC/read
 2: queue1-dy-p4d24xlarge-1:10489:10656 [2] NCCL INFO Channel 03 : 2[201c0] -> 3[201d0] via P2P/IPC/read
 6: queue1-dy-p4d24xlarge-1:10493:10658 [6] NCCL INFO Channel 05 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
10: queue1-dy-p4d24xlarge-2:10517:10684 [2] NCCL INFO Channel 04 : 10[201c0] -> 11[201d0] via P2P/IPC/read
 5: queue1-dy-p4d24xlarge-1:10492:10661 [5] NCCL INFO Channel 04 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
 9: queue1-dy-p4d24xlarge-2:10516:10686 [1] NCCL INFO Channel 03 : 9[101d0] -> 10[201c0] via P2P/IPC/read
 1: queue1-dy-p4d24xlarge-1:10488:10659 [1] NCCL INFO Channel 06 : 1[101d0] -> 2[201c0] via P2P/IPC/read
13: queue1-dy-p4d24xlarge-2:10520:10683 [5] NCCL INFO Channel 04 : 13[901d0] -> 14[a01c0] via P2P/IPC/read
 2: queue1-dy-p4d24xlarge-1:10489:10656 [2] NCCL INFO Channel 04 : 2[201c0] -> 3[201d0] via P2P/IPC/read
 6: queue1-dy-p4d24xlarge-1:10493:10658 [6] NCCL INFO Channel 06 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
 5: queue1-dy-p4d24xlarge-1:10492:10661 [5] NCCL INFO Channel 05 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
 1: queue1-dy-p4d24xlarge-1:10488:10659 [1] NCCL INFO Channel 07 : 1[101d0] -> 2[201c0] via P2P/IPC/read
10: queue1-dy-p4d24xlarge-2:10517:10684 [2] NCCL INFO Channel 05 : 10[201c0] -> 11[201d0] via P2P/IPC/read
 2: queue1-dy-p4d24xlarge-1:10489:10656 [2] NCCL INFO Channel 05 : 2[201c0] -> 3[201d0] via P2P/IPC/read
 6: queue1-dy-p4d24xlarge-1:10493:10658 [6] NCCL INFO Channel 07 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
 5: queue1-dy-p4d24xlarge-1:10492:10661 [5] NCCL INFO Channel 06 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
 9: queue1-dy-p4d24xlarge-2:10516:10686 [1] NCCL INFO Channel 04 : 9[101d0] -> 10[201c0] via P2P/IPC/read
13: queue1-dy-p4d24xlarge-2:10520:10683 [5] NCCL INFO Channel 05 : 13[901d0] -> 14[a01c0] via P2P/IPC/read
 2: queue1-dy-p4d24xlarge-1:10489:10656 [2] NCCL INFO Channel 06 : 2[201c0] -> 3[201d0] via P2P/IPC/read
10: queue1-dy-p4d24xlarge-2:10517:10684 [2] NCCL INFO Channel 06 : 10[201c0] -> 11[201d0] via P2P/IPC/read
 9: queue1-dy-p4d24xlarge-2:10516:10686 [1] NCCL INFO Channel 05 : 9[101d0] -> 10[201c0] via P2P/IPC/read
13: queue1-dy-p4d24xlarge-2:10520:10683 [5] NCCL INFO Channel 06 : 13[901d0] -> 14[a01c0] via P2P/IPC/read
 5: queue1-dy-p4d24xlarge-1:10492:10661 [5] NCCL INFO Channel 07 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
10: queue1-dy-p4d24xlarge-2:10517:10684 [2] NCCL INFO Channel 07 : 10[201c0] -> 11[201d0] via P2P/IPC/read
 9: queue1-dy-p4d24xlarge-2:10516:10686 [1] NCCL INFO Channel 06 : 9[101d0] -> 10[201c0] via P2P/IPC/read
 2: queue1-dy-p4d24xlarge-1:10489:10656 [2] NCCL INFO Channel 07 : 2[201c0] -> 3[201d0] via P2P/IPC/read
13: queue1-dy-p4d24xlarge-2:10520:10683 [5] NCCL INFO Channel 07 : 13[901d0] -> 14[a01c0] via P2P/IPC/read
 9: queue1-dy-p4d24xlarge-2:10516:10686 [1] NCCL INFO Channel 07 : 9[101d0] -> 10[201c0] via P2P/IPC/read
 0: queue1-dy-p4d24xlarge-1:10487:10655 [0] NCCL INFO Channel 02 : 0[101c0] -> 7[a01d0] via P2P/IPC/read
 0: queue1-dy-p4d24xlarge-1:10487:10655 [0] NCCL INFO Channel 03 : 0[101c0] -> 7[a01d0] via P2P/IPC/read
 8: queue1-dy-p4d24xlarge-2:10515:10685 [0] NCCL INFO Channel 02 : 8[101c0] -> 15[a01d0] via P2P/IPC/read
 0: queue1-dy-p4d24xlarge-1:10487:10655 [0] NCCL INFO Channel 06 : 0[101c0] -> 7[a01d0] via P2P/IPC/read
11: queue1-dy-p4d24xlarge-2:10518:10687 [3] NCCL INFO Connected all trees
11: queue1-dy-p4d24xlarge-2:10518:10687 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
11: queue1-dy-p4d24xlarge-2:10518:10687 [3] NCCL INFO 8 coll channels, 8 p2p channels, 1 p2p channels per peer
 8: queue1-dy-p4d24xlarge-2:10515:10685 [0] NCCL INFO Channel 03 : 8[101c0] -> 15[a01d0] via P2P/IPC/read
 0: queue1-dy-p4d24xlarge-1:10487:10655 [0] NCCL INFO Channel 07 : 0[101c0] -> 7[a01d0] via P2P/IPC/read
 8: queue1-dy-p4d24xlarge-2:10515:10685 [0] NCCL INFO Channel 06 : 8[101c0] -> 15[a01d0] via P2P/IPC/read
 3: queue1-dy-p4d24xlarge-1:10490:10660 [3] NCCL INFO Connected all trees
 3: queue1-dy-p4d24xlarge-1:10490:10660 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
 3: queue1-dy-p4d24xlarge-1:10490:10660 [3] NCCL INFO 8 coll channels, 8 p2p channels, 1 p2p channels per peer
12: queue1-dy-p4d24xlarge-2:10519:10682 [4] NCCL INFO Channel 02 : 4[901c0] -> 12[901c0] [receive] via NET/Socket/2
12: queue1-dy-p4d24xlarge-2:10519:10682 [4] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
 8: queue1-dy-p4d24xlarge-2:10515:10685 [0] NCCL INFO Channel 07 : 8[101c0] -> 15[a01d0] via P2P/IPC/read
14: queue1-dy-p4d24xlarge-2:10521:10681 [6] NCCL INFO Connected all trees
14: queue1-dy-p4d24xlarge-2:10521:10681 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
14: queue1-dy-p4d24xlarge-2:10521:10681 [6] NCCL INFO 8 coll channels, 8 p2p channels, 1 p2p channels per peer
 4: queue1-dy-p4d24xlarge-1:10491:10662 [4] NCCL INFO Channel 02 : 12[901c0] -> 4[901c0] [receive] via NET/Socket/2
 4: queue1-dy-p4d24xlarge-1:10491:10662 [4] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
13: queue1-dy-p4d24xlarge-2:10520:10683 [5] NCCL INFO Channel 02 : 13[901d0] -> 12[901c0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-2:10519:10682 [4] NCCL INFO Channel 03 : 4[901c0] -> 12[901c0] [receive] via NET/Socket/3
12: queue1-dy-p4d24xlarge-2:10519:10682 [4] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
10: queue1-dy-p4d24xlarge-2:10517:10684 [2] NCCL INFO Channel 02 : 10[201c0] -> 9[101d0] via P2P/IPC/read
 6: queue1-dy-p4d24xlarge-1:10493:10658 [6] NCCL INFO Connected all trees
 6: queue1-dy-p4d24xlarge-1:10493:10658 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
 6: queue1-dy-p4d24xlarge-1:10493:10658 [6] NCCL INFO 8 coll channels, 8 p2p channels, 1 p2p channels per peer
13: queue1-dy-p4d24xlarge-2:10520:10683 [5] NCCL INFO Channel 03 : 13[901d0] -> 12[901c0] via P2P/IPC/read
10: queue1-dy-p4d24xlarge-2:10517:10684 [2] NCCL INFO Channel 03 : 10[201c0] -> 9[101d0] via P2P/IPC/read
13: queue1-dy-p4d24xlarge-2:10520:10683 [5] NCCL INFO Channel 06 : 13[901d0] -> 12[901c0] via P2P/IPC/read
15: queue1-dy-p4d24xlarge-2:10522:10680 [7] NCCL INFO Channel 02 : 15[a01d0] -> 8[101c0] via P2P/IPC/read
10: queue1-dy-p4d24xlarge-2:10517:10684 [2] NCCL INFO Channel 06 : 10[201c0] -> 9[101d0] via P2P/IPC/read
13: queue1-dy-p4d24xlarge-2:10520:10683 [5] NCCL INFO Channel 07 : 13[901d0] -> 12[901c0] via P2P/IPC/read
15: queue1-dy-p4d24xlarge-2:10522:10680 [7] NCCL INFO Channel 03 : 15[a01d0] -> 8[101c0] via P2P/IPC/read
10: queue1-dy-p4d24xlarge-2:10517:10684 [2] NCCL INFO Channel 07 : 10[201c0] -> 9[101d0] via P2P/IPC/read
15: queue1-dy-p4d24xlarge-2:10522:10680 [7] NCCL INFO Channel 06 : 15[a01d0] -> 8[101c0] via P2P/IPC/read
 5: queue1-dy-p4d24xlarge-1:10492:10661 [5] NCCL INFO Channel 02 : 5[901d0] -> 4[901c0] via P2P/IPC/read
 7: queue1-dy-p4d24xlarge-1:10494:10657 [7] NCCL INFO Channel 02 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-1:10491:10662 [4] NCCL INFO Channel 03 : 12[901c0] -> 4[901c0] [receive] via NET/Socket/3
 2: queue1-dy-p4d24xlarge-1:10489:10656 [2] NCCL INFO Channel 02 : 2[201c0] -> 1[101d0] via P2P/IPC/read
 4: queue1-dy-p4d24xlarge-1:10491:10662 [4] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
 9: queue1-dy-p4d24xlarge-2:10516:10686 [1] NCCL INFO Channel 00 : 9[101d0] -> 8[101c0] via P2P/IPC/read
15: queue1-dy-p4d24xlarge-2:10522:10680 [7] NCCL INFO Channel 07 : 15[a01d0] -> 8[101c0] via P2P/IPC/read
 9: queue1-dy-p4d24xlarge-2:10516:10686 [1] NCCL INFO Channel 01 : 9[101d0] -> 8[101c0] via P2P/IPC/read
12: queue1-dy-p4d24xlarge-2:10519:10682 [4] NCCL INFO Channel 06 : 4[901c0] -> 12[901c0] [receive] via NET/Socket/2
12: queue1-dy-p4d24xlarge-2:10519:10682 [4] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
 9: queue1-dy-p4d24xlarge-2:10516:10686 [1] NCCL INFO Channel 04 : 9[101d0] -> 8[101c0] via P2P/IPC/read
 7: queue1-dy-p4d24xlarge-1:10494:10657 [7] NCCL INFO Channel 03 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
 5: queue1-dy-p4d24xlarge-1:10492:10661 [5] NCCL INFO Channel 03 : 5[901d0] -> 4[901c0] via P2P/IPC/read
 2: queue1-dy-p4d24xlarge-1:10489:10656 [2] NCCL INFO Channel 03 : 2[201c0] -> 1[101d0] via P2P/IPC/read
 9: queue1-dy-p4d24xlarge-2:10516:10686 [1] NCCL INFO Channel 05 : 9[101d0] -> 8[101c0] via P2P/IPC/read
 7: queue1-dy-p4d24xlarge-1:10494:10657 [7] NCCL INFO Channel 06 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
 2: queue1-dy-p4d24xlarge-1:10489:10656 [2] NCCL INFO Channel 06 : 2[201c0] -> 1[101d0] via P2P/IPC/read
 5: queue1-dy-p4d24xlarge-1:10492:10661 [5] NCCL INFO Channel 06 : 5[901d0] -> 4[901c0] via P2P/IPC/read
 7: queue1-dy-p4d24xlarge-1:10494:10657 [7] NCCL INFO Channel 07 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
 2: queue1-dy-p4d24xlarge-1:10489:10656 [2] NCCL INFO Channel 07 : 2[201c0] -> 1[101d0] via P2P/IPC/read
 5: queue1-dy-p4d24xlarge-1:10492:10661 [5] NCCL INFO Channel 07 : 5[901d0] -> 4[901c0] via P2P/IPC/read
 1: queue1-dy-p4d24xlarge-1:10488:10659 [1] NCCL INFO Channel 00 : 1[101d0] -> 0[101c0] via P2P/IPC/read
 1: queue1-dy-p4d24xlarge-1:10488:10659 [1] NCCL INFO Channel 01 : 1[101d0] -> 0[101c0] via P2P/IPC/read
 1: queue1-dy-p4d24xlarge-1:10488:10659 [1] NCCL INFO Channel 04 : 1[101d0] -> 0[101c0] via P2P/IPC/read
10: queue1-dy-p4d24xlarge-2:10517:10684 [2] NCCL INFO Connected all trees
10: queue1-dy-p4d24xlarge-2:10517:10684 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
10: queue1-dy-p4d24xlarge-2:10517:10684 [2] NCCL INFO 8 coll channels, 8 p2p channels, 1 p2p channels per peer
 8: queue1-dy-p4d24xlarge-2:10515:10685 [0] NCCL INFO Channel 00 : 0[101c0] -> 8[101c0] [receive] via NET/Socket/0
 1: queue1-dy-p4d24xlarge-1:10488:10659 [1] NCCL INFO Channel 05 : 1[101d0] -> 0[101c0] via P2P/IPC/read
 8: queue1-dy-p4d24xlarge-2:10515:10685 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
 0: queue1-dy-p4d24xlarge-1:10487:10655 [0] NCCL INFO Channel 00 : 8[101c0] -> 0[101c0] [receive] via NET/Socket/0
 0: queue1-dy-p4d24xlarge-1:10487:10655 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
 4: queue1-dy-p4d24xlarge-1:10491:10662 [4] NCCL INFO Channel 06 : 12[901c0] -> 4[901c0] [receive] via NET/Socket/2
 4: queue1-dy-p4d24xlarge-1:10491:10662 [4] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
12: queue1-dy-p4d24xlarge-2:10519:10682 [4] NCCL INFO Channel 07 : 4[901c0] -> 12[901c0] [receive] via NET/Socket/3
12: queue1-dy-p4d24xlarge-2:10519:10682 [4] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
 2: queue1-dy-p4d24xlarge-1:10489:10656 [2] NCCL INFO Connected all trees
 2: queue1-dy-p4d24xlarge-1:10489:10656 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
 2: queue1-dy-p4d24xlarge-1:10489:10656 [2] NCCL INFO 8 coll channels, 8 p2p channels, 1 p2p channels per peer
 8: queue1-dy-p4d24xlarge-2:10515:10685 [0] NCCL INFO Channel 01 : 0[101c0] -> 8[101c0] [receive] via NET/Socket/1
 0: queue1-dy-p4d24xlarge-1:10487:10655 [0] NCCL INFO Channel 01 : 8[101c0] -> 0[101c0] [receive] via NET/Socket/1
 8: queue1-dy-p4d24xlarge-2:10515:10685 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
 0: queue1-dy-p4d24xlarge-1:10487:10655 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
 4: queue1-dy-p4d24xlarge-1:10491:10662 [4] NCCL INFO Channel 07 : 12[901c0] -> 4[901c0] [receive] via NET/Socket/3
 4: queue1-dy-p4d24xlarge-1:10491:10662 [4] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
12: queue1-dy-p4d24xlarge-2:10519:10682 [4] NCCL INFO Channel 02 : 12[901c0] -> 4[901c0] [send] via NET/Socket/2
 8: queue1-dy-p4d24xlarge-2:10515:10685 [0] NCCL INFO Channel 04 : 0[101c0] -> 8[101c0] [receive] via NET/Socket/0
 8: queue1-dy-p4d24xlarge-2:10515:10685 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
 0: queue1-dy-p4d24xlarge-1:10487:10655 [0] NCCL INFO Channel 04 : 8[101c0] -> 0[101c0] [receive] via NET/Socket/0
 0: queue1-dy-p4d24xlarge-1:10487:10655 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
 4: queue1-dy-p4d24xlarge-1:10491:10662 [4] NCCL INFO Channel 02 : 4[901c0] -> 12[901c0] [send] via NET/Socket/2
12: queue1-dy-p4d24xlarge-2:10519:10682 [4] NCCL INFO Channel 03 : 12[901c0] -> 4[901c0] [send] via NET/Socket/3
 8: queue1-dy-p4d24xlarge-2:10515:10685 [0] NCCL INFO Channel 05 : 0[101c0] -> 8[101c0] [receive] via NET/Socket/1
 8: queue1-dy-p4d24xlarge-2:10515:10685 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
 0: queue1-dy-p4d24xlarge-1:10487:10655 [0] NCCL INFO Channel 05 : 8[101c0] -> 0[101c0] [receive] via NET/Socket/1
 0: queue1-dy-p4d24xlarge-1:10487:10655 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread
 4: queue1-dy-p4d24xlarge-1:10491:10662 [4] NCCL INFO Channel 03 : 4[901c0] -> 12[901c0] [send] via NET/Socket/3
12: queue1-dy-p4d24xlarge-2:10519:10682 [4] NCCL INFO Channel 06 : 12[901c0] -> 4[901c0] [send] via NET/Socket/2
 8: queue1-dy-p4d24xlarge-2:10515:10685 [0] NCCL INFO Channel 00 : 8[101c0] -> 0[101c0] [send] via NET/Socket/0
 0: queue1-dy-p4d24xlarge-1:10487:10655 [0] NCCL INFO Channel 00 : 0[101c0] -> 8[101c0] [send] via NET/Socket/0
 4: queue1-dy-p4d24xlarge-1:10491:10662 [4] NCCL INFO Channel 06 : 4[901c0] -> 12[901c0] [send] via NET/Socket/2
12: queue1-dy-p4d24xlarge-2:10519:10682 [4] NCCL INFO Channel 07 : 12[901c0] -> 4[901c0] [send] via NET/Socket/3
 8: queue1-dy-p4d24xlarge-2:10515:10685 [0] NCCL INFO Channel 01 : 8[101c0] -> 0[101c0] [send] via NET/Socket/1
 0: queue1-dy-p4d24xlarge-1:10487:10655 [0] NCCL INFO Channel 01 : 0[101c0] -> 8[101c0] [send] via NET/Socket/1
 4: queue1-dy-p4d24xlarge-1:10491:10662 [4] NCCL INFO Channel 07 : 4[901c0] -> 12[901c0] [send] via NET/Socket/3
 8: queue1-dy-p4d24xlarge-2:10515:10685 [0] NCCL INFO Channel 04 : 8[101c0] -> 0[101c0] [send] via NET/Socket/0
 0: queue1-dy-p4d24xlarge-1:10487:10655 [0] NCCL INFO Channel 04 : 0[101c0] -> 8[101c0] [send] via NET/Socket/0
 8: queue1-dy-p4d24xlarge-2:10515:10685 [0] NCCL INFO Channel 05 : 8[101c0] -> 0[101c0] [send] via NET/Socket/1
 0: queue1-dy-p4d24xlarge-1:10487:10655 [0] NCCL INFO Channel 05 : 0[101c0] -> 8[101c0] [send] via NET/Socket/1
13: queue1-dy-p4d24xlarge-2:10520:10683 [5] NCCL INFO Connected all trees
13: queue1-dy-p4d24xlarge-2:10520:10683 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
13: queue1-dy-p4d24xlarge-2:10520:10683 [5] NCCL INFO 8 coll channels, 8 p2p channels, 1 p2p channels per peer
12: queue1-dy-p4d24xlarge-2:10519:10682 [4] NCCL INFO Connected all trees
12: queue1-dy-p4d24xlarge-2:10519:10682 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
12: queue1-dy-p4d24xlarge-2:10519:10682 [4] NCCL INFO 8 coll channels, 8 p2p channels, 1 p2p channels per peer
15: queue1-dy-p4d24xlarge-2:10522:10680 [7] NCCL INFO Connected all trees
15: queue1-dy-p4d24xlarge-2:10522:10680 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
15: queue1-dy-p4d24xlarge-2:10522:10680 [7] NCCL INFO 8 coll channels, 8 p2p channels, 1 p2p channels per peer
 8: queue1-dy-p4d24xlarge-2:10515:10685 [0] NCCL INFO Connected all trees
 8: queue1-dy-p4d24xlarge-2:10515:10685 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
 8: queue1-dy-p4d24xlarge-2:10515:10685 [0] NCCL INFO 8 coll channels, 8 p2p channels, 1 p2p channels per peer
 9: queue1-dy-p4d24xlarge-2:10516:10686 [1] NCCL INFO Connected all trees
 9: queue1-dy-p4d24xlarge-2:10516:10686 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
 9: queue1-dy-p4d24xlarge-2:10516:10686 [1] NCCL INFO 8 coll channels, 8 p2p channels, 1 p2p channels per peer
 8: queue1-dy-p4d24xlarge-2:10515:10685 [0] NCCL INFO comm 0x7f4d80002fb0 rank 8 nranks 16 cudaDev 0 busId 101c0 - Init COMPLETE
12: queue1-dy-p4d24xlarge-2:10519:10682 [4] NCCL INFO comm 0x7f1160002fb0 rank 12 nranks 16 cudaDev 4 busId 901c0 - Init COMPLETE
10: queue1-dy-p4d24xlarge-2:10517:10684 [2] NCCL INFO comm 0x7fb300002fb0 rank 10 nranks 16 cudaDev 2 busId 201c0 - Init COMPLETE
13: queue1-dy-p4d24xlarge-2:10520:10683 [5] NCCL INFO comm 0x7fa1c0002fb0 rank 13 nranks 16 cudaDev 5 busId 901d0 - Init COMPLETE
14: queue1-dy-p4d24xlarge-2:10521:10681 [6] NCCL INFO comm 0x7f9ffc002fb0 rank 14 nranks 16 cudaDev 6 busId a01c0 - Init COMPLETE
15: queue1-dy-p4d24xlarge-2:10522:10680 [7] NCCL INFO comm 0x7f9c58002fb0 rank 15 nranks 16 cudaDev 7 busId a01d0 - Init COMPLETE
 9: queue1-dy-p4d24xlarge-2:10516:10686 [1] NCCL INFO comm 0x7f1414002fb0 rank 9 nranks 16 cudaDev 1 busId 101d0 - Init COMPLETE
11: queue1-dy-p4d24xlarge-2:10518:10687 [3] NCCL INFO comm 0x7fda90002fb0 rank 11 nranks 16 cudaDev 3 busId 201d0 - Init COMPLETE
 4: queue1-dy-p4d24xlarge-1:10491:10662 [4] NCCL INFO Connected all trees
 4: queue1-dy-p4d24xlarge-1:10491:10662 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
 4: queue1-dy-p4d24xlarge-1:10491:10662 [4] NCCL INFO 8 coll channels, 8 p2p channels, 1 p2p channels per peer
 5: queue1-dy-p4d24xlarge-1:10492:10661 [5] NCCL INFO Connected all trees
 5: queue1-dy-p4d24xlarge-1:10492:10661 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
 5: queue1-dy-p4d24xlarge-1:10492:10661 [5] NCCL INFO 8 coll channels, 8 p2p channels, 1 p2p channels per peer
 7: queue1-dy-p4d24xlarge-1:10494:10657 [7] NCCL INFO Connected all trees
 7: queue1-dy-p4d24xlarge-1:10494:10657 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
 7: queue1-dy-p4d24xlarge-1:10494:10657 [7] NCCL INFO 8 coll channels, 8 p2p channels, 1 p2p channels per peer
 0: queue1-dy-p4d24xlarge-1:10487:10655 [0] NCCL INFO Connected all trees
 0: queue1-dy-p4d24xlarge-1:10487:10655 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
 0: queue1-dy-p4d24xlarge-1:10487:10655 [0] NCCL INFO 8 coll channels, 8 p2p channels, 1 p2p channels per peer
 1: queue1-dy-p4d24xlarge-1:10488:10659 [1] NCCL INFO Connected all trees
 1: queue1-dy-p4d24xlarge-1:10488:10659 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512
 1: queue1-dy-p4d24xlarge-1:10488:10659 [1] NCCL INFO 8 coll channels, 8 p2p channels, 1 p2p channels per peer
 0: queue1-dy-p4d24xlarge-1:10487:10655 [0] NCCL INFO comm 0x7f01dc002fb0 rank 0 nranks 16 cudaDev 0 busId 101c0 - Init COMPLETE
 1: queue1-dy-p4d24xlarge-1:10488:10659 [1] NCCL INFO comm 0x7efbe0002fb0 rank 1 nranks 16 cudaDev 1 busId 101d0 - Init COMPLETE
 4: queue1-dy-p4d24xlarge-1:10491:10662 [4] NCCL INFO comm 0x7faf1c002fb0 rank 4 nranks 16 cudaDev 4 busId 901c0 - Init COMPLETE
 2: queue1-dy-p4d24xlarge-1:10489:10656 [2] NCCL INFO comm 0x7f822c002fb0 rank 2 nranks 16 cudaDev 2 busId 201c0 - Init COMPLETE
 5: queue1-dy-p4d24xlarge-1:10492:10661 [5] NCCL INFO comm 0x7feffc002fb0 rank 5 nranks 16 cudaDev 5 busId 901d0 - Init COMPLETE
 6: queue1-dy-p4d24xlarge-1:10493:10658 [6] NCCL INFO comm 0x7fd9e4002fb0 rank 6 nranks 16 cudaDev 6 busId a01c0 - Init COMPLETE
 7: queue1-dy-p4d24xlarge-1:10494:10657 [7] NCCL INFO comm 0x7f73d0002fb0 rank 7 nranks 16 cudaDev 7 busId a01d0 - Init COMPLETE
 3: queue1-dy-p4d24xlarge-1:10490:10660 [3] NCCL INFO comm 0x7efb98002fb0 rank 3 nranks 16 cudaDev 3 busId 201d0 - Init COMPLETE
 0: queue1-dy-p4d24xlarge-1:10487:10487 [0] NCCL INFO Launch mode Parallel
 0: 2022-11-02 19:14:25 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.1, activation_fn='silu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=True, adaptive_input_cutoff='20000,60000', adaptive_input_factor=4, adaptive_softmax_cutoff='20000,60000', adaptive_softmax_dropout=0.2, adaptive_softmax_factor=4, add_bos_token=False, all_gather_list_size=16384, arch='mega_lm_adaptive_big', attention_activation_fn='softmax', attention_dropout=0.1, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', clip_mode='total', clip_norm=0.25, cpu=False, criterion='adaptive_loss', curriculum=0, data='/fsx/datasets/wikitext-103', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', decoder_chunk_size=1024, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_hidden_dim=2048, decoder_input_dim=1024, decoder_layers=16, decoder_n_dim=16, decoder_z_dim=256, device_id=0, disable_validation=False, distributed_b
 0: ackend='nccl', distributed_init_method='env://', distributed_no_spawn=True, distributed_port=-1, distributed_rank=0, distributed_world_size=16, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, end_learning_rate=0.0, fast_stat_sync=False, feature_dropout=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, future_target=False, hidden_dropout=0.1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, localsgd_frequency=3, log_format='simple', log_interval=100, lr=[0.005], lr_scheduler='linear_decay', max_epoch=0, max_sentences=None, max_sentences_valid=None, max_target_positions=8096, max_tokens=6144, max_tokens_valid=6144, max_update=400000, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_affine_final_norm=True
 0: , no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, normalization_type='layernorm', normalize_before=True, normalize_embedding=False, nprocs_per_node=8, num_workers=0, optimizer='adam', optimizer_overrides='{}', output_dictionary_size=-1, past_target=False, patience=-1, profile=False, quantization_config_path=None, rel_pos_bias='rotary', report_ema_alpha=False, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sample_break_mode='complete', save_dir='saved_models/cu11_test', save_interval=1, save_interval_updates=0, seed=42, self_target=False, sentence_avg=False, share_all_embeddings=False, share_decoder_input_output_embed=False, shorten_data_split_list='', shorten_method='none', skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_min_
 0: lr=-1, stop_time_hours=0, task='language_modeling', tensorboard_logdir='', test_subset='test', threshold_loss_scale=None, tie_adaptive_proj=True, tie_adaptive_weights=True, tokenizer=None, tokens_per_sample=2048, total_num_update=400000, tpu=False, train_subset='train', truncation_length=8192, update_freq=[1], use_bmuf=False, use_old_adam=False, user_dir=None, valid_block='splits:10', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, variant_block_multiple_max=6, variant_block_multiple_min=2, wandb_entity=None, wandb_id=None, wandb_project=None, warmup_init_lr=1e-07, warmup_power=1, warmup_updates=24000, weight_decay=0.1, write_out_alpha=False)
 0: 2022-11-02 19:14:25 | INFO | fairseq.tasks.language_modeling | dictionary: 267744 types
 0: 2022-11-02 19:14:25 | INFO | fairseq.data.data_utils | loaded 3760 examples from: /fsx/datasets/wikitext-103/valid
 0: 2022-11-02 19:14:29 | INFO | fairseq_cli.train | MegaLanguageModel(
 0:   (decoder): MegaDecoderNoCrossAttn(
 0:     (embedding_dropout): FairseqDropout(p=0.3)
 0:     (embed_tokens): AdaptiveInput(
 0:       (embeddings): ModuleList(
 0:         (0): Sequential(
 0:           (0): Embedding(20000, 1024, padding_idx=1)
 0:           (1): Linear(in_features=1024, out_features=1024, bias=False)
 0:         )
 0:         (1): Sequential(
 0:           (0): Embedding(40000, 256)
 0:           (1): Linear(in_features=256, out_features=1024, bias=False)
 0:         )
 0:         (2): Sequential(
 0:           (0): Embedding(207744, 64)
 0:           (1): Linear(in_features=64, out_features=1024, bias=False)
 0:         )
 0:       )
 0:     )
 0:     (layers): ModuleList(
 0:       (0): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:       (1): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:       (2): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:       (3): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:       (4): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:       (5): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:       (6): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:       (7): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:       (8): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:       (9): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:       (10): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:       (11): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:       (12): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:       (13): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:       (14): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:       (15): MegaDecoderLayer(
 0:         (mega_layer): MovingAverageGatedAttention(
 0:           edim=1024, zdim=256, hdim=2048, ndim=16, chunk=1024, attn_act=softmax, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (attention_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (move): MultiHeadEMA(edim=1024, ndim=16, bidirectional=False, trunction=8192)
 0:           (v_proj): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (mx_proj): Linear(in_features=1024, out_features=4352, bias=True)
 0:           (h_proj): Linear(in_features=2048, out_features=1024, bias=True)
 0:           (rel_pos_bias): RotaryRelativePositionalBias(dim=256, max positions=1024)
 0:         )
 0:         (nffn): NormalizedFeedForwardNetwork(
 0:           edim=1024, hdim=2048, act=silu, prenorm=True
 0:           (dropout): FairseqDropout(p=0.3)
 0:           (hidden_dropout): FairseqDropout(p=0.1)
 0:           (norm): SequenceNorm(
 0:             (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
 0:           )
 0:           (fc1): Linear(in_features=1024, out_features=2048, bias=True)
 0:           (fc2): Linear(in_features=2048, out_features=1024, bias=True)
 0:         )
 0:       )
 0:     )
 0:     (final_norm): SequenceNorm(
 0:       (norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=False)
 0:     )
 0:     (adaptive_softmax): AdaptiveSoftmax(
 0:       (dropout_module): FairseqDropout(p=0.2)
 0:       (lsm): LogSoftmax(dim=1)
 0:       (head): TiedHeadModule(
 0:         (word_proj): TiedLinear()
 0:         (class_proj): Linear(in_features=1024, out_features=2, bias=False)
 0:       )
 0:       (tail): ModuleList(
 0:         (0): Sequential(
 0:           (0): TiedLinear()
 0:           (1): Dropout(p=0.2, inplace=False)
 0:           (2): TiedLinear()
 0:         )
 0:         (1): Sequential(
 0:           (0): TiedLinear()
 0:           (1): Dropout(p=0.2, inplace=False)
 0:           (2): TiedLinear()
 0:         )
 0:       )
 0:     )
 0:   )
 0: )
 0: 2022-11-02 19:14:29 | INFO | fairseq_cli.train | task: language_modeling (LanguageModelingTask)
 0: 2022-11-02 19:14:29 | INFO | fairseq_cli.train | model: mega_lm_adaptive_big (MegaLanguageModel)
 0: 2022-11-02 19:14:29 | INFO | fairseq_cli.train | criterion: adaptive_loss (AdaptiveLoss)
 0: 2022-11-02 19:14:29 | INFO | fairseq_cli.train | num. model params: 252237824 (num. trained: 252237824)
 0: 2022-11-02 19:14:30 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.embeddings.0.0.weight <- decoder.adaptive_softmax.head.word_proj.weight
 0: 2022-11-02 19:14:30 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.embeddings.0.1.bias <- decoder.embed_tokens.embeddings.1.1.bias
 0: 2022-11-02 19:14:30 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.embeddings.0.1.bias <- decoder.embed_tokens.embeddings.2.1.bias
 0: 2022-11-02 19:14:30 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.embeddings.0.1.bias <- decoder.final_norm.norm.weight
 0: 2022-11-02 19:14:30 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.embeddings.0.1.bias <- decoder.final_norm.norm.bias
 0: 2022-11-02 19:14:30 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.embeddings.0.1.bias <- decoder.adaptive_softmax.head.class_proj.bias
 0: 2022-11-02 19:14:30 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.embeddings.1.0.weight <- decoder.adaptive_softmax.tail.0.2.weight
 0: 2022-11-02 19:14:30 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.embeddings.1.1.weight <- decoder.adaptive_softmax.tail.0.0.weight
 0: 2022-11-02 19:14:30 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.embeddings.2.0.weight <- decoder.adaptive_softmax.tail.1.2.weight
 0: 2022-11-02 19:14:30 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.embeddings.2.1.weight <- decoder.adaptive_softmax.tail.1.0.weight
 0: 2022-11-02 19:14:30 | INFO | fairseq.utils | ***********************CUDA enviroments for all 16 workers***********************
 0: 2022-11-02 19:14:30 | INFO | fairseq.utils | rank   0: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-02 19:14:30 | INFO | fairseq.utils | rank   1: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-02 19:14:30 | INFO | fairseq.utils | rank   2: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-02 19:14:30 | INFO | fairseq.utils | rank   3: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-02 19:14:30 | INFO | fairseq.utils | rank   4: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-02 19:14:30 | INFO | fairseq.utils | rank   5: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-02 19:14:30 | INFO | fairseq.utils | rank   6: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-02 19:14:30 | INFO | fairseq.utils | rank   7: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-02 19:14:30 | INFO | fairseq.utils | rank   8: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-02 19:14:30 | INFO | fairseq.utils | rank   9: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-02 19:14:30 | INFO | fairseq.utils | rank  10: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-02 19:14:30 | INFO | fairseq.utils | rank  11: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-02 19:14:30 | INFO | fairseq.utils | rank  12: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-02 19:14:30 | INFO | fairseq.utils | rank  13: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-02 19:14:30 | INFO | fairseq.utils | rank  14: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-02 19:14:30 | INFO | fairseq.utils | rank  15: capabilities =  8.0  ; total memory = 39.586 GB ; name = NVIDIA A100-SXM4-40GB                   
 0: 2022-11-02 19:14:30 | INFO | fairseq.utils | ***********************CUDA enviroments for all 16 workers***********************
 0: 2022-11-02 19:14:30 | INFO | fairseq_cli.train | training on 16 devices (GPUs/TPUs)
 0: 2022-11-02 19:14:30 | INFO | fairseq_cli.train | max tokens per GPU = 6144 and max sentences per GPU = None
 0: 2022-11-02 19:14:30 | INFO | fairseq.trainer | no existing checkpoint found saved_models/cu11_test/checkpoint_last.pt
 0: 2022-11-02 19:14:30 | INFO | fairseq.trainer | loading train data for epoch 1
 0: 2022-11-02 19:14:30 | INFO | fairseq.data.data_utils | loaded 1801350 examples from: /fsx/datasets/wikitext-103/train
 0: 2022-11-02 19:14:30 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16
 0: 2022-11-02 19:14:30 | INFO | fairseq.optim.adam | using FusedAdam
 0: 2022-11-02 19:14:30 | INFO | fairseq.trainer | begin training epoch 1
 0: 2022-11-02 19:15:31 | INFO | train_inner | epoch 001:    100 / 1230 loss=13.75, ppl=13779, wps=141672, ups=1.69, wpb=83611.2, bsz=20.6, num_updates=100, lr=2.09329e-05, gnorm=2.509, clip=100, train_wall=61, wall=62
 0: 2022-11-02 19:16:28 | INFO | train_inner | epoch 001:    200 / 1230 loss=11.124, ppl=2232.35, wps=149693, ups=1.78, wpb=84167.9, bsz=20.8, num_updates=200, lr=4.17658e-05, gnorm=0.55, clip=100, train_wall=56, wall=118
 0: 2022-11-02 19:17:24 | INFO | train_inner | epoch 001:    300 / 1230 loss=10.581, ppl=1532.29, wps=148950, ups=1.77, wpb=84139.5, bsz=20.9, num_updates=300, lr=6.25987e-05, gnorm=0.591, clip=100, train_wall=56, wall=174
 0: 2022-11-02 19:18:21 | INFO | train_inner | epoch 001:    400 / 1230 loss=10.13, ppl=1120.86, wps=148376, ups=1.77, wpb=83759.7, bsz=20.9, num_updates=400, lr=8.34317e-05, gnorm=0.567, clip=100, train_wall=56, wall=231
 0: 2022-11-02 19:19:18 | INFO | train_inner | epoch 001:    500 / 1230 loss=9.691, ppl=826.69, wps=145194, ups=1.74, wpb=83446.3, bsz=20.7, num_updates=500, lr=0.000104265, gnorm=0.604, clip=100, train_wall=57, wall=288
 0: 2022-11-02 19:20:14 | INFO | train_inner | epoch 001:    600 / 1230 loss=9.319, ppl=638.66, wps=149006, ups=1.77, wpb=83961, bsz=21.2, num_updates=600, lr=0.000125097, gnorm=0.58, clip=100, train_wall=56, wall=345
 0: 2022-11-02 19:21:11 | INFO | train_inner | epoch 001:    700 / 1230 loss=9.026, ppl=521.18, wps=148320, ups=1.77, wpb=84005.2, bsz=20.7, num_updates=700, lr=0.00014593, gnorm=0.547, clip=100, train_wall=56, wall=401
 0: 2022-11-02 19:22:07 | INFO | train_inner | epoch 001:    800 / 1230 loss=8.775, ppl=438.11, wps=150036, ups=1.78, wpb=84386.1, bsz=21, num_updates=800, lr=0.000166763, gnorm=0.57, clip=100, train_wall=56, wall=458
 0: 2022-11-02 19:23:04 | INFO | train_inner | epoch 001:    900 / 1230 loss=8.555, ppl=376.21, wps=147835, ups=1.75, wpb=84358.3, bsz=21.1, num_updates=900, lr=0.000187596, gnorm=0.542, clip=100, train_wall=57, wall=515
 0: 2022-11-02 19:24:01 | INFO | train_inner | epoch 001:   1000 / 1230 loss=8.346, ppl=325.37, wps=148461, ups=1.77, wpb=83970.9, bsz=20.8, num_updates=1000, lr=0.000208429, gnorm=0.535, clip=100, train_wall=56, wall=571
 0: 2022-11-02 19:24:58 | INFO | train_inner | epoch 001:   1100 / 1230 loss=8.182, ppl=290.47, wps=148550, ups=1.77, wpb=84045.5, bsz=21.2, num_updates=1100, lr=0.000229262, gnorm=0.511, clip=100, train_wall=56, wall=628
